
= A formal account of nets =


The aim of this page is to provide a common framework for describing linear
logic proof nets, interaction nets, multiport interaction nets, and the likes,
while factoring out most of the tedious, uninteresting details (clearly not the
fanciest page of LLWiki).

== Preliminaries ==

=== The short story ===

* the general flavor is that of multiport interaction nets;
* the top/down or passive/active orientation of cells is related with the distinction between premisses and conclusions of rules, (and in that sense, a cut is not a logical rule, but the focus of interaction between two rules);
* cuts are thus wires rather than cells/links: this fits with the intuition of GoI, but not with the most common presentations of proof nets;
* because the notion of subnet is not trivial in multiport interaction nets, and to avoid the use of geometric conditions (boxes must not overlap but can be nested), we introduce boxes as particular cells;
* when representing proof nets, we introduce axioms explicitly as cells, so that axiom-cuts do not vanish.

== Nets ==

=== Wires ===

A ''wiring'' is the data of a finite set <math>P</math> of ports
and of a partition <math>{W}</math> of <math>P</math> by pairs (the ''wires''):
if <math>\{p,q\}\in{W}</math>, we write <math>{W}(p)=q</math> and <math>{W}(q)=p</math>.
Hence a wiring is equivalently given by
an involutive permutation <math>{W}</math> of finite domain <math>P</math>,
without fixpoints (forall <math>p</math>, <math>{W}(p)\not=p</math>): the wires are then
the orbits.
Another equivalent presentation is to consider <math>{W}</math> as
a (simple, loopless, undirected) graph, with vertices in P,
all of degree 1.

We say two wirings are disjoint when their sets of ports are.
A ''connection'' between two disjoint wirings <math>W</math> and <math>W'</math> is
a partial injection <math>(I,I',f):P\pinj P'</math>: <math>I\subseteq P</math>,
<math>I'\subseteq P'</math> and <math>f</math> is a bijection <math>I\cong I'</math>.
We then write <math>W\bowtie_f{{W}'}</math> for the wiring obtained
by identifying the ports pairwise mapped by <math>f</math>, and then
``straightening`` the paths thus obtained to recover wires:
notice this might also introduce loops and we write <math>\Inner{W}{W'}_f</math>
for the number of loops thus appeared.

We describe these operations a bit more formally.
Write <math>P = P_0\uplus I</math> and <math>P' = P_0'\uplus I'</math>.
Then consider the graph <math>W\dblcolon_f{{W}'}</math> with vertices in
<math>P\cup P'</math>, and such that
there is an edge between <math>p</math> and <math>q</math> iff
<math>q={W}(p)</math> or
<math>q={W'}(p)</math> or
<math>q=f(p)</math> or
<math>p=f(q)</math>:
in other words, <math>W\dblcolon_f{{W}'}=W\cup W'\cup f\cup f^{-1}</math>.
Vertices in <math>P_0\cup P'_0</math>
are of degree 1, and the others are of degree 2.
Hence maximal paths in <math>W\dblcolon_f{{W}'}</math> are of two kinds:

* straight paths, with both ends in <math>P_0\cup P_0'</math>;
* cycles, with vertices all in <math>I\cup I'</math>.
Then the wires in <math>W\bowtie_f{{W}'}</math> are the pairs <math>\{p,p'\}</math> such that
<math>p</math> and <math>p'</math> are the ends of a path in <math>W\dblcolon_f{{W}'}</math>.
And <math>\Inner{W}{W'}_{f}</math> is the number of
cycles in <math>W\dblcolon_f{{W}'}</math>, or more precisely
the number of support sets of cycles (i.e. we forget about the starting
vertice of cycles).

{{Lemma|
Consider three wirings <math>(P,W)</math>, <math>(P',W')</math> and <math>(P'',W'')</math>,
and two connections <math>(I,I',f):P\pinj P'</math> and
<math>(J',J'',g):P'\pinj P''</math> such that <math>I'\cap J'=\emptyset</math>.
Then

<math> (W\bowtie_f W')\bowtie_g W''=
  W\bowtie_f(W'\bowtie_g W'')</math>

and

<math> \Inner{W}{W'}_{f}+\Inner{(W\bowtie_f{{W}'})}{{W}''}_g=
    \Inner{W}{(W'\bowtie_g W'')}_f+\Inner{{W}'}{{W}''}_g.</math>
}}

{{Proof|
The first equation holds because open maximal paths in
<math>W\dblcolon_f{W'\bowtie_g}{{W}''}</math> correspond with those in
<math>W\dblcolon_f{(W'\dblcolon_g{{W}''})}</math>, hence in
<math>(W\dblcolon_f W')\dblcolon_g W''</math>, hence in
<math>{W\bowtie_f{{W}'}}\dblcolon_g{{W}''}</math>.
The second equation holds because both sides are two possible writings for
the number of loops in <math>{(W\dblcolon_f{{W}'})}\dblcolon_g{{W}''}</math>
}}

=== Nets ===

A ''signature'' is the data of a set
<math>\Sigma</math> of symbols, together with arity functions
<math>\alpha:\Sigma\to\mathbf{N}\setminus\{0\}</math> (number of
''active'' ports, or conclusions)
and <math>\pi:\Sigma\to\mathbf N</math> (number of
''passive'' ports, or hypotheses).
In the remaining, we assume
such a signature is given.

A ''cell'' <math>c</math> with ports in <math>P</math> is the data of
a symbol <math>\sigma\in\Sigma</math> and of two disjoint lists
of pairwise distinct ports:
<math>\alpha(c)\in P^{\alpha(\sigma)}</math>
is the list of ''active'' ports
and <math>\pi(c)\in P^{\pi(\sigma)}</math>
is the list of ''passive'' ports.

A ''net'' is the data of a wiring <math>(P,W)</math>, of a set <math>C</math> of disjoint cells on
<math>P</math>, and of a number <math>L\in\mathbf N</math> (the number of ''loops''). It follows
from the definitions that each port <math>p\in P</math> appears in exactly one wire and in
at most one cell: we say <math>p</math> is ''free'' if it is not part of a cell, and
<math>p</math> is ''internal'' otherwise. Moreover, we say <math>p</math> is ''dangling'' if
<math>p</math> is free and <math>W(p)</math> is internal. We write <math>\textrm{fp}(R)</math> for the set of
free ports of a net <math>R</math>.

Generally, the “names” of internal ports of a net are not relevant, but free
ports matter most often: internal ports are the analogue of bound variables in
<math>\lambda</math>-terms.
More formally, an ''isomorphism'' from net <math>R</math> to net <math>R'</math>
is the data of a bijection of ports <math>\phi:P\cong P'</math> and a bijection
of cells <math>\psi:C\cong C'</math> such that:

* for all <math>p\in P</math>, <math>W'(\phi(p))=\phi(W(p))</math>;
* for all <math>c\in C</math>:
** <math>\sigma_{\psi(c)}=\sigma_c</math>,
** for all <math>i\in\{0,\dotsc,\alpha(\sigma_c)-1\}</math>, <math>\alpha(\psi(c))_i=\phi(\alpha(c)_i)</math>,
** for all <math>j\in\{0,\dotsc,\pi(\sigma_c)-1\}</math>, <math>\pi(\psi(c))_j=\phi(\pi(c)_j)</math>;
* <math>L=L'</math>.
Observe that under these conditions  <math>\psi</math> is uniquely induced by <math>\phi</math>.
We say that the isomorphism <math>\phi</math> is ''nominal'' if moreover <math>p\in\textrm{fp}(R)</math>
implies <math>p=\phi(p)</math>.

=== Subnets ===

We say two nets are disjoint when their sets of ports are.
Let <math>R</math> and <math>R'</math> be disjoint nets,  and <math>(I,I',f)</math> be a
connection between <math>W</math> and <math>W'</math>, such that
<math>I\subseteq \textrm{fp}(R)</math> and <math>I'\subseteq \textrm{fp}(R')</math>.
We then write <math>R\bowtie_f{R'}</math> for the net
with wiring <math>W\bowtie_f{W'}</math>, cells
<math>C\cup C'</math> and loops <math>\Inner{R}{R'}_f=L+L'+\Inner{W}{W'}_f</math>.
We say this connection is ''orthogonal'' if <math>\Inner{W}{W'}_f=0</math>, and it is
''modular'' if the ports in <math>I\cup I'</math> are all dangling: a modular connection is
always orthogonal.

We say <math>R_0</math> is a ''subnet'' of <math>R</math>, if there exists a net <math>R'</math> and a connection
<math>(I,I',f)</math> between <math>R_0</math> and <math>R'</math> such that <math>R=R_0\bowtie_f{R'}</math>.

{{Lemma|
Let <math>R_0</math>, <math>R_1</math> and <math>R'</math> be nets such that <math>R_0</math> and <math>R_1</math>
are disjoint from <math>R</math>,
<math>(I,I',f)</math> be a connection <math>\textrm{fp}(R_0)\pinj \textrm{fp}(R')</math>
and <math>\phi</math> an isomorphism <math>R_0\cong R_1</math> such that
<math>\phi(p)=\phi(p')</math> for all <math>p\in\textrm{fp}(R_0)\setminus I</math>.
Then <math>R_0\bowtie_f{R'}</math> is nominally isomorphic to
<math>R_1\bowtie_{f\circ\phi^{-1}}{R'}</math>.
}}

=== Rewriting ===

A ''net rewriting rule'' is a pair <math>(r_0,r_1)</math> of nets
such that <math>\textrm{fp}(r_0)=\textrm{fp}(r_1)</math>. Then an instance of this rule
is a pair <math>(R_0,R_1)</math> such that there exist:

* nets <math>R'_0</math> and <math>R'_1</math> isomorphic to <math>r_0</math> and <math>r_1</math> respectively, namely <math>R'_i=\phi_i(r_i)</math>, so that moreover <math>\phi_0(p)=\phi_1(p)</math> for all <math>p\in\textrm{fp}(r_0)</math> (in particular, <math>\textrm{fp}(R'_0)=\textrm{fp}(R'_1)</math>);
* a net <math>R''</math>, disjoint from <math>R'_0</math> and <math>R'_1</math>;
* a connection <math>(\textrm{fp}(R'_0),J,f):\textrm{fp}(R'_0)\pinj \textrm{fp}(R'')</math>;
such that each <math>R_i</math> is nominally isomorphic to
<math>R'_i\bowtie_f{R''}</math>.

=== Typing ===

A typing system on signature <math>\Sigma</math> is the data of a
set <math>\Theta</math> of types, an involutive negation <math>\cdot\orth:\Theta\to\Theta</math>,
together with a typing discipline for each symbol, ''i.e.'' a relation
<math>\Theta(\sigma)\subseteq\Theta^{\alpha(\sigma)}\times\Theta^{\pi(\sigma)}</math>.
We then write <math>A_1,\cdots,A_{\pi(\sigma)}\vdash_\sigma
B_1,\cdots,B_{\alpha(\sigma)}</math> for <math>(\vec A,\vec B)\in\Theta(\sigma)</math>.

Then a ''typing'' for net <math>R=(P,W,C)</math> is a function <math>\tau:P\to \Theta</math>
such that:

* for all <math>p\in P</math>, <math>\tau(W(p))=\tau(p)\orth</math>;
* for all <math>c\in C</math> of symbol <math>\sigma</math>, <math>\tau(\pi(c))\vdash\tau(\alpha(c))\orth</math>;
where, in the last formula, we implicitly generalized
<math>\tau</math> to lists of ports and <math>\cdot\orth</math> to lists of formulas,
in the obvious, componentwise fashion.
The ''interface'' of the typed net <math>(R,\tau)</math> is then
the restriction of <math>\tau</math> to <math>\textrm{fp}(R)</math>.

The idea, is that a wire <math>\{p,q\}</math> bears the type <math>\tau(q)</math> (resp. <math>\tau(p)</math>) in
the direction <math>(p,q)</math> (resp. <math>(q,p)</math>), so that a rule
<math>\vec A\vdash_\sigma \vec B</math> reads as an inference from passive inputs (hypotheses)
to active outputs (conclusions).

Observe that if <math>(R,\tau)</math> is a typed net, and
<math>\phi:R\cong R'</math> is an isomorphism,
then <math>R'</math> is typed and its interface is <math>\tau\circ\phi^{-1}</math>:
in particular, if <math>\phi</math> is nominal, <math>R'</math> and <math>R</math> have the same interface.

Now let <math>(R,\tau)</math> and <math>(R',\tau')</math> be typed nets
and <math>(I,I',f)</math> a connection so that <math>\tau'\circ f</math> and
<math>\tau\orth=\cdot\orth\circ\tau</math> coincide on <math>I</math>.
Then this induces a typing of <math>R\bowtie_f{R'}</math>
preserving the interface on the remaining free ports.

=== Boxes ===

A signature with boxes is the data of a signature <math>\Sigma</math>, together
with a ''box arity'' <math>\beta(\sigma)</math> for all symbol
<math>\sigma\in\Sigma</math>.

Then a net on signature with boxes <math>(\Sigma,\beta)</math> is the same as a net
<math>R</math> on <math>\Sigma</math> plus, for each cell <math>c</math> of <math>R</math>, the data <math>\beta(c)</math> of a
<math>\beta(\sigma_c)</math>-tuple of nets (with boxes) with free ports the
internal ports of <math>c</math>.


= Additive cut rule =


The additive cut rule is:
<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma,A\vdash\Delta}
\LabelRule{\rulename{cut\;add}}
\BinRule{\Gamma\vdash\Delta}
\DisplayProof
</math>

In contrary to what happens in classical logic, this rule is '''not''' admissible in linear logic.

The formula <math>\alpha\plus\alpha\orth</math> is not provable in linear logic, while it is derivable with the additive cut rule:

<math>
\NulRule{\alpha\vdash\alpha}
\UnaRule{\vdash\alpha,\alpha\orth}
\LabelRule{\plus_{R2}}
\UnaRule{\vdash\alpha,\alpha\plus\alpha\orth}
\NulRule{\alpha\vdash\alpha}
\LabelRule{\plus_{R1}}
\UnaRule{\alpha\vdash\alpha\plus\alpha\orth}
\LabelRule{\rulename{cut\;add}}
\BinRule{\vdash\alpha\plus\alpha\orth}
\DisplayProof
</math>



= Categorical semantics =


Constructing denotational models of linear can be a tedious work. Categorical semantics are useful to identify the fundamental structure of these models, and thus simplify and make more abstract the elaboration of those models.

 TODO: why categories? how to extract categorical models? etc.

See <ref>{{BibEntry|bibtype=book|author=MacLane, Saunders|title=Categories for the Working Mathematician|publisher=Springer Verlag|year=1971|volume=5|series=Graduate Texts in Mathematics}}</ref>for a more detailed introduction to category theory. See <ref>{{BibEntry|bibtype=book|author=Melliès, Paul-André|title=Categorical Semantics of Linear Logic}}</ref>for a detailed treatment of categorical semantics of linear logic.

== Basic category theory recalled ==
{{Definition|title=Category|
}}

{{Definition|title=Functor|
}}

{{Definition|title=Natural transformation|
}}

{{Definition|title=Adjunction|
}}

{{Definition|title=Monad|
}}

== Overview ==

In order to interpret the various [[fragment]]s of linear logic, we define incrementally what structure we need in a categorical setting.

* The most basic underlying structure are '''symmetric monoidal categories''' which model the symmetric tensor <math>\otimes</math> and its unit <math>1</math>. 
* The <math>\otimes, \multimap</math> fragment ([[IMLL]]) is captured by so-called '''symmetric monoidal closed categories'''.
* Upgrading to [[ILL]], that is, adding the exponential <math>\oc</math> modality to IMLL requires modelling it categorically. There are various ways to do so: using rich enough '''adjunctions''', or with an ad-hoc definition of a well-behaved comonad which leads to '''linear categories''' and close relatives.
* Dealing with the additives <math>\with, \oplus</math> is quite easy, as they are plain '''cartesian product''' and '''coproduct''', usually defined through universal properties in category theory.
* Retrieving <math>\parr</math>, <math>\bot</math> and <math>\wn</math> is just a matter of dualizing <math>\otimes</math>, <math>1</math> and <math>\oc</math>, thus requiring the model to be a '''*-autonomous category''' for that purpose.

== Modeling [[IMLL]] ==

A model of [[IMLL]] is a ''closed symmetric monoidal category''. We recall the definition of these categories below.

{{Definition|title=Monoidal category|
A ''monoidal category'' <math>(\mathcal{C},\otimes,I,\alpha,\lambda,\rho)</math> is a category <math>\mathcal{C}</math> equipped with
* a functor <math>\otimes:\mathcal{C}\times\mathcal{C}\to\mathcal{C}</math> called ''tensor product'',
* an object <math>I</math> called ''unit object'',
* three natural isomorphisms <math>\alpha</math>, <math>\lambda</math> and <math>\rho</math>, called respectively ''associator'', ''left unitor'' and ''right unitor'', whose components are
: <math>
\alpha_{A,B,C}:(A\otimes B)\otimes C\to A\otimes (B\otimes C)
\qquad
\lambda_A:I\otimes A\to A
\qquad
\rho_A:A\otimes I\to A
</math>
such that
* for every objects <math>A,B,C,D</math> in <math>\mathcal{C}</math>, the diagram
:<math>
\xymatrix{
    ((A\otimes B)\otimes C)\otimes D\ar[d]_{\alpha_{A\otimes B,C,D}}\ar[r]^{\alpha_{A,B,C}\otimes D}&amp;(A\otimes(B\otimes C))\otimes D\ar[r]^{\alpha_{A,B\otimes C,D}}&amp;A\otimes((B\otimes C)\otimes D)\ar[d]^{A\otimes\alpha_{B,C,D}}\\
    (A\otimes B)\otimes(C\otimes D)\ar[rr]_{\alpha_{A,B,C\otimes D}}&amp;&amp;A\otimes(B\otimes (C\otimes D))
}
</math>
commutes,
* for every objects <math>A</math> and <math>B</math> in <math>\mathcal{C}</math>, the diagram
:<math>
\xymatrix{
    (A\otimes I)\otimes B\ar[dr]_{\rho_A\otimes B}\ar[rr]^{\alpha_{A,I,B}}&amp;&amp;\ar[dl]^{A\otimes\lambda_B}A\otimes(I\otimes B)\\
    &amp;A\otimes B&amp;
}
</math>
commutes.
}}

{{Definition|title=Braided, symmetric monoidal category|
A ''braided'' monoidal category is a category together with a natural isomorphism of components
:<math>\gamma_{A,B}:A\otimes B\to B\otimes A</math>
called ''braiding'', such that the two diagrams
:<math>
\xymatrix{
&amp;A\otimes(B\otimes C)\ar[r]^{\gamma_{A,B\otimes C}}&amp;(B\otimes C)\otimes A\ar[dr]^{\alpha_{B,C,A}}\\
(A\otimes B)\otimes C\ar[ur]^{\alpha_{A,B,C}}\ar[dr]_{\gamma_{A,B}\otimes C}&amp;&amp;&amp;B\otimes (C\otimes A)\\
&amp;(B\otimes A)\otimes C\ar[r]_{\alpha_{B,A,C}}&amp;B\otimes(A\otimes C)\ar[ur]_{B\otimes\gamma_{A,C}}\\
}
</math>
and
:<math>
\xymatrix{
&amp;(A\otimes B)\otimes C\ar[r]^{\gamma_{A\otimes B,C}}&amp;C\otimes (A\otimes B)\ar[dr]^{\alpha^{-1}_{C,A,B}}&amp;\\
A\otimes (B\otimes C)\ar[ur]^{\alpha^{-1}_{A,B,C}}\ar[dr]_{A\otimes\gamma_{B,C}}&amp;&amp;&amp;(C\otimes A)\otimes B\\
&amp;A\otimes(C\otimes B)\ar[r]_{\alpha^{-1}_{A,C,B}}&amp;(A\otimes C)\otimes B\ar[ur]_{\gamma_{A,C}\otimes B}&amp;\\
}
</math>
commute for every objects <math>A</math>, <math>B</math> and <math>C</math>.

A ''symmetric'' monoidal category is a braided monoidal category in which the braiding satisfies
:<math>\gamma_{B,A}\circ\gamma_{A,B}=A\otimes B</math>
for every objects <math>A</math> and <math>B</math>.
}}

{{Definition|title=Closed monoidal category|
A monoidal category <math>(\mathcal{C},\tens,I)</math> is ''left closed'' when for every object <math>A</math>, the functor
:<math>B\mapsto A\otimes B</math>
has a right adjoint, written
:<math>B\mapsto(A\limp B)</math>
This means that there exists a bijection
:<math>\mathcal{C}(A\tens B, C) \cong \mathcal{C}(B,A\limp C)</math>
which is natural in <math>B</math> and <math>C</math>.
Equivalently, a monoidal category is left closed when it is equipped with a ''left closed structure'', which consists of
* an object <math>A\limp B</math>,
* a morphism <math>\mathrm{eval}_{A,B}:A\tens (A\limp B)\to B</math>, called ''left evaluation'',
for every objects <math>A</math> and <math>B</math>, such that for every morphism <math>f:A\otimes X\to B</math> there exists a unique morphism <math>h:X\to A\limp B</math> making the diagram
:<math>
\xymatrix{
A\tens X\ar@{.>}[d]_{A\tens h}\ar[dr]^{f}\\
A\tens(A\limp B)\ar[r]_-{\mathrm{eval}_{A,B}}&amp;B
}
</math>
commute.

Dually, the monoidal category <math>\mathcal{C}</math> is ''right closed'' when the functor <math>B\mapsto B\otimes A</math> admits a right adjoint. The notion of ''right closed structure'' can be defined similarly.
}}

In a symmetric monoidal category, a left closed structure induces a right closed structure and conversely, allowing us to simply speak of a ''closed symmetric monoidal category''.

== Modeling the additives ==
{{Definition|title=Product|
A ''product'' <math>(X,\pi_1,\pi_2)</math> of two coinitial morphisms <math>f:A\to B</math> and <math>g:A\to C</math> in a category <math>\mathcal{C}</math> is an object <math>X</math> of <math>\mathcal{C}</math> together with two morphisms <math>\pi_1:X\to A</math> and <math>\pi_2:X\to B</math> such that there exists a unique morphism <math>h:A\to X</math> making the diagram
:<math>
\xymatrix{
&amp;\ar[ddl]_fA\ar@{.>}[d]_h\ar[ddr]^g&amp;\\
&amp;\ar[dl]^{\pi_1}X\ar[dr]_{\pi_2}&amp;\\
B&amp;&amp;C
}
</math>
commute.
}}

A category has ''finite products'' when it has products and a terminal object.

{{Definition|title=Monoid|
A ''monoid'' <math>(M,\mu,\eta)</math> in a monoidal category <math>(\mathcal{C},\tens,I)</math> is an object <math>M</math> together with two morphisms
:<math>\mu:M\tens M \to M</math> and <math>\eta:I\to M</math>
such that the diagrams
:<math>
\xymatrix{
&amp;(M\tens M)\tens M\ar[dl]_{\alpha_{M,M,M}}\ar[r]^-{\mu\tens M}&amp;M\tens M\ar[dd]^{\mu}\\
M\tens(M\tens M)\ar[d]_{M\tens\mu}&amp;&amp;\\
M\tens M\ar[rr]_{\mu}&amp;&amp;M\\
}
</math>
and
:<math>
\xymatrix{
I\tens M\ar[r]^{\eta\tens M}\ar[dr]_{\lambda_M}&amp;M\tens M\ar[d]_\mu&amp;\ar[l]_{M\tens\eta}\ar[dl]^{\rho_M}M\tens I\\
&amp;M&amp;
}
</math>
commute.
}}

{{Property|
Categories with products vs monoidal categories.
}}

== Modeling [[ILL]] ==

Introduced in<ref>{{BibEntry|type=journal|author=Benton, Nick|title=A Mixed Linear and Non-Linear Logic: Proofs, Terms and Models.|journal=CSL'94|volume=933|year=1995}}</ref>.

{{Definition|title=Linear-non linear (LNL) adjunction|
A ''linear-non linear adjunction'' is a symmetric monoidal adjunction between lax monoidal functors
:<math>
\xymatrix{
(\mathcal{M},\times,\top)\ar@/^/[rr]^{(L,l)}&amp;\bot&amp;\ar@/^/[ll]^{(M,m)}(\mathcal{L},\otimes,I)
}
</math>
in which the category <math>\mathcal{M}</math> has finite products.
}}

:<math>\oc=L\circ M</math>

This section is devoted to defining the concepts necessary to define these adjunctions.

{{Definition|title=Monoidal functor|
A ''lax monoidal functor'' <math>(F,f)</math> between two monoidal categories <math>(\mathcal{C},\tens,I)</math> and <math>(\mathcal{D},\bullet,J)</math> consists of
* a functor <math>F:\mathcal{C}\to\mathcal{D}</math> between the underlying categories,
* a natural transformation <math>f</math> of components <math>f_{A,B}:FA\bullet FB\to F(A\tens B)</math>,
* a morphism <math>f:J\to FI</math>
such that the diagrams
:<math>
\xymatrix{
    (FA\bullet FB)\bullet FC\ar[d]_{\phi_{A,B}\bullet FC}\ar[r]^{\alpha_{FA,FB,FC}}&amp;FA\bullet(FB\bullet FC)\ar[dr]^{FA\bullet\phi_{B,C}}\\
    F(A\otimes B)\bullet FC\ar[dr]_{\phi_{A\otimes B,C}}&amp;&amp;FA\bullet F(B\otimes C)\ar[d]^{\phi_{A,B\otimes C}}\\
    &amp;F((A\otimes B)\otimes C)\ar[r]_{F\alpha_{A,B,C}}&amp;F(A\otimes(B\otimes C))
}
</math>
and
:<math>
\xymatrix{
    FA\bullet J\ar[d]_{\rho_{FA}}\ar[r]^{FA\bullet\phi}&amp;FA\bullet FI\ar[d]^{\phi_{A,I}}\\
    FA&amp;\ar[l]^{F\rho_A}F(A\otimes I)
}
</math> and <math>
\xymatrix{
    J\bullet FB\ar[d]_{\lambda_{FB}}\ar[r]^{\phi\bullet FB}&amp;FI\bullet FB\ar[d]^{\phi_{I,B}}\\
    FB&amp;\ar[l]^{F\lambda_B}F(I\otimes B)
}
</math>
commute for every objects <math>A</math>, <math>B</math> and <math>C</math> of <math>\mathcal{C}</math>. The morphisms <math>f_{A,B}</math> and <math>f</math> are called ''coherence maps''.

A lax monoidal functor is ''strong'' when the coherence maps are invertible and ''strict'' when they are identities.
}}

{{Definition|title=Monoidal natural transformation|
Suppose that <math>(\mathcal{C},\tens,I)</math> and <math>(\mathcal{D},\bullet,J)</math> are two monoidal categories and
:<math>
(F,f):(\mathcal{C},\tens,I)\Rightarrow(\mathcal{D},\bullet,J)
</math> and <math>
(G,g):(\mathcal{C},\tens,I)\Rightarrow(\mathcal{D},\bullet,J)
</math>
are two monoidal functors between these categories. A ''monoidal natural transformation'' <math>\theta:(F,f)\to (G,g)</math> between these monoidal functors is a natural transformation <math>\theta:F\Rightarrow G</math> between the underlying functors such that the diagrams
:<math>
\xymatrix{
    FA\bullet FB\ar[d]_{f_{A,B}}\ar[r]^{\theta_A\bullet\theta_B}&amp;\ar[d]^{g_{A,B}}GA\bullet GB\\
    F(A\tens B)\ar[r]_{\theta_{A\tens B}}&amp;G(A\tens B)
}
</math> and <math>
\xymatrix{
  &amp;\ar[dl]_{f}J\ar[dr]^{g}&amp;\\
  FI\ar[rr]_{\theta_I}&amp;&amp;GI
}
</math>
commute for every objects <math>A</math> and <math>B</math> of <math>\mathcal{D}</math>.
}}

{{Definition|title=Monoidal adjunction|
A ''monoidal adjunction'' between two monoidal functors
:<math>
(F,f):(\mathcal{C},\tens,I)\Rightarrow(\mathcal{D},\bullet,J)
</math> and <math>
(G,g):(\mathcal{D},\bullet,J)\Rightarrow(\mathcal{C},\tens,I)
</math>
is an adjunction between the underlying functors <math>F</math> and <math>G</math> such that the unit and the counit
:<math>\eta:\mathcal{C}\Rightarrow G\circ F</math> and <math>\varepsilon:F\circ G\Rightarrow\mathcal{D}</math>
induce monoidal natural transformations between the corresponding monoidal functors.
}}

== Modeling negation ==

=== *-autonomous categories ===

{{Definition|title=*-autonomous category|
Suppose that we are given a symmetric monoidal closed category <math>(\mathcal{C},\tens,I)</math> and an object <math>R</math> of <math>\mathcal{C}</math>. For every object <math>A</math>, we define a morphism
:<math>\partial_{A}:A\to(A\limp R)\limp R</math>
as follows. By applying the bijection of the adjunction defining (left) closed monoidal categories to the identity morphism <math>\mathrm{id}_{A\limp R}:A\limp R \to A\limp R</math>, we get a morphism <math>A\tens (A\limp R)\to R</math>, and thus a morphism <math>(A\limp R)\tens A\to R</math> by precomposing with the symmetry <math>\gamma_{A\limp R,A}</math>. The morphism <math>\partial_A</math> is finally obtained by applying the bijection of the adjunction defining (left) closed monoidal categories to this morphism. The object <math>R</math> is called ''dualizing'' when the morphism <math>\partial_A</math> is a bijection for every object <math>A</math> of <math>\mathcal{C}</math>. A symmetric monoidal closed category is ''*-autonomous'' when it admits such a dualizing object.
}}

=== Compact closed categories ===

{{Definition|title=Dual objects|
A ''dual object'' structure <math>(A,B,\eta,\varepsilon)</math> in a monoidal category <math>(\mathcal{C},\tens,I)</math> is a pair of objects <math>A</math> and <math>B</math> together with two morphisms
:<math>\eta:I\to B\otimes A</math> and <math>\varepsilon:A\otimes B\to I</math>
such that the diagrams
:<math>
\xymatrix{
&amp;A\tens(B\tens A)\ar[r]^{\alpha_{A,B,A}^{-1}}&amp;(A\tens B)\tens A\ar[dr]^{\varepsilon\tens A}\\
A\tens I\ar[ur]^{A\tens\eta}&amp;&amp;&amp;I\tens A\ar[d]^{\lambda_A}\\
A\ar[u]^{\rho_A^{-1}}\ar@{=}[rrr]&amp;&amp;&amp;A\\
}
</math>
and
:<math>
\xymatrix{
&amp;(B\tens A)\tens B\ar[r]^{\alpha_{B,A,B}}&amp;B\tens(A\tens B)\ar[dr]^{B\tens\varepsilon}\\
I\tens B\ar[ur]^{\eta\tens B}&amp;&amp;&amp;B\tens I\ar[d]^{\rho_B}\\
B\ar[u]^{\lambda_B^{-1}}\ar@{=}[rrr]&amp;&amp;&amp;B\\
}
</math>
commute. The object <math>A</math> is called a left dual of <math>B</math> (and conversely <math>B</math> is a right dual of <math>A</math>).
}}

{{Lemma|
Two left (resp. right) duals of a same object <math>B</math> are necessarily isomorphic.
}}

{{Definition|title=Compact closed category|
A symmetric monoidal category <math>(\mathcal{C},\tens,I)</math> is ''compact closed'' when every object <math>A</math> has a right dual <math>A^*</math>. We write
:<math>\eta_A:I\to A^*\tens A</math> and <math>\varepsilon_A:A\tens A^*\to I</math>
for the corresponding duality morphisms.
}}

{{Lemma|
In a compact closed category the left and right duals of an object <math>A</math> are isomorphic.
}}

{{Property|
A compact closed category <math>\mathcal{C}</math> is monoidal closed, with closure defined by
:<math>\mathcal{C}(A\tens B,C)\cong\mathcal{C}(B,A^*\tens C)</math>
}}
{{Proof|
To every morphism <math>f:A\tens B\to C</math>, we associate a morphism <math>\ulcorner f\urcorner:B\to A^*\tens C</math> defined as
:<math>
\xymatrix{
B\ar[r]^-{\lambda_B^{-1}}&amp;I\tens B\ar[r]^-{\eta_A\tens B}&amp;(A^*\tens A)\tens B\ar[r]^-{\alpha_{A^*,A,B}}&amp;A^*\tens(A\tens B)\ar[r]^-{A^*\tens f}&amp;A\tens C\\
}
</math>
and to every morphism <math>g:B\to A^*\tens C</math>, we associate a morphism <math>\llcorner g\lrcorner:A\tens B\to C</math> defined as
:<math>
\xymatrix{
A\tens B\ar[r]^-{A\tens g}&amp;A\tens(A^*\tens C)\ar[r]^-{\alpha_{A,A^*,C}^{-1}}&amp;(A\tens A^*)\tens C\ar[r]^-{\varepsilon_A\tens C}&amp;I\tens C\ar[r]^-{\lambda_C}&amp;C
}
</math>
It is easy to show that <math>\llcorner \ulcorner f\urcorner\lrcorner=f</math> and <math>\ulcorner\llcorner g\lrcorner\urcorner=g</math> from which we deduce the required bijection.
}}

{{Property|
A compact closed category is a (degenerated) *-autonomous category, with the obvious duality structure. In particular, <math>(A \otimes B)^* \cong A^*\otimes B^*</math>.
}}

{{Remark|The above isomorphism does not hold in *-autonomous categories in general. This means that models which are compact closed categories identify <math>\otimes</math> and <math>\parr</math> as well as <math>1</math> and <math>\bot</math>.}}

{{Proof|
The dualizing object <math>R</math> is simply <math>I^*</math>.

For any <math>A</math>, the reverse isomorphism <math>\delta_A : (A \multimap R)\multimap R \rightarrow A</math> is constructed as follows:

<math>\mathcal{C}((A \multimap R)\multimap R, A) := \mathcal{C}((A \otimes I^{**})\otimes I^{**}, A) \cong \mathcal{C}((A \otimes I)\otimes I, A) \cong \mathcal{C}(A, A)</math>

Identity on <math>A</math> is taken as the canonical morphism required.

}}

== Other categorical models ==

=== Lafont categories ===

=== Seely categories ===

=== Linear categories ===

== Properties of categorical models ==

=== The Kleisli category ===



= Coherent semantics =


''Coherent semantics'' was invented by Girard in the paper ''The system F, 15 years later''<ref>{{BibEntry|bibtype=journal|author=Girard, Jean-Yves|title=The System F of Variable Types, Fifteen Years Later|journal=Theoretical Computer Science|volume=45|issue=2|pages=159-192|doi=10.1016/0304-3975(86)90044-7|year=1986}}</ref>with the objective of building a denotationnal interpretation of second order intuitionnistic logic (aka polymorphic lambda-calculus).

Coherent semantics is based on the notion of ''stable functions'' that was initially proposed by Gérard Berry. Stability is a condition on Scott continuous functions that expresses the determinism of the relation between the output and the input: the typical Scott continuous but non stable function is the ''parallel or'' because when the two inputs are both set to '''true''', only one of them is the reason why the result is '''true''' but there is no way to determine which one.

A further achievement of coherent semantics was that it allowed to endow the set of stable functions from <math>X</math> to <math>Y</math> with a structure of domain, thus closing the category of coherent spaces and stable functions. However the most interesting point was the discovery of a special class of stable functions, ''linear functions'', which was the first step leading to Linear Logic.

== The cartesian closed structure of coherent semantics ==

There are three equivalent definitions of coherent spaces: the first one, ''coherent spaces as domains'', is interesting from a historical point of view as it emphazises the fact that coherent spaces are particular cases of Scott domains. The second one, ''coherent spaces as graphs'', is the most commonly used and will be our &quot;official&quot; definition in the sequel. The last one, ''cliqued spaces'' is a particular example of a more general scheme that one could call &quot;symmetric reducibility&quot;; this scheme is underlying lots of constructions in linear logic such as [[phase semantics]] or the proof of strong normalisation for proof-nets.

=== Coherent spaces ===

A coherent space <math>X</math> is a collection of subsets of a set <math>\web X</math> satisfying some conditions that will be detailed shortly. The elements of <math>X</math> are called the ''cliques'' of <math>X</math> (for reasons that will be made clear in a few lines). The set <math>\web X</math> is called the ''web'' of <math>X</math> and its elements are called the ''points'' of <math>X</math>; thus a clique is a set of points. Note that the terminology is a bit ambiguous as the points of <math>X</math> are the elements of the web of <math>X</math>, not the elements of <math>X</math>.

The definitions below give three equivalent conditions that have to be satisfied by the cliques of a coherent space.

==== As domains ====

The cliques of <math>X</math> have to satisfy:
* subset closure: if <math>x\subset y\in X</math> then <math>x\in X</math>,
* singletons: <math>\{a\}\in X</math> for <math>a\in\web X</math>.
* binary compatibility: if <math>A</math> is a family of pairwise compatible cliques of <math>X</math>, that is if <math>x\cup y\in X</math> for any <math>x,y\in A</math>, then <math>\bigcup A\in X</math>.

A coherent space is thus ordered by inclusion; one easily checks that it is a domain.  In particular finite cliques of <math>X</math> correspond to compact elements.

====  As graphs ====

There is a reflexive and symetric relation <math>\coh_X</math> on <math>\web X</math> (the ''coherence relation'') such that any subset <math>x</math> of <math>\web X</math>  is a clique of <math>X</math> iff <math>\forall a,b\in x,\, a\coh_X b</math>. In other terms <math>X</math> is the set of complete subgraphs of the simple unoriented graph of the <math>\coh_X</math> relation; this is the reason why elements of <math>X</math> are called ''cliques''.

The ''strict coherence relation'' <math>\scoh_X</math> on <math>X</math> is defined by: <math>a\scoh_X b</math> iff <math>a\neq b</math> and <math>a\coh_X b</math>.

A coherent space in the domain sense is seen to be a coherent space in the graph sense by setting <math>a\coh_X b</math> iff <math>\{a,b\}\in X</math>; conversely one can check that cliques in the graph sense are subset closed and satisfy the binary compatibility condition.

A coherent space is completely determined by its web and its coherence relation, or equivalently by its web and its strict coherence.

==== As cliqued spaces ====

{{Definition|title=Duality|
Let <math>x, y\subseteq \web{X}</math> be two sets. We will say that they are dual, written <math>x\perp y</math> if their intersection contains at most one element: <math>\mathrm{Card}(x\cap y)\leq 1</math>. As usual, it defines an [[orthogonality relation]] over <math>\powerset{\web{X}}</math>.}}

The last way to express the conditions on the cliques of a coherent space <math>X</math> is simply to say that we must have <math>X\biorth = X</math>.

==== Equivalence of definitions ====

Let <math>X</math> be a cliqued space and define a relation on <math>\web X</math> by setting <math>a\coh_X b</math> iff there is <math>x\in X</math> such that <math>a, b\in x</math>. This relation is obviously symetric; it is also reflexive because all singletons belong to <math>X</math>: if <math>a\in \web X</math> then <math>\{a\}</math> is dual to any element of <math>X\orth</math> (actually <math>\{a\}</math> is dual to any subset of <math>\web X</math>), thus <math>\{a\}</math> is in <math>X\biorth</math>, thus in <math>X</math>.

Let <math>a\coh_X b</math>. Then <math>\{a,b\}\in X</math>; indeed there is an <math>x\in X</math> such that <math>a, b\in x</math>. This <math>x</math> is dual to any <math>y\in X\orth</math>, that is meets any <math>y\in X\orth</math> in a most one point. Since <math>\{a,b\}\subset x</math> this is also true of <math>\{a,b\}</math>, so that <math>\{a,b\}</math> is in <math>X\biorth</math> thus in <math>X</math>.

Now let <math>x</math> be a clique for <math>\coh_X</math> and <math>y</math> be an element of <math>X\orth</math>. Suppose <math>a, b\in x\cap y</math>, then since <math>a</math> and <math>b</math> are coherent (by hypothesis on <math>x</math>) we have <math>\{a,b\}\in X</math> and since <math>y\in X\orth</math> we must have that <math>\{a,b\}</math> and <math>y</math> meet in at most one point. Thus <math>a = b</math> and we have shown that <math>x</math> and <math>y</math> are dual. Since <math>y</math> was arbitrary this means that <math>x</math> is in <math>X\biorth</math>, thus in <math>X</math>. Finally we get that any set of pairwise coherent points of <math>X</math> is in <math>X</math>. Conversely given <math>x\in X</math> its points are obviously pairwise coherent so eventually we get that <math>X</math> is a coherent space in the graph sense.

Conversely given a coherent space <math>X</math> in the graph sense, one can check that it is a cliqued space. Call ''anticlique'' a set <math>y\subset \web X</math> of pairwise incoherent points: for all <math>a, b</math> in <math>y</math>, if <math>a\coh_X b</math> then <math>a=b</math>. Any anticlique intersects any clique in at most one point: let <math>x</math> be a clique and <math>y</math> be an anticlique, then if <math>a,b\in x\cap y</math>, since <math>a, b\in x</math> we have <math>a\coh_X b</math> and since <math>y</math> is an anticlique we have <math>a = b</math>. Thus <math>y\in X\orth</math>. Conversely given any <math>y\in X\orth</math> and <math>a, b\in y</math>, suppose <math>a\coh_X b</math>. Then <math>\{a,b\}\in X</math>, thus <math>\{a,b\}\perp y</math> which entails that <math>\{a, b\}</math> has at most one point so that <math>a = b</math>: we have shown that any two elements of <math>y</math> are incoherent.

Thus the collection of anticliques of <math>X</math> is the dual <math>X\orth</math> of <math>X</math>. Note that the incoherence relation defined above is reflexive and symetric, so that <math>X\orth</math> is a coherent space in the graph sense. Thus we can do for <math>X\orth</math> exactly what we've just done for <math>X</math> and consider the anti-anticliques, that is the anticliques for the incoherent relation which are the cliques for the in-incoherent relation. It is not difficult to see that this  in-incoherence relation is just the coherence relation we started with; we thus obtain that <math>X\biorth = X</math>, so that <math>X</math> is a cliqued space.

=== Stable functions ===

{{Definition|title=Stable function|
Let <math>X</math> and <math>Y</math> be two coherent spaces. A function <math>F:X\longrightarrow Y</math> is ''stable'' if it satisfies:
* it is non decreasing: for any <math>x,y\in X</math> if <math>x\subset y</math> then <math>F(x)\subset F(y)</math>;
* it is continuous (in the Scott sense): if <math>A</math> is a directed family of cliques of <math>X</math>, that is if for any <math>x,y\in A</math> there is a <math>z\in A</math> such that <math>x\cup y\subset z</math>, then <math>\bigcup_{x\in A}F(x) = F(\bigcup A)</math>;
* it satisfies the stability condition: if <math>x,y\in X</math> are compatible, that is if <math>x\cup y\in X</math>, then <math>F(x\cap y) = F(x)\cap F(y)</math>.
}}

This definition is admitedly not very tractable. An equivalent and most useful caracterisation of stable functions is given by the following theorem.

{{Theorem|
Let <math>F:X\longrightarrow Y</math> be a non-decreasing function from the coherent space <math>X</math> to the coherent space <math>Y</math>. The function <math>F</math> is stable iff it satisfies: for any <math>x\in X</math>, <math>b\in\web Y</math>, if <math>b\in F(x)</math> then there is a finite clique <math>x_0\subset x</math> such that:
* <math>b\in F(x_0)</math>,
* for any <math>y\subset x</math> if <math>b\in F(y)</math> then <math>x_0\subset y</math> (<math>x_0</math> is ''the'' minimum sub-clique of <math>x</math> such that <math>b\in F(x_0)</math>). 
}}

Note that the stability condition doesn't depend on the coherent space structure and can be expressed more generally for continuous functions on domains. However, as mentionned in the introduction, the restriction to coherent spaces allows to endow the set of stable functions from <math>X</math> to <math>Y</math> with a structure of coherent space.

{{Definition|title=The space of stable functions|
Let <math>X</math> and <math>Y</math> be coherent spaces. We denote by <math>X_{\mathrm{fin}}</math> the set of ''finite'' cliques of <math>X</math>. The function space <math>X\imp Y</math> is defined by:
* <math>\web{X\imp Y} = X_{\mathrm{fin}}\times \web Y</math>,
* <math>(x_0, a)\coh_{X\imp Y}(y_0, b)</math> iff <math>\begin{cases}\text{if } x_0\cup y_0\in X\text{ then } a\coh_Y b,\\
                                                                            \text{if } x_0\cup y_0\in X\text{ and } a = b\text{ then } x_0 = y_0\end{cases}</math>.
}}

One could equivalently define the strict coherence relation on <math>X\imp Y</math> by: <math>(x_0,a)\scoh_{X\imp Y}(y_0, b)</math> iff when <math>x_0\cup y_0\in X</math> then <math>a\scoh_Y b</math> (equivalently <math>x_0\cup y_0\not\in X</math> or <math>a\scoh_Y b</math>).
 
{{Definition|title=Trace of a stable function|
Let <math>F:X\longrightarrow Y</math> be a function. The ''trace'' of <math>F</math> is the set:

  <math>\mathrm{Tr}(F) = \{(x_0, b), x_0\text{ minimal such that } b\in F(x_0)\}</math>.
}}

{{theorem|
<math>F</math> is stable iff <math>\mathrm{Tr}(F)</math> is a clique of the function space <math>X\imp Y</math>
}}

In particular the continuity of <math>F</math> entails that if <math>x_0</math> is minimal such that <math>b\in F(x_0)</math>, then <math>x_0</math> is finite.

{{Definition|title=The evaluation function|
Let <math>f</math> be a clique in <math>X\imp Y</math>. We define a function <math>\mathrm{Fun}\,f:X\longrightarrow Y</math> by: <math>\mathrm{Fun}\,f(x) = \{b\in Y,\text{ there is }x_0\subset x\text{ such that }(x_0, b)\in f\}</math>.
}}

{{Theorem|title=Closure|
If <math>f</math> is a clique of the function space <math>X\imp Y</math> then we have <math>\mathrm{Tr}(\mathrm{Fun}\,f) = f</math>. Conversely if <math>F:X\longrightarrow Y</math> is a stable function then we have <math>F = \mathrm{Fun}\,\mathrm{Tr}(F)</math>.
}}

=== Cartesian product ===

{{Definition|title=Cartesian product|
Let <math>X_1</math> and <math>X_2</math> be two coherent spaces. We define the coherent space <math>X_1\with X_2</math> (read <math>X_1</math> ''with'' <math>X_2</math>):
* the web is the disjoint union of the webs: <math>\web{X_1\with X_2} = \{1\}\times\web{X_1}\cup \{2\}\times\web{X_2}</math>;
* the coherence relation is the serie composition of the relations on <math>X_1</math> and <math>X_2</math>: <math>(i, a)\coh_{X_1\with X_2}(j, b)</math> iff either <math>i\neq j</math> or <math>i=j</math> and <math>a\coh_{X_i} b</math>.
}}

This definition is just the way to put a coherent space structure on the cartesian product. Indeed one easily shows the

{{Theorem|
Given cliques <math>x_1</math> and <math>x_2</math> in <math>X_1</math> and <math>X_2</math>, we define the subset <math>\langle x_1, x_2\rangle</math> of <math>\web{X_1\with X_2}</math> by: <math>\langle x_1, x_2\rangle = \{1\}\times x_1\cup \{2\}\times x_2</math>. Then <math>\langle x_1, x_2\rangle</math> is a clique in <math>X_1\with X_2</math>.

Conversely, given a clique <math>x\in X_1\with X_2</math>, for <math>i=1,2</math> we define <math>\pi_i(x) = \{a\in X_i, (i, a)\in x\}</math>. Then <math>\pi_i(x)</math> is a clique in <math>X_i</math> and the function <math>\pi_i:X_1\with X_2\longrightarrow X_i</math> is stable.

Furthemore these two operations are inverse of each other: <math>\pi_i(\langle x_1, x_2\rangle) = x_i</math> and <math>\langle\pi_1(x), \pi_2(x)\rangle = x</math>. In particular any clique in <math>X_1\with X_2</math> is of the form <math>\langle x_1, x_2\rangle</math>.
}}

Altogether the results above (and a few other more that we shall leave to the reader) allow to get:

{{Theorem|
The category of coherent spaces and stable functions is cartesian closed.
}}

In particular this means that if we define <math>\mathrm{Eval}:(X\imp Y)\with X\longrightarrow Y</math> by: <math>\mathrm{Eval}(\langle f, x\rangle) = \mathrm{Fun}\,f(x)</math> then <math>\mathrm{Eval}</math> is stable.

== The monoidal structure of coherent semantics ==

=== Linear functions ===

{{Definition|title=Linear function|
A function <math>F:X\longrightarrow Y</math> is ''linear'' if it is stable and furthemore satisfies: for any family <math>A</math> of pairwise compatible cliques of <math>X</math>, that is such that for any <math>x, y\in A</math>, <math>x\cup y\in X</math>, we have <math>\bigcup_{x\in A}F(x) = F(\bigcup A)</math>.
}}

In particular if we take <math>A</math> to be the empty family, then we have <math>F(\emptyset) = \emptyset</math>.

The condition for linearity is quite similar to the condition for Scott continuity, except that we dropped the constraint that <math>A</math> is ''directed''. Linearity is therefore much stronger than stability: most stable functions are not linear.

However most of the functions seen so far are linear. Typically the function <math>\pi_i:X_1\with X_2\longrightarrow X_i</math> is linear from wich one may deduce that the ''with'' construction is also a cartesian product in the category of coherent spaces and linear functions.

As with stable function we have an equivalent and much more tractable caracterisation of linear function:

{{Theorem|
Let <math>F:X\longrightarrow Y</math> be a continuous function. Then <math>F</math> is linear iff it satisfies: for any clique <math>x\in X</math> and any <math>b\in F(x)</math> there is a unique <math>a\in x</math> such that <math>b\in F(\{a\})</math>.
}}

Just as the caracterisation theorem for stable functions allowed us to build the coherent space of stable functions, this theorem will help us to endow the set of linear maps with a structure of coherent space.

{{Definition|title=The linear functions space|
Let <math>X</math> and <math>Y</math> be coherent spaces. The ''linear function space'' <math>X\limp Y</math> is defined by:
* <math>\web{X\limp Y} = \web X\times \web Y</math>,
* <math>(a,b)\coh_{X\limp Y}(a', b')</math> iff <math>\begin{cases}\text{if }a\coh_X a'\text{ then } b\coh_Y b'\\
                                                                   \text{if }a\coh_X a' \text{ and }b=b'\text{ then }a=a'\end{cases}</math>
}}

Equivalently one could define the strict coherence to be: <math>(a,b)\scoh_{X\limp Y}(a',b')</math> iff <math>a\scoh_X a'</math> entails <math>b\scoh_Y b'</math>.

{{Definition|title=Linear trace|
Let <math>F:X\longrightarrow Y</math> be a function. The ''linear trace'' of <math>F</math> denoted as <math>\mathrm{LinTr}(F)</math> is the set:
  <math>\mathrm{LinTr}(F) = \{(a, b)\in\web X\times\web Y</math> such that <math>b\in F(\{a\})\}</math>.
}}

{{Theorem|
If <math>F</math> is linear then <math>\mathrm{LinTr}(F)</math> is a clique of <math>X\limp Y</math>.
}}

{{Definition|title=Evaluation of linear function|
Let <math>f</math> be a clique of <math>X\limp Y</math>. We define the function <math>\mathrm{LinFun}\,f:X\longrightarrow Y</math> by: <math>\mathrm{LinFun}\,f(x) = \{b\in\web Y</math> such that there is an <math>a\in x</math> satisfying <math>(a,b)\in f\}</math>.
}}

{{Theorem|title=Linear closure|
Let <math>f</math> be a clique in <math>X\limp Y</math>. Then we have <math>\mathrm{LinTr}(\mathrm{LinFun}\, f) = f</math>. Conversely if <math>F:X\longrightarrow Y</math> is linear then we have <math>F = \mathrm{LinFun}\,\mathrm{LinTr}(F)</math>.
}}

It remains to define a tensor product and we will get that the category of coherent spaces with linear functions is monoidal symetric (it is actually *-autonomous).

=== Tensor product ===

{{Definition|title=Tensor product|
Let <math>X</math> and <math>Y</math> be coherent spaces. Their tensor product <math>X\tens Y</math> is defined by: <math>\web{X\tens Y} = \web X\times\web Y</math> and <math>(a,b)\coh_{X\tens Y}(a',b')</math> iff <math>a\coh_X a'</math> and <math>b\coh_Y b'</math>.
}}

{{Theorem|
The category of coherent spaces with linear maps and tensor product is [[Categorical semantics#Modeling IMLL|monoidal symetric closed]].
}}

The closedness is a consequence of the existence of the linear isomorphism:
  <math>\varphi:X\tens Y\limp Z\ \stackrel{\sim}{\longrightarrow}\ X\limp(Y\limp Z)</math>

that is defined by its linear trace: <math>\mathrm{LinTr}(\varphi) = \{(((a, b), c), (a, (b, c))),\, a\in\web X,\, b\in \web Y,\, c\in\web Z\}</math>.

=== Linear negation ===

{{Definition|title=Linear negation|
Let <math>X</math> be a coherent space. We define the ''incoherence relation'' on <math>\web X</math> by: <math>a\incoh_X b</math> iff <math>a\coh_X b</math> entails <math>a=b</math>. The incoherence relation is reflexive and symetric; we call ''dual'' or ''linear negation'' of <math>X</math> the associated coherent space denoted <math>X\orth</math>, thus defined by: <math>\web{X\orth} = \web X</math> and <math>a\coh_{X\orth} b</math> iff <math>a\incoh_X b</math>.
}}

The cliques of <math>X\orth</math> are called the ''anticliques'' of <math>X</math>. As seen in the section on cliqued spaces we have <math>X\biorth=X</math>.

{{Theorem|
The category of coherent spaces with linear maps, tensor product and linear negation is *-autonomous.
}}

This is in particular consequence of the existence of the isomorphism:
  <math>\varphi:X\limp Y\ \stackrel{\sim}{\longrightarrow}\ Y\orth\limp X\orth</math>

defined by its linear trace: <math>\mathrm{LinTr}(\varphi) = \{((a, b), (b, a)),\, a\in\web X,\, b\in\web Y\}</math>.

== Exponentials ==

In linear algebra, bilinear maps may be factorized through the tensor product. Similarly there is a coherent space <math>\oc X</math> that allows to factorize stable functions through linear functions.

{{Definition|title=Of course|
Let <math>X</math> be a coherent space; recall that <math>X_{\mathrm{fin}}</math> denotes the set of finite cliques of <math>X</math>. We define the space <math>\oc X</math> (read ''of course <math>X</math>'') by: <math>\web{\oc X} = X_{\mathrm{fin}}</math> and <math>x_0\coh_{\oc X}y_0</math> iff <math>x_0\cup y_0</math> is a clique of <math>X</math>.
}}

Thus a clique of <math>\oc X</math> is a set of finite cliques of <math>X</math> the union of wich is a clique of <math>X</math>.

{{Theorem|
Let <math>X</math> be a coherent space. Denote by <math>\beta:X\longrightarrow \oc X</math> the stable function whose trace is: <math>\mathrm{Tr}(\beta) = \{(x_0, x_0),\, x_0\in X_{\mathrm{fin}}\}</math>. Then for any coherent space <math>Y</math> and any stable function <math>F: X\longrightarrow Y</math> there is a unique ''linear'' function <math>\bar F:\oc X\longrightarrow Y</math> such that <math>F = \bar F\circ \beta</math>.

Furthermore we have <math>X\imp Y = \oc X\limp Y</math>.
}}

{{Theorem|title=The exponential isomorphism|
Let <math>X</math> and <math>Y</math> be two coherent spaces. Then there is a linear isomorphism:
  <math>\varphi:\oc(X\with Y)\quad\stackrel{\sim}{\longrightarrow}\quad \oc X\tens\oc Y</math>.
}}

The iso <math>\varphi</math> is defined by its trace: <math>\mathrm{Tr}(\varphi) = \{(x_0, (\pi_1(x_0), \pi_2(x_0)), x_0\text{ finite clique of } X\with Y\}</math>. 

This isomorphism, that sends an additive structure (the web of a with is obtained by disjoint union) onto a multiplicative one (the web of a tensor is obtained by cartesian product) is the reason why the of course is called an ''exponential''.

== Dual connectives and neutrals ==

By linear negation all the constructions defined so far (<math>\with, \tens, \oc</math>) have a dual.

=== The direct sum ===

The dual of <math>\with</math> is <math>\plus</math> defined by: <math>X\plus Y = (X\orth\with Y\orth)\orth</math>. An equivalent definition is given by: <math>\web{X\plus Y} = \web{X\with Y} = \{1\}\times \web X \cup \{2\}\times\web Y</math> and <math>(i, a)\coh_{X\plus Y} (j, b)\text{ iff } i = j = 1 \text{ and } a\coh_X b,\text{ or }i = j = 2\text{ and } a\coh_Y b</math>.

{{Theorem|
Let <math>x'</math> be a clique of <math>X\plus Y</math>; then <math>x'</math> is of the form <math>\{i\}\times x</math> where <math>i = 1\text{ and }x\in X</math>, or <math>i = 2\text{ and }x\in Y</math>.

Denote <math>\mathrm{inl}:X\longrightarrow X\plus Y</math> the function defined by <math>\mathrm{inl}(x) = \{1\}\times x</math> and by <math>\mathrm{inr}:Y\longrightarrow X\plus Y</math> the function defined by <math>\mathrm{inr}(x) = \{2\}\times x</math>. Then <math>\mathrm{inl}</math> and <math>\mathrm{inr}</math> are linear.

If <math>F:X\longrightarrow Z</math> and <math>G:Y\longrightarrow Z</math> are ''linear'' functions then the function <math>H:X\plus Y \longrightarrow Z</math> defined by <math>H(\mathrm{inl}(x)) = F(x)</math> and <math>H(\mathrm{inr}(y)) = G(y)</math> is linear.
}}

In other terms <math>X\plus Y</math> is the direct sum of <math>X</math> and <math>Y</math>. Note that in the theorem all functions are ''linear''. Things doesn't work so smoothly for stable functions. Historically it was after noting this defect of coherent semantics w.r.t. the intuitionnistic implication that Girard was leaded to discover linear functions.

=== The par and the why not ===

We now come to the most mysterious constructions of coherent semantics: the duals of the tensor and the of course.

The ''par'' is the dual of the tensor, thus defined by: <math>X\parr Y = (X\orth\tens Y\orth)\orth</math>. From this one can deduce the definition in graph terms: <math>\web{X\parr Y} = \web{X\tens Y} = \web X\times \web Y</math> and <math>(a,b)\scoh_{X\parr Y} (a',b')</math> iff <math>a\scoh_X a'</math> or <math>b\scoh_Y b'</math>. With this definition one sees that we have:

  <math>X\limp Y = X\orth\parr Y</math>

for any coherent spaces <math>X</math> and <math>Y</math>. This equation can be seen as an alternative definition of the par: <math>X\parr Y = X\orth\limp Y</math>.

Similarly the dual of the of course is called ''why not'' defined by: <math>\wn X = (\oc X\orth)\orth</math>. From this we deduce the definition in the graph sense which is a bit tricky: <math>\web{\wn X}</math> is the set of finite anticliques of <math>X</math>, and given two finite anticliques <math>x</math> and <math>y</math> of <math>X</math> we have <math>x\scoh_{\wn X} y</math> iff there is <math>a\in x</math> and <math>b\in y</math> such that <math>a\scoh_X b</math>.

Note that both for the par and the why not it is much more convenient to define the strict coherence than the coherence.

With these two last constructions, the equation between the stable function space, the of course and the linear function space may be written:

  <math>X\imp Y = \wn X\orth\parr Y</math>.

=== One and bottom ===

Depending on the context we denote by <math>\one</math> or <math>\bot</math> the coherent space whose web is a singleton and whose coherence relation is the trivial reflexive relation.

{{Theorem|
<math>\one</math> is neutral for tensor, that is, there is a linear isomorphism <math>\varphi:X\tens\one\ \stackrel{\sim}{\longrightarrow}\ X</math>.

Similarly <math>\bot</math> is neutral for par.
}}

=== Zero and top ===

Depending on the context we denote by <math>\zero</math> or <math>\top</math> the coherent space with empty web.

{{Theorem|
<math>\zero</math> is neutral for the direct sum <math>\plus</math>, <math>\top</math> is neutral for the cartesian product <math>\with</math>.
}}

{{Remark|
It is one of the main defect of coherent semantics w.r.t. linear logic that it identifies the neutrals: in coherent semantics <math>\zero = \top</math> and <math>\one = \bot</math>. However there is no known semantics of LL that solves this problem in a satisfactory way.}}

== After coherent semantics ==

Coherent semantics was an important milestone in the modern theory of logic of programs, in particular because it leaded to the invention of Linear Logic, and more generally because it establishes a strong link between logic and linear algebra; this link is nowadays aknowledged by the customary use of [[Categorical semantics|monoidal categories]] in logic. In some sense coherent semantics is a precursor of many forthcoming works that explore the linear nature of logic as for example [[geometry of interaction]] which interprets proofs by operators or [[finiteness semantics]] which interprets formulas as vector spaces and resulted in [[differential linear logic]]...

Lots of this work have been motivated by the fact that coherent semantics is not complete as a semantics of programs (technically one says that it is not ''fully abstract''). In order to see this, let us firts come back on the origin of the central concept of ''stability'' which as pointed above originated in the study of the sequentiality in programs.

=== Sequentiality ===

Sequentiality is a property that we will not define here (it would diserve its own article). We rely on the intuition that a function of <math>n</math> arguments is sequential if one can determine which of these argument is examined first during the computation. Obviously any function implemented in a functionnal language is sequential; for example the function ''or'' defined à la CAML by:

  <code>or = fun (x, y) -> if x then true else y</code>

examines its argument x first. Note that this may be expressed more abstractly by the property: <math>\mathrm{or}(\bot, x) = \bot</math> for any boolean <math>x</math>: the function ''or'' needs its first argument in order to compute anything. On the other hand we have <math>\mathrm{or}(\mathrm{true}, \bot) = \mathrm{true}</math>: in some case (when the first argument is true), the function doesn't need its second argument at all.

The typical non sequential function is the ''parallel or'' (that one cannot define in a CAML like language).

For a while one may have believed that the stability condition on which coherent semantics is built was enough to capture the notion of ''sequentiality'' of programs. A hint was the already mentionned fact that the ''parallel or'' is not stable. This diserves a bit of explanation.

==== The parallel or is not stable ====

Let <math>B</math> be the coherent space of booleans, also know as the flat domain of booleans: <math>\web B = \{tt, ff\}</math> where <math>tt</math> and <math>ff</math> are two arbitrary distinct objects (for example one may take <math>tt = 0</math> and <math>ff = 1</math>) and for any <math>b_1, b_2\in \web B</math>, define <math>b_1\coh_B b_2</math> iff <math>b_1 = b_2</math>. Then <math>B</math> has exactly three cliques: the empty clique that we shall denote <math>\bot</math>, the singleton <math>\{tt\}</math> that we shall denote <math>T</math> and the singleton <math>\{ff\}</math> that we shall denote <math>F</math>. These three cliques are ordered by inclusion: <math>\bot \leq T, F</math> (we use <math>\leq</math> for <math>\subset</math> to enforce the idea that coherent spaces are domains).

Recall the [[#Cartesian product|definition of the with]], and in particular that any clique of <math>B\with B</math> has the form <math>\langle x, y\rangle</math> where <math>x</math> and <math>y</math> are cliques of <math>B</math>. Thus <math>B\with B</math> has 9 cliques: <math>\langle\bot,\bot\rangle,\ \langle\bot, T\rangle,\ \langle\bot, F\rangle,\ \langle T,\bot\rangle,\ \dots</math> that are ordered by the product order: <math>\langle x,y\rangle\leq \langle x,y\rangle</math> iff <math>x\leq x'</math> and <math>y\leq y'</math>.

With these notations in mind one may define the parallel or by:

<math>
\begin{array}{rcl}
\mathrm{Por} : B\with B &amp;\longrightarrow&amp; B\\
  \langle T,\bot\rangle &amp;\longrightarrow&amp; T\\
  \langle \bot,T\rangle &amp;\longrightarrow&amp; T\\
    \langle F, F\rangle &amp;\longrightarrow&amp; F
\end{array}
</math>

The function is completely determined if we add the assumption that it is non decreasing; for example one must have <math>\mathrm{Por}\langle\bot,\bot\rangle = \bot</math> because the lhs has to be less than both <math>T</math> and <math>F</math> (because <math>\langle\bot,\bot\rangle \leq \langle T,\bot\rangle</math> and <math>\langle\bot,\bot\rangle \leq \langle F,F\rangle</math>).

The function is not stable because <math>\langle T,\bot\rangle \cap \langle \bot, T\rangle = \langle\bot, \bot\rangle</math>, thus <math>\mathrm{Por}(\langle T,\bot\rangle \cap \langle \bot, T\rangle) = \bot</math> whereas <math>\mathrm{Por}\langle T,\bot\rangle \cap \mathrm{Por}\langle \bot, T\rangle = T\cap T = T</math>.

Another way to see this is: suppose <math>x</math> and <math>y</math> are two cliques of <math>B</math> such that <math>tt\in \mathrm{Por}\langle x, y\rangle</math>, which means that <math>\mathrm{Por}\langle x, y\rangle = T</math>; according to the [[#Stable functions|caracterisation theorem of stable functions]], if <math>\mathrm{Por}</math> were stable then there would be a unique minimum <math>x_0</math> included in <math>x</math>, and a unique minimum <math>y_0</math> included in <math>y</math> such that <math>\mathrm{Por}\langle x_0, y_0\rangle = T</math>. This is not the case because both <math>\langle T,\bot\rangle</math> and <math>\langle T,\bot\rangle</math> are minimal such that their value is <math>T</math>.

In other terms, knowing that <math>\mathrm{Por}\langle x, y\rangle = T</math> doesn't tell which of <math>x</math> of <math>y</math> is responsible for that, although we know by the definition of <math>\mathrm{Por}</math> that only one of them is. Indeed the <math>\mathrm{Por}</math> function is not representable in sequential programming languages such as (typed) lambda-calculus.

So the first genuine idea would be that stability caracterises sequentiality; but...

==== The Gustave function is stable ====

The Gustave function, so-called after an old joke, was found by Gérard Berry as an example of a function that is stable but non sequential. It is defined by:

<math>
\begin{array}{rcl}
  B\with B\with B           &amp;\longrightarrow&amp; B\\
  \langle T, F, \bot\rangle &amp;\longrightarrow&amp; T\\
  \langle \bot, T, F\rangle &amp;\longrightarrow&amp; T\\
  \langle F, \bot, T\rangle &amp;\longrightarrow&amp; T\\
  \langle x, y, z\rangle    &amp;\longrightarrow&amp; F
\end{array}
</math>

The last clause is for all cliques <math>x</math>, <math>y</math> and <math>z</math> such that <math>\langle x, y ,z\rangle</math> is incompatible with the three cliques <math>\langle T, F, \bot\rangle</math>, <math>\langle \bot, T, F\rangle</math> and <math>\langle F, \bot, T\rangle</math>, that is such that the union with any of these three cliques is not a clique in <math>B\with B\with B</math>. We shall denote <math>x_1</math>, <math>x_2</math> and <math>x_3</math> these three cliques.

We furthemore assume that the Gustave function is non decreasing, so that we get <math>G\langle\bot,\bot,\bot\rangle = \bot</math>.

We note that <math>x_1</math>, <math>x_2</math> and <math>x_3</math> are pairwise incompatible. From this we can deduce that the Gustave function is stable: typically if <math>G\langle x,y,z\rangle = T</math> then exactly one of the <math>x_i</math>s is contained in <math>\langle x, y, z\rangle</math>.

However it is not sequential because there is no way to determine which of its three arguments is examined first: it is not the first one otherwise we would have <math>G\langle\bot, T, F\rangle = \bot</math> and similarly it is not the second one nor the third one.

In other terms there is no way to implement the Gustave function by a lambda-term (or in any sequential programming language). Thus coherent semantics is not complete w.r.t. lambda-calculus.

The research for a right model for sequentiality was the motivation for lot of
work, ''e.g.'', ''sequential algorithms'' by Gérard Bérry and Pierre-Louis
Currien in the early eighties, that were more recently reformulated as a kind
of [[Game semantics|game model]], and the theory of ''hypercoherent spaces'' by
Antonio Bucciarelli and Thomas Ehrhard.

=== Multiplicative neutrals and the mix rule ===

Coherent semantics is slightly degenerated w.r.t. linear logic because it identifies multiplicative neutrals (it also identifies additive neutrals but that's yet another problem): the coherent spaces <math>\one</math> and <math>\bot</math> are equal.

The first consequence of the identity <math>\one = \bot</math> is that the formula <math>\one\limp\bot</math> becomes provable, and so does the formula <math>\bot</math>. Note that this doesn't entail (as in classical logic or intuitionnistic logic) that linear logic is incoherent because the principle <math>\bot\limp A</math> for any formula <math>A</math> is still not provable.

The equality <math>\one = \bot</math> has also as consequence the fact that <math>\bot\limp\one</math> (or equivalently the formula <math>\one\parr\one</math>) is provable. This principle is also known as the [[Mix|mix rule]]

<math>
\AxRule{\vdash \Gamma}
\AxRule{\vdash \Delta}
\LabelRule{\rulename{mix}}
\BinRule{\vdash \Gamma,\Delta}
\DisplayProof
</math>

as it can be used to show that this rule is admissible:

<math>
\AxRule{\vdash\Gamma}
\LabelRule{\bot_R}
\UnaRule{\vdash\Gamma, \bot}
\AxRule{\vdash\Delta}
\LabelRule{\bot_R}
\UnaRule{\vdash\Delta, \bot}
\BinRule{\vdash \Gamma, \Delta, \bot\tens\bot}
\NulRule{\vdash \one\parr\one}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\Gamma,\Delta}
\DisplayProof
</math>

None of the two principles <math>1\limp\bot</math> and <math>\bot\limp\one</math> are valid in linear logic. To correct this one could extend the syntax of linear logic by adding the mix-rule. This is not very satisfactory as the mix rule violates some principles of [[Polarized linear logic]], typically the fact that as sequent of the form <math>\vdash P_1, P_2</math> where <math>P_1</math> and <math>P_2</math> are positive, is never provable.

On the other hand the mix-rule is valid in coherent semantics so one could try to find some other model that invalidates the mix-rule. For example Girard's Coherent Banach spaces were an attempt to address this issue.


= Equiprovability =


Two formulas <math>A</math> and <math>B</math> are equiprovable, when <math>\vdash A</math> is provable if and only if <math>\vdash B</math> is provable.

* for any <math>A</math> and <math>B</math>, <math>A\tens B</math> and <math>A\with B</math> are equiprovable.
* for any <math>A</math>, <math>A</math>, <math>\oc A</math> and <math>\forall\xi A</math> are equiprovable.


= Finiteness semantics =


The category <math>\mathbf{Fin}</math> of finiteness spaces and finitary relations was introduced by Ehrhard, refining the [[relational semantics|purely relational model of linear logic]]. A finiteness space is a set equipped with a finiteness structure, i.e.  a particular set of subsets which are said to be finitary; and the model is such that the usual relational denotation of a proof in linear logic is always a finitary subset of its conclusion.  By the usual co-Kleisli construction, this also provides a model of the simply typed lambda-calculus: the cartesian closed category <math>\mathbf{Fin}_\oc</math>.

The main property of finiteness spaces is that the intersection of two finitary subsets of dual types is always finite.  This feature allows to reformulate Girard's quantitative semantics in a standard algebraic setting, where morphisms interpreting typed <math>\lambda</math>-terms are analytic functions between the topological vector spaces generated by vectors with finitary supports.  This provided the semantical foundations of Ehrhard-Regnier's differential <math>\lambda</math>-calculus and motivated the general study of a differential extension of linear logic.

It is worth noticing that finiteness spaces can accomodate typed <math>\lambda</math>-calculi only: for instance, the relational semantics of fixpoint combinators is never finitary. The whole point of the finiteness construction is actually to reject infinite computations.  Indeed, from a logical point of view, computation is cut elimination: the finiteness structure ensures the intermediate sets involved in the relational interpretation of a cut are all finite. In that sense, the finitary semantics is intrinsically typed.

== Finiteness spaces ==

The construction of finiteness spaces follows a well known pattern.  It is given by the following notion of orthogonality: <math>a\mathrel \bot a'</math> iff <math>a\cap a'</math> is finite. Then one unrolls [[Orthogonality relation|familiar definitions]], as we do in the following paragraphs.

Let <math>A</math> be a set. Denote by <math>\powerset A</math> the powerset of <math>A</math> and by <math>\finpowerset A</math> the set of all finite subsets of <math>A</math>. Let <math>{\mathfrak F} \subseteq \powerset A</math> any set of subsets of <math>A</math>.  We define the pre-dual of <math>{\mathfrak F}</math> in <math>A</math> as <math>{\mathfrak F}^{\bot_{A}}=\left\{a'\subseteq A;\ \forall a\in{\mathfrak F},\ a\cap a'\in\finpowerset A\right\}</math>. In general we will omit the subscript in the pre-dual notation and just write <math>{\mathfrak F}\orth</math>.  For all <math>{\mathfrak F}\subseteq\powerset A</math>, we have the following immediate properties: <math>\finpowerset A\subseteq {\mathfrak F}\orth</math>; <math>{\mathfrak F}\subseteq {\mathfrak F}\biorth</math>; if <math>{\mathfrak G}\subseteq{\mathfrak F}</math>, <math>{\mathfrak F}\orth\subseteq {\mathfrak G}\orth</math>.  By the last two, we get <math>{\mathfrak F}\orth = {\mathfrak F}\triorth</math>. A finiteness structure on <math>A</math> is then a set <math>{\mathfrak F}</math> of subsets of <math>A</math> such that <math>{\mathfrak F}\biorth = {\mathfrak F}</math>.

A finiteness space is a dependant pair <math>{\mathcal A}=\left(\web{\mathcal A},\mathfrak F\left(\mathcal A\right)\right)</math> where <math>\web {\mathcal A}</math> is the underlying set (the web of <math>{\mathcal A}</math>) and <math>\mathfrak F\left(\mathcal A\right)</math> is a finiteness structure on <math>\web {\mathcal A}</math>.  We then write <math>{\mathcal A}\orth</math> for the dual finiteness space: <math>\web {{\mathcal A}\orth} = \web {\mathcal A}</math> and <math>\mathfrak F\left({\mathcal A}\orth\right)=\mathfrak F\left({\mathcal A}\right)^{\bot}</math>.  The elements of <math>\mathfrak F\left(\mathcal A\right)</math> are called the finitary subsets of <math>{\mathcal A}</math>.

===== Example. =====
	For all set <math>A</math>, <math>(A,\finpowerset A)</math> is a finiteness space and <math>(A,\finpowerset A)\orth = (A,\powerset A)</math>.  In particular, each finite set <math>A</math> is the web of exactly one finiteness space: <math>(A,\finpowerset A)=(A,\powerset A)</math>. We introduce the following two: <math>\zero = \zero\orth = \left(\emptyset, \{\emptyset\}\right)</math> and <math>\one = \one\orth = \left(\{\emptyset\}, \{\emptyset, \{\emptyset\}\}\right)</math>.  We also introduce the finiteness space of natural numbers <math>{\mathcal N}</math> by: 	<math>|{\mathcal N}|={\mathbf N}</math> and <math>a\in\mathfrak F\left(\mathcal N\right)</math> iff <math>a</math> is finite.  We write <math>\mathcal O=\{0\}\in\mathfrak F\left({\mathcal N}\right)</math>.

Notice that <math>{\mathfrak F}</math> is a finiteness structure iff it is of the form <math>{\mathfrak G}\orth</math>. It follows that any finiteness structure <math>{\mathfrak F}</math> is downwards closed for inclusion, and closed under finite unions and arbitrary intersections. Notice however that <math>{\mathfrak F}</math> is not closed under directed unions in general: for all <math>k\in{\mathbf N}</math>, write <math>k{\downarrow}=\left\{j;\  j\le k\right\}\in\mathfrak F\left({\mathcal N}\right)</math>; then <math>k{\downarrow}\subseteq k'{\downarrow}</math> as soon as <math>k\le k'</math>, but <math>\bigcup_{k\ge0} k{\downarrow}={\mathbf N}\not\in\mathfrak F\left({\mathcal N}\right)</math>.


=== Multiplicatives ===
For all finiteness spaces <math>{\mathcal A}</math> and <math>{\mathcal B}</math>, we define <math>{\mathcal A} \tens {\mathcal B}</math> by <math>\web {{\mathcal A} \tens {\mathcal B}} = \web{\mathcal A} \times \web{\mathcal B}</math> and <math>\mathfrak F\left({\mathcal A} \tens {\mathcal B}\right) = \left\{a\times b;\ a\in \mathfrak F\left(\mathcal A\right),\ b\in\mathfrak F\left(\mathcal B\right)\right\}\biorth</math>.  It can be shown  that <math>\mathfrak F\left({\mathcal A} \tens {\mathcal B}\right) = \left\{ c \subseteq \web{\mathcal A}\times\web{\mathcal B};\  \left.c\right|_l\in \mathfrak F\left(\mathcal A\right),\ \left.c\right|_r\in\mathfrak F\left(\mathcal B\right)\right\}</math>, where <math>\left.c\right|_l</math> and <math>\left.c\right|_r</math> are the obvious projections. 

Let <math>f\subseteq A \times B</math> be a relation from <math>A</math> to <math>B</math>, we write <math>f\orth=\left\{(\beta,\alpha);\  (\alpha,\beta)\in f\right\}</math>.  For all <math>a\subseteq A</math>, we set <math>f\cdot a = \left\{\beta\in B;\  \exists \alpha\in a,\ (\alpha,\beta)\in f\right\}</math>.  If moreover <math>g\subseteq B \times C</math>, we define <math>g \bullet f = \left\{(\alpha,\gamma)\in A\times C;\  \exists \beta\in B,\ (\alpha,\beta)\in f\wedge(\beta,\gamma)\in g\right\}</math>.  Then, setting <math>{\mathcal A}\limp{\mathcal B} = \left({\mathcal A}\otimes {\mathcal B}\orth\right)\orth</math>, <math>\mathfrak F\left({\mathcal A}\limp{\mathcal B}\right)\subseteq {\web{\mathcal A}\times\web{\mathcal B}}</math> is characterized as follows:

<math>
\begin{align}
		f\in \mathfrak F\left({\mathcal A}\limp{\mathcal B}\right) &amp;\iff \forall a\in \mathfrak F\left({\mathcal A}\right), f\cdot a \in\mathfrak F\left({\mathcal B}\right) \text{ and } \forall b\in \mathfrak F\left({\mathcal B}\orth\right), f\orth\cdot b \in\mathfrak F\left({\mathcal A}\orth\right)
		\\
		&amp;\iff \forall a\in \mathfrak F\left({\mathcal A}\right), f\cdot a \in\mathfrak F\left({\mathcal B}\right) \text{ and } \forall \beta\in \web{{\mathcal B}}, f\orth\cdot \left\{\beta\right\} \in\mathfrak F\left({\mathcal A}\orth\right)
		\\
		&amp;\iff \forall \alpha\in \web{{\mathcal A}}, f\cdot \left\{\alpha\right\} \in\mathfrak F\left({\mathcal B}\right) \text{ and } \forall b\in \mathfrak F\left({\mathcal B}\orth\right), f\orth\cdot b \in\mathfrak F\left({\mathcal A}\orth\right)
\end{align}
</math>

The elements of <math>\mathfrak F\left({\mathcal A}\limp{\mathcal B}\right)</math> are called finitary relations from <math>{\mathcal A}</math> to <math>{\mathcal B}</math>. By the previous characterization, the identity relation <math>\mathsf{id}_{{\mathcal A}} = \left\{(\alpha,\alpha);\  \alpha\in\web{{\mathcal A}}\right\}</math> is finitary, and the composition of two finitary relations is also finitary. One can thus define the category <math>\mathbf{Fin}</math> of finiteness spaces and finitary relations: the objects of <math>\mathbf{Fin}</math> are all finiteness spaces, and <math>\mathbf{Fin}({\mathcal A},{\mathcal B})=\mathfrak F\left({\mathcal A}\limp{\mathcal B}\right)</math>.  Equipped with the tensor product <math>\tens</math>, <math>\mathbf{Fin}</math> is symmetric monoidal, with unit <math>\one</math>; it is monoidal closed by the definition of <math>\limp</math>; it is <math>*</math>-autonomous by the obvious isomorphism between <math>{\mathcal A}\orth</math> and <math>{\mathcal A}\limp\one</math>.
<!--  By contrast with the purely relational model, it is not compact closed:  -->
<!--  in general, <math>{\mathcal A}\limp {\mathcal B}\not\cong{\mathcal A}\orth\tens {\mathcal B}</math> (consider <math>{\mathcal A}</math> and -->
<!--  <math>{\mathcal B}</math> such that <math>\mathfrak F\left({\mathcal A}\right)=\powerset{\web{{\mathcal A}}}</math> and <math>\web{{\mathcal B}}</math> is finite). -->

===== Example. =====
	Setting <math>\mathcal{S}=\left\{(k,k+1);\  k\in{\mathbf N}\right\}</math> and <math>\mathcal{P}=\left\{(k+1,k);\  k\in{\mathbf N}\right\}</math>, we have <math>\mathcal{S},\mathcal{P}\in\mathbf{Fin}({\mathcal N},{\mathcal N})</math> and <math>\mathcal{P}\bullet\mathcal{S}=\mathsf{id}_{{\mathcal N}}</math>.

=== Additives ===
We now introduce the cartesian structure of <math>\mathbf{Fin}</math>.  We define <math>{\mathcal A} \oplus {\mathcal B}</math> by  <math>\web {{\mathcal A} \oplus {\mathcal B}} = \web{\mathcal A} \uplus \web{\mathcal B}</math> and <math>\mathfrak F\left({\mathcal A} \oplus {\mathcal B}\right) = \left\{ a\uplus b;\  a\in \mathfrak F\left(\mathcal A\right),\ b\in\mathfrak F\left(\mathcal B\right)\right\}</math> where <math>\uplus</math> denotes the disjoint union of sets: <math>x\uplus y=(\{1\}\times x)\cup(\{2\}\times y)</math>.  We have <math>\left({\mathcal A}\oplus {\mathcal B}\right)\orth = {\mathcal A}\orth\oplus{\mathcal B}\orth</math>.<ref>The fact that the additive connectors are identified, i.e. that we obtain a biproduct, is to be related with the enrichment of <math>\mathbf{Fin}</math> over the monoid structure of set union: see {{BibEntry|bibtype=journal|author=Marcello P. Fiore|title=Differential Structure in Models of Multiplicative Biadditive Intuitionistic Linear Logic|journal=TLCA 2007}} This identification can also be shown to be a [[isomorphism]] of LL with sums of proofs.</ref>
The category <math>\mathbf{Fin}</math> is both cartesian and co-cartesian, with <math>\oplus</math> being the product and co-product, and <math>\zero</math> the initial and terminal object.  Projections are given by:

<math>
\begin{align}
\lambda_{{\mathcal A},{\mathcal B}}&amp;=\left\{\left((1,\alpha),\alpha\right);\ \alpha\in\web{\mathcal A}\right\}
\in\mathbf{Fin}({\mathcal A}\oplus{\mathcal B},{\mathcal A}) \\
\rho_{{\mathcal A},{\mathcal B}}&amp;=\left\{\left((2,\beta),\beta\right);\ \beta\in\web{\mathcal B}\right\}
\in\mathbf{Fin}({\mathcal A}\oplus{\mathcal B},{\mathcal B}) 
\end{align}
</math>

and if <math>f\in\mathbf{Fin}({\mathcal C},{\mathcal A})</math> and <math>g\in\mathbf{Fin}({\mathcal C},{\mathcal B})</math>, pairing is given by: 

<math>\left\langle f,g\right\rangle = \left\{\left(\gamma,(1,\alpha)\right);\ (\gamma,\alpha)\in f\right\} \cup \left\{\left(\gamma,(2,\beta)\right);\ (\gamma,\beta)\in g\right\} \in\mathbf{Fin}({\mathcal C},{\mathcal A}\oplus{\mathcal B}).</math>


The unique morphism from <math>{\mathcal A}</math> to <math>\zero</math> is the empty relation.  The co-cartesian structure is obtained symmetrically.

===== Example. =====
	Write <math>{\mathcal O}\orth=\left\{(0,\emptyset)\right\}\in\mathbf{Fin}({\mathcal N},\one)</math>.  Then <math>\left\langle{{\mathcal O}\orth},{\mathcal{P}}\right\rangle =\{ (0,(1,\emptyset)) \}\cup \{ (k+1,(2,k)) ;\  k\in{\mathbf N} \} \in\mathbf{Fin}\left({\mathcal N},\one\oplus{\mathcal N}\right)</math>
	is an isomorphism.
	<!--  the inverse of which we denote by <math>\mathcal{P}</math>.  -->
	<!-- Hence <math>\one\oplus{\mathcal N}\cong {\mathcal N}</math>. -->

=== Exponentials ===
If <math>A</math> is a set, we denote by <math>\finmulset A</math> the set of all finite multisets of
elements of <math>A</math>, and if <math>a\subseteq A</math>, we write <math>a^{\oc}=\finmulset a\subseteq\finmulset A</math>.
If <math>\overline\alpha\in\finmulset A</math>, we denote its support by
<math>\mathrm{Support}\left(\overline \alpha\right)\in\finpowerset A</math>.  For all finiteness space <math>{\mathcal A}</math>, we define
<math>\oc {\mathcal A}</math> by: <math>\web{\oc {\mathcal A}}= \finmulset{\web{{\mathcal A}}}</math> and <math>\mathfrak F\left(\oc{\mathcal A}\right)=\left\{a^{\oc};\  a\in\mathfrak F\left({\mathcal A}\right)\right\}\biorth</math>.
It can be shown that <math>\mathfrak F\left(\oc{\mathcal A}\right) = \left\{\overline a\subseteq\finmulset{\web{{\mathcal A}}};\ \bigcup_{\overline\alpha\in \overline a}\mathrm{Support}\left(\overline \alpha\right)\in\mathfrak F\left(\mathcal A\right)\right\}</math>.
Then, for all <math>f\in\mathbf{Fin}({\mathcal A},{\mathcal B})</math>, we set


<math>\oc f =\left\{\left(\left[\alpha_1,\ldots,\alpha_n\right],\left[\beta_1,\ldots,\beta_n\right]\right);\  \forall i,\ (\alpha_i,\beta_i)\in f\right\} \in \mathbf{Fin}(\oc {\mathcal A}, \oc {\mathcal B}),</math>


which defines a functor.
Natural transformations 
<math>\mathsf{der}_{{\mathcal A}}=\left\{([\alpha],\alpha);\  \alpha\in \web{{\mathcal A}}\right\}\in\mathbf{Fin}(\oc{\mathcal A},{\mathcal A})</math> and 
<math>\mathsf{digg}_{{\mathcal A}}=\left\{\left(\sum_{i=1}^n\overline\alpha_i,\left[\overline\alpha_1,\ldots,\overline\alpha_n\right]\right);\ \forall i,\ \overline\alpha_i\in\web{\oc {\mathcal A}}\right\}</math> make this functor a comonad.

===== Example. =====
We have isomorphisms 
<!-- <math>\left\{(n[\emptyset],n);\  n\in{\mathbf N}\right\}\in\mathbf{Fin}(\oc\one,{\mathcal N}\orth)</math>, -->
<math>\left\{([],\emptyset)\right\}\in\mathbf{Fin}(\oc\zero,\one)</math>
and


<math>\left\{ \left(\overline\alpha_l+\overline\beta_r,\left(\overline\alpha,\overline\beta\right)\right);\ (\overline\alpha_l,\overline\alpha)\in\oc\lambda_{{\mathcal A},{\mathcal B}}\wedge(\overline\beta_r,\overline\beta)\in\oc\rho_{{\mathcal A},{\mathcal B}}\right\} \in\mathbf{Fin}(\oc({\mathcal A}\oplus{\mathcal B}),\oc{\mathcal A}\tens\oc{\mathcal B}).</math>


More generally, we have
<math>\oc\left({\mathcal A}_1\oplus\cdots\oplus{\mathcal A}_n\right)\cong\oc{\mathcal A}_1\tens\cdots\tens\oc{\mathcal A}_n</math>.


= Fragment =


In general, a '''fragment''' of a logical system ''S'' is a logical system obtained by restricting the language of ''S'', and by restricting the rules of ''S'' accordingly. In linear logic, the most well known fragments are obtained by combining/removing in different ways the classes of connectives present in the [[Sequent calculus|language of linear logic]] itself:

* '''Multiplicative connectives:''' the conjunction <math>\tens</math> (''tensor'') and the disjunction <math>\parr</math> (''par''), with their respective units <math>\one</math> (''one'') and <math>\bot</math> (''bottom''); these connectives are the combinatorial base of linear logic (permutations, circuits, etc.). 
* '''Additive connectives:''' the conjunction <math>\with</math> (''with'') and the disjunction <math>\plus</math> (''plus''), with their respective units <math>\top</math> (''top'') and <math>\zero</math> (''zero''); the computational content of these connectives, which behave more closely to their intuitionistic counterparts (''e.g.'', <math>A\with B\limp A</math> and <math>A\with B\limp B</math> are provable), is strongly related to choice (''if...then...else'', product and sum types, etc.).
* '''Exponential connectives:''' the modalities <math>\oc</math> (''of course'') and <math>\wn</math> (''why not'') handle the structural rules in linear logic, and are necessary to recover the expressive power of intuitionistic or classical logic.
* '''Quantifiers:''' just as in classical logic, quantifiers may be added to propositional linear logic, at any order. The most frequently considered are the second order ones (in analogy with System F).

The additive and exponential connectives, if taken alone, yield fragments of limited interest, so one usually considers only fragments containing at least the multiplicative connectives (perhaps without units). It is important to observe that the [[Sequent calculus#Cut elimination and consequences|cut elimination rules]] of linear logic do not introduce connectives belonging to a different class than that of the pair of dual formulas whose cut is being reduced. Hence, any fragment defined by combining the above classes will enjoy cut elimination. Since cut elimination implies the subformula property, all of the [[Sequent calculus#Equivalences|fundamental equivalences]] provable in full linear logic remain valid within such fragments, as soon as the formulas concerned belong to the fragment itself.

Conventionally, if <math>LL</math> denotes full linear logic, its fragments are denoted by prefixing <math>LL</math> with letters corresponding to the classes of connectives being considered: <math>M</math> for multiplicative connectives, <math>A</math> for additive connectives, and <math>E</math> for exponential connectives. Additional subscripts may specify whether units and/or quantifiers are present or not, and, for quantifiers, of what order (see the article on [[notations]]).

== Motivations ==

The main interest of studying fragments of linear logic is that these are usually simpler than the whole system, so that certain properties may be first analyzed on fragments, and then extended or adapted to increasingly larger fragments. It may also be interesting to see, given a property that does not hold for full linear logic, whether it holds for a fragment, and where the &quot;breaking point&quot; is situated. Examples of such questions include:

* '''logical complexity:''' proving cut elimination for full linear logic with second order quantification is equivalent to proving the consistency of second order Peano arithmetic (Girard, via [[Translations of intuitionistic logic|translations of System F]] in linear logic). One may expect that smaller fragments have lower logical complexity.
* '''provability:''' the ''provability problem'' for a logical system ''S'' is defined as follows: given a formula <math>A</math> in the language of ''S'', is <math>A</math> provable in ''S''? This problem is undecidable in full linear logic with quantifiers, of whatever order (again, because [[Translations of classical logic|classical logic can be translated]] in linear logic). On what fragments does it become decidable? And if it does, what is its computational complexity?
* '''computational complexity of cut elimination:''' the ''cut elimination problem'' (Mairson-Terui) for a logical system ''S'' is defined as follows: given two proofs of <math>A</math> in ''S'', do they reduce to the same cut-free proof? Although decidable (thanks to strong normalization), this problem is not elementary recursive in full propositional linear logic (Statman, again via the above-mentioned translations). Does the problem fall into any interesting complexity class when applied to fragments?
* '''proof nets:''' the definition of proof nets, and in particular the formulation of correctness criteria and the study of their complexity, is a good example of how a methodology can be applied to a small fragment of linear logic and later adapted (more or less successfully) to wider fragments.
* '''denotational semantics:''' several problems related to denotational semantics (formulation of [[Categorical semantics|categorical models]], full abstraction, full completeness, injectivity, etc.) may be first attacked in the simpler case of fragments, and then extended to wider subsystems.

== Multiplicative fragments ==

Multiplicative linear logic (<math>MLL</math>) is the simplest of the well known fragments of linear logic. Its formulas are obtained by combining propositional atoms with the connectives ''tensor'' and ''par'' only. As a consequence, the [[Sequent calculus#Sequents and proofs|sequent calculus]] of <math>MLL</math> is limited to the rules <math>\rulename{axiom}</math>, <math>\rulename{cut}</math>, <math>\tens</math>, and <math>\parr</math>. These rules actually determine the multiplicative connectives: if a dual pair of connectives <math>\tens'</math> and <math>\parr'</math> is introduced, with the same rules as <math>\tens</math> and <math>\parr</math>, respectively, then one can show <math>A\tens' B</math> to be provably equivalent to <math>A\tens B</math> (and, dually, <math>A\parr'B</math> to be provably equivalent to <math>A\parr B</math>).

The cut elimination problem for <math>MLL</math> is <math>\mathbf P</math>-complete (Mairson-Terui), even though there exists a deterministic algorithm solving the problem in logarithmic space if one considers only [[Sequent calculus#Expansion of identities|eta-expanded]] proofs (Mairson-Terui). On the other hand, provability for <math>MLL</math> is <math>\mathbf{NP}</math>-complete, and it remains so even in presence of first order quantifiers.

Another multiplicative fragment, less considered in the literature, can be defined by using the units <math>\one</math> and <math>\bot</math> instead of the propositional atoms. In this fragment, denoted by <math>MLL_u</math>, one can also eliminate the <math>\rulename{axiom}</math> rule from sequent calculus, since it is redundant. <math>MLL_u</math> is even simpler than <math>MLL</math>: its provability problem is in <math>\mathbf P</math>, and, since all proofs are eta-expandend, its cut elimination problem is in <math>\mathbf L</math>.

The union of <math>MLL</math> and <math>MLL_u</math> is the full propositional multiplicative fragment of linear logic, and is denoted by <math>MLL_0</math>. It has the same properties as <math>MLL</math>, which shows that the presence/absence of propositional atoms (and of the <math>\rulename{axiom}</math> rule) has a non-trivial effect on the complexity of provability and cut elimination, ''i.e.'', the complexity is not altered iff <math>\mathbf P\subsetneq\mathbf{NP}</math> and <math>\mathbf L\subsetneq\mathbf P</math>, respectively.

If we add second order quantifiers to <math>MLL</math> (resp. <math>MLL_u</math>), we obtain a system denoted by <math>MLL_2</math> (resp. <math>MLL_{02}</math>). In <math>MLL_{02}</math> one can show that <math>\one</math> and <math>\bot</math> are provably equivalent to <math>\forall X.(X\orth\parr X)</math> and <math>\exists X.(X\orth\tens X)</math>, respectively. Hence, <math>MLL_2</math> is as expressive as <math>MLL_{02}</math>. In these second order fragments, provability is undecidable, while cut elimination is still <math>\mathbf P</math>-complete.

== Additive fragments ==

The most studied additive fragments of linear logic are defined by taking <math>MLL</math> or <math>MLL_0</math> and by enriching their language with the additive connectives, with or without units. The same can be done in presence of quantifiers. We thus obtain:

* <math>MALL</math>: formulas built from propositional atoms using <math>\tens,\parr,\with,\plus</math>;
* <math>MALL_0</math>: formulas built from propositional atoms and <math>\one,\bot,\top,\zero</math>, using <math>\tens,\parr,\with,\plus</math>;
* <math>MALL_n</math>: <math>MALL</math> with quantifiers of order <math>n</math>;
* <math>MALL_{0n}</math>: <math>MALL_0</math> with quantifiers of order <math>n</math>.

The [[Additive linear logic|purely additive framents]] are less common:

* <math>ALL</math>: formulas built from propositional atoms using <math>\with,\plus</math>;
* <math>ALL_0</math>: formulas built from propositional atoms and <math>\top,\zero</math>, using <math>\with,\plus</math>;
* <math>ALL_n</math>: <math>ALL</math> with quantifiers of order <math>n</math>;
* <math>ALL_{0n}</math>: <math>ALL_0</math> with quantifiers of order <math>n</math>.

As for the multiplicative connectives, the additive connectives are also defined by their rules: adding a pair of dual connectives <math>\with',\plus'</math> to <math>MALL</math>, and giving them the same rules as <math>\with,\plus</math>, makes the new connectives provably equivalent to the old ones.

In <math>MALL_{02}</math>, the additive units <math>\top</math> and <math>\zero</math> are provably equivalent to <math>\exists X.X\orth</math> and <math>\forall X.X</math>, respectively. Since multiplicative units are also definable in terms of second order quantification, we obtain that <math>MALL_2</math> is as expressive as <math>MALL_{02}</math>.

The cut elimination problem is <math>\mathbf{coNP}</math>-complete for all of the fragments defined above (Mairson-Terui).

Provability is undecidable in any additive fragment as soon as second order quantification is considered. It is decidable, although quite complex, in the propositional and first order case: it is <math>\mathbf{PSPACE}</math>-complete in <math>MALL_0</math>, and <math>\mathbf{NEXP}</math>-complete in <math>MALL_{01}</math>. This latter result is indicative of the fact that the undecidability of predicate calculus is not ascribable to existential quantification alone, but rather to the simultaneous presence of existential quantification and contraction.

== Exponential fragments ==

The most common proper fragments of linear logic containing the exponential connectives are defined as in the case of the additive fragments, ''i.e.'', by adding the modalities on top of <math>MLL</math> and its variants:

* <math>MELL</math>: formulas built from propositional atoms using <math>\tens,\parr,\oc,\wn</math>;
* <math>MELL_0</math>: formulas built from propositional atoms and <math>\one,\bot</math>, using <math>\tens,\parr,\oc,\wn</math>;
* <math>MELL_n</math>: <math>MELL</math> with quantifiers of order <math>n</math>;
* <math>MELL_{0n}</math>: <math>MELL_0</math> with quantifiers of order <math>n</math>.

If, instead of taking <math>MLL</math>, we add the modalities to <math>MALL</math>, we obtain of course various versions of full linear logic:

* <math>LL</math>: full linear logic, without units;
* <math>LL_0</math>: full linear logic, with units;
* <math>LL_n</math>: <math>LL</math> with quantifiers of order <math>n</math>;
* <math>LL_{0n}</math>: <math>LL_0</math> with quantifiers of order <math>n</math>.

In <math>LL_{02}</math> the formulas <math>A\with B</math> and <math>A\plus B</math> are provably equivalent to <math>\exists X.(\oc{(X\orth\parr A)}\tens\oc{(X\orth\parr B)}\tens X)</math> and <math>\forall X.(\wn{(X\orth\tens A)}\parr\wn{(X\orth \tens B)}\parr X)</math>, respectively, for all <math>A,B</math>. Thanks to the second-order definability of units discussed above, we obtain that <math>MELL_2</math> is as expressive as <math>LL_{02}</math>, ''i.e.'', full propositional second order linear logic embeds in its second order multiplicative exponential fragment without units.

Girard showed how cut elimination for <math>LL_{02}</math> ''without the contraction rule'' can be proved by a simple induction up to <math>\omega</math>, ''i.e.'', in first order Peano arithmetic. This gives a huge gap between the logical complexity of full linear logic and its contraction-free subsystem: in fact, still by Girard's results, we know that cut elimination in <math>MELL_2</math> is equivalent to the consistency of second order Peano arithmetic, for which no ordinal analysis is known. There are nevertheless subsystems of <math>MELL_2</math>, the so-called [[Light linear logics|light subsystems]] of linear logic, in which the exponential connectives are weakened, whose cut elimination can be proved in seconder order Peano arithmetic even in presence of contraction.

The cut elimination problem is never elementary recursive in presence of exponential connectives: the simply typed <math>\lambda</math>-calculus with arrow types only can be encoded in <math>MELL</math>, and this is enough for Statman's lower bound to apply. However, it becomes elementary recursive in the above mentioned [[Light linear logics|light logics]].

Albeit perhaps surprisingly, provability in <math>LL</math> is already undecidable. This result, obtained by coding Minsky machines with linear logic formulas, contrasts with the situation in classical logic, whose propositional fragment is notoriously decidable. It is indicative of the fact that modalities are themselves a form of quantification, although this claim is far from being clear: as a matter of fact, the decidability of propositional provability in the absence of additives, ''i.e.'', in <math>MELL</math> alone, is still an open problem. It is known that adding first order quantification to <math>MELL</math> makes it undecidable.

=== About exponential rules ===

In this section, provability is assumed to be in <math>LL_{02}</math>, ''i.e.'', full propositional second order linear logic.

In contrast with multiplicative and additive connectives, the modalities of linear logic are not defined by their rules: one may introduce a pair of dual modalities <math>\oc',\wn'</math>, each with the same rules as <math>\oc,\wn</math>, without <math>\oc'A</math> (resp. <math>\wn'A</math>) being in general provably equivalent to <math>\oc A</math>  (resp. <math>\wn A</math>).

The [[Sequent calculus#Sequents and proofs|promotion rule]] is derivable from the following two rules, called ''functorial promotion'' and ''digging'', respectively:

<center><math>
\AxRule{\vdash\Gamma,A}
\LabelRule{f\oc}
\UnaRule{\vdash\wn\Gamma,\oc A}
\DisplayProof
\qquad\qquad\qquad
\AxRule{\vdash\Gamma,\wn{\wn A}}
\LabelRule{dig}
\UnaRule{\vdash\Gamma,\wn A}
\DisplayProof
</math></center>

Functorial promotion is itself derivable from dereliction and promotion; the digging rule is also derivable, but only using the <math>\rulename{cut}</math> rule (in fact, digging does not enjoy the subformula property). It may be convenient to consider this pair of rules instead of the standard promotion rule in the context of [[categorical semantics]] of linear logic.

In presence of the digging rule, dereliction, weakening, and contraction can be derived from the following rule, called ''multiplexing'', in which <math>A^{(n)}</math> stands for the sequence <math>A,\ldots,A</math> containing <math>n</math> occurrences of <math>A</math>:

<center><math>
\AxRule{\vdash\Gamma,A^{(n)}}
\LabelRule{mux}
\UnaRule{\vdash\Gamma,\wn A}
\DisplayProof
</math></center>

Of course, multiplexing is itself derivable from dereliction, weakening, and contraction. Hence, there are several alternative but equivalent presentations of the exponential fragment of linear logic, such as
# remove promotion, and replace it with functorial promotion and digging;
# remove promotion, dereliction, weakening, and contraction, and replace them with functorial promotion, digging, and multiplexing.
Apart from their usefulness in [[categorical semantics]], these alternative formulations are of interest in the context of the so-called [[light linear logics]] mentioned above. For example, ''elementary linear logic'' is obtained by removing dereliction and digging from formulation 1, and ''soft linear logic'' is obtained by removing digging from formulation 2.

Multiplexing is invertible in certain circumstances. A sequent <math>\vdash\Gamma,\wn A</math> containing no occurrence of <math>\with</math>, <math>\oc</math>, or second order <math>\exists</math> is provable iff <math>\vdash\Gamma,A^{(n)}</math> is provable for some <math>n</math> (this is easily checked by induction on cut-free proofs). To see that this does not hold in general, take for instance <math>A=X\orth</math> and <math>\Gamma=X\with\one</math>, or <math>\Gamma=\oc X</math>. The restriction on the presence of additive conjunction can be removed by slightly changing the statement: a sequent <math>\vdash\Gamma,\wn A</math> containing no occurrence of <math>\oc</math> or second order <math>\exists</math> is provable iff <math>\vdash\Gamma,(A\plus\bot)^{(n)}</math> is provable for some <math>n</math>.

The latter result can be generalized as follows. If <math>A</math> is a formula, <math>\oc_nA</math> stands for the formula <math>(A\with\one)\tens\cdots\tens(A\with\one)</math> (<math>n</math> times) and <math>\wn_nA</math> for the formula <math>(A\plus\bot)\parr\cdots\parr(A\plus\bot)</math> (<math>n</math> times). Then, we have

{{Theorem|title=Approximation Theorem|
Let <math>\vdash\Gamma</math> be a provable sequent containing <math>p</math> occurrences of <math>\oc</math>, <math>q</math> occurrences of <math>\wn</math>, and no occurrence of second order <math>\exists</math>. Then, for all <math>m_1,\ldots,m_p\in\mathbb N</math>, there are <math>n_1,\ldots,n_q\in\mathbb N</math> such that the sequent obtained from <math>\vdash\Gamma</math> by replacing the <math>p</math> occurrences of <math>\oc</math> with <math>\oc_{m_1},\ldots,\oc_{m_p}</math> and the <math>q</math> occurrences of <math>\wn</math> with <math>\wn_{n_1},\ldots,\wn_{n_q}</math> is provable.}}

A ''structural formula'' is a formula <math>C</math> such that <math>C\limp C\tens C</math> and <math>C\limp\one</math> are provable. Obviously, any formula of the form <math>\wn B</math> is structural. However, the promotion rule cannot be extended to arbitrary structural formulas, ''i.e.'', the following rule is ''not'' admissible:

<center>
<math>
\AxRule{C\vdash A}
\AxRule{C\vdash C\tens C}
\AxRule{C\vdash\one}
\TriRule{C\vdash\oc A}
\DisplayProof
</math>
</center>

For instance, if <math>A=C=\alpha\tens\oc{(\alpha\limp\alpha\tens\alpha)}\tens\oc{(\alpha\limp\one)}</math>, the three premises are provable but not the conclusion.

The following rule, called ''absorption'', is derivable in the standard sequent calculus:
<center>
<math>
\AxRule{\vdash\Gamma,\wn A,A}
\UnaRule{\vdash\Gamma,\wn A}
\DisplayProof
</math>
</center>
The absorption rule is useful in the context of proof search in linear logic.

== The provability problem ==

It is well known that the decidability of the provability problem is connected to the [[Phase semantics|finite model property]]: if a fragment of a logic with a truth semantics enjoys the finite model property, then the provability in that fragment is decidable. Note of course that the converse may fail.

In this section, we summarize the known results about the validity of the final model property and the decidability of provability, with its complexity, for the various fragments of linear logic introduced above. Question marks in the tables below denote open problems. For brevity, all fragments are assumed to have units and propositional atoms, ''e.g.'', <math>MLL</math> actually denotes what we called <math>MLL_0</math> above.

=== The finite model property ===
{| border=&quot;1&quot;
|-
| <math>MLL</math>
| <math>MALL</math>
| <math>MELL</math>
| <math>LL</math>
|-
| <span style=&quot;color: green&quot;>yes</span>
| <span style=&quot;color: green&quot;>yes</span>
| <span style=&quot;color: red&quot;>no</span>
| <span style=&quot;color: red&quot;>no</span>
|}

=== Provability ===
{| border=&quot;1&quot;
|-
|
| <math>MLL</math>
| <math>MALL</math>
| <math>MELL</math>
| <math>LL</math>
|-
| propositional case
| <span style=&quot;color: green&quot;><math>\mathbf{NP}</math>-complete</span>
| <span style=&quot;color: green&quot;><math>\mathbf{PSPACE}</math>-complete</span>
| ?
| <span style=&quot;color: red&quot;>undecidable</span>
|-
| first order case
| <span style=&quot;color: green&quot;><math>\mathbf{NP}</math>-complete</span>
| <span style=&quot;color: green&quot;><math>\mathbf{NEXP}</math>-complete</span>
| <span style=&quot;color: red&quot;>undecidable</span>
| <span style=&quot;color: red&quot;>undecidable</span>
|-
| second order case
| <span style=&quot;color: red&quot;>undecidable</span>
| <span style=&quot;color: red&quot;>undecidable</span>
| <span style=&quot;color: red&quot;>undecidable</span>
| <span style=&quot;color: red&quot;>undecidable</span>
|}

== The cut elimination problem ==

In this section, we summarize the known results about the complexity of the cut elimination problem for the various fragments of linear logic introduced above, plus some [[light linear logics]]. All fragments are assumed to be propositional; the results do not change in presence of quantification of any order.

{| border=&quot;1&quot;
|-
| <math>MLL_u</math>
| <math>MLL</math>
| <math>MALL</math>
| <math>MSLL</math>
| <math>MLLL</math>
| <math>MELL</math>
|-
| <math>\mathbf L</math>
| <math>\mathbf{P}</math>-complete
| <math>\mathbf{coNP}</math>-complete
| <math>\mathbf{EXP}</math>-complete
| <math>\mathbf{2EXP}</math>-complete
| not elementary recursive
|}

Notations used in the above table:
* <math>MSLL</math>: multiplicative soft linear logic;
* <math>MLLL</math>: multiplicative light linear logic.


= Game semantics =


This article presents the game-theoretic [[fully complete model]] of <math>MLL</math>.
Formulas are interpreted by games between two players, Player and Opponent, and proofs
are interpreted by strategies for Player.

== Preliminary definitions and notations ==

=== Sequences, Polarities ===

{{Definition|title=Sequences|
If <math>M</math> is a set of ''moves'', a '''sequence''' or a '''play''' on <math>M</math>
is a finite sequence of elements of <math>M</math>. The set of sequences of <math>M</math> is denoted by <math>M^*</math>.
}}

We introduce some convenient notations on sequences.
* If <math>s\in M^*</math>, <math>|s|</math> will denote the ''length'' of <math>s</math>;
* If <math>1\leq i\leq |s|</math>, <math>s_i</math> will denote the i-th move of <math> s</math>;
* We denote by <math>\sqsubseteq</math> the prefix partial order on <math>M^*</math>;
* If <math>s_1</math> is an even-length prefix of <math>s_2</math>, we denote it by <math>s_1\sqsubseteq^P s_2</math>;
* The empty sequence will be denoted by <math>\epsilon</math>.


All moves will be equipped with a '''polarity''', which will be either Player (<math>P</math>) or Opponent (<math>O</math>).
* We define <math>\overline{(\_)}:\{O,P\}\to \{O,P\}</math> with <math> \overline{O} = P </math> and <math>\overline{P} = O</math>.
* This operation extends in a pointwise way to functions onto <math>\{O,P\}</math>.

=== Sequences on Components ===

We will often need to speak of sequences over (the disjoint sum of) multiple sets of moves, along with a restriction operation.
* If <math> M_1 </math> and <math> M_2 </math> are two sets, <math> M_1 + M_2 </math> will denote their disjoint sum, implemented as <math> M_1 + M_2 = \{1\}\times M_1 \cup \{2\}\times M_2</math>;
* In this case, if we have two functions <math>\lambda_1:M_1 \to R</math> and <math> \lambda_2:M_2\to R</math>, we denote by <math> [\lambda_1,\lambda_2]:M_1 + M_2 \to R</math> their ''co-pairing'';
* If <math>s\in (M_A + M_B)^*</math>, the '''restriction''' of <math>s</math> to <math>M_A</math> (resp. <math>M_B</math>) is denoted by <math>s\upharpoonright M_A</math> (resp.<math>s \upharpoonright M_B</math>). Later, if <math>A</math> and <math>B</math> are games, this will be abbreviated <math>s\upharpoonright A</math> and <math>s\upharpoonright B</math>.


== Games and Strategies ==

=== Game constructions ===
We first give the definition for a game, then all the constructions used to interpret the connectives and operations of <math>MLL</math>

{{Definition|title=Games|
A '''game''' <math> A</math> is a triple <math>(M_A,\lambda_A,P_A)</math> where:
* <math>M_A</math> is a finite set of ''moves'';
* <math>\lambda_A: M_A \to \{O,P\}</math> is a ''polarity function'';
* <math>P_A</math> is a subset of <math>M_A^*</math> such that
** Each <math>s\in P_A</math> is '''alternating''', ''i.e.'' if <math>\lambda_A (s_i) = Q</math> then, if defined, <math>\lambda_A(s_{i+1}) = \overline{Q}</math>;
** <math>A</math> is '''finite''': there is no infinite strictly increasing sequence <math>s_1 \sqsubset s_2 \sqsubset \dots </math> in <math>P_A</math>.
}}


{{Definition|title=Linear Negation|
If <math>A</math> is a game, the game <math>A^\bot</math> is <math>A</math> where Player and Opponent are interchanged. Formally:
* <math>M_{A^\bot} = M_A</math>
* <math>\lambda_{A^\bot} = \overline{\lambda_A}</math>
* <math>P_{A^\bot} = P_A</math>
}}


{{Definition|title=Tensor|
If <math>A</math> and <math>B</math> are games, we define <math>A \tens B</math> as:
* <math>M_{A\tens B} = M_A + M_B</math>;
* <math>\lambda_{A\tens B} = [\lambda_A,\lambda_B]</math>
* <math>P_{A\tens B}</math> is the set of all finite, alternating sequences in <math>M_{A\tens B}^*</math> such that <math>s\in P_{A\tens B}</math> if and only if:
# <math>s\upharpoonright A\in P_A</math> and <math>s \upharpoonright B \in P_B</math>;
# If we have <math>i\le |s|</math> such that <math> s_i</math> and <math>s_{i+1}</math> are in different components, then <math>\lambda_{A\tens B}(s_{i+1}) = O</math>. We will refer to this condition as the ''switching convention for tensor game''.
}}


The ''par'' connective can be defined either as <math>A\parr B = (A^\bot \tens B^\bot)^\bot</math>, or similarly to the ''tensor'' except that the switching convention is in favor of Player. We will refer to this as the ''switching convention for par game''. Likewise, we define <math>A\limp B = A
^\bot\parr B</math>.


=== Strategies ===

{{Definition|title=Strategies|
A '''strategy''' for Player in a game <math>A</math> is defined as a subset <math>\sigma\subseteq P_A</math> satisfying the following conditions:
* <math>\sigma</math> is non-empty: <math>\epsilon\in \sigma</math>
* Opponent starts: If <math>s\in \sigma</math>, <math>\lambda_A(s_1)=O</math>;
* <math>\sigma</math> is closed by ''even prefix'', ''i.e.'' if <math>s\in \sigma</math> and <math>s'\sqsubseteq^P s</math>, then <math>s'\in \sigma</math>;
* Determinacy: If we have <math>soa\in \sigma</math> and <math>sob\in \sigma</math>, then <math>a=b</math>.
We write <math>\sigma:A</math>.
}}


Composition is defined by parallel interaction plus hiding. We take all valid sequences on <math>A, B</math> and <math>C</math> which behave accordingly to <math>\sigma</math> (resp. <math>\tau</math>) on <math>A, B</math> (resp. <math>B,C</math>). Then, we hide all the communication in <math>B</math>.

{{Definition|title=Parallel Interaction|
If <math>A, B</math> and <math>C</math> are games, we define the set of '''interactions'''
<math>I(A,B,C)</math> as the set of sequences <math>s</math> over <math>A, B</math> and <math>C</math> such that their respective restrictions on
<math>A\limp B</math> and <math>B\limp C</math> are in <math>P_{A\limp B}</math> and <math>P_{B\limp C}</math>, and such that no successive
moves of <math>s</math> are respectively in <math>A</math> and <math>C</math>, or <math>C</math> and <math>A</math>.
If <math>\sigma:A\limp B</math> and <math>\tau:B\limp C</math>, we define the set of '''parallel interactions''' of
<math>\sigma</math> and <math>\tau</math>, denoted by <math>\sigma||\tau</math>, as <math>\{s\in I(A,B,C)~|~s\upharpoonright A,B\in \sigma \wedge s\upharpoonright B,C \in \tau\}</math>.
}}


{{Definition|title=Composition|
If <math>\sigma:A\limp B</math> and <math>\tau:B\limp C</math>, we define <math>\sigma;\tau = \{s\upharpoonright A,C~|~s\in \sigma||\tau\}</math>.
}}


We also define the identities, which are simple copycat strategies : they immediately copy on the left (resp. right) component the last Opponent's move on the right (resp.left) component. In the following definition, let <math>L</math> (resp. <math>R</math>) denote the left (resp. right) occurrence of <math>A</math> in <math>A\limp A</math>.

{{Definition|title=Identities|
If <math>A</math> is a game, we define a strategy <math>id_A: A\limp A</math> by <math>id_A = \{s\in P_{A\limp A}~|~\forall s'\sqsubseteq^P s,~s'\upharpoonright L = s' \upharpoonright R\}</math>.
}}


With these definitions, we get the following theorem:

{{Theorem|title=Category of Games and Strategies|
Composition of strategies is associative and identities are neutral for it. More precisely, there is a [[*-autonomous category]] with games as objects and strategies on <math>A\limp B</math> as morphisms from <math>A</math> to <math>B</math>.
}}


= Geometry of interaction =


The ''geometry of interaction'', GoI in short, was defined in the early nineties by Girard as an interpretation of linear logic into operators algebra: formulae were interpreted by Hilbert spaces and proofs by partial isometries.

This was a striking novelty as it was the first time that a mathematical model of logic (lambda-calculus) didn't interpret a proof of <math>A\limp B</math> as a morphism ''from'' <math>A</math> ''to'' <math>B</math> and proof composition (cut rule) as the composition of morphisms. Rather the proof was interpreted as an operator acting ''on'' <math>A\limp B</math>, that is a morphism from <math>A\limp B</math> to <math>A\limp B</math>. For proof composition the problem was then, given an operator on <math>A\limp B</math> and another one on <math>B\limp C</math> to construct a new operator on <math>A\limp C</math>. This problem was solved by the ''execution formula'' that bares some formal analogies with Kleene's formula for recursive functions. For this reason GoI was claimed to be an ''operational semantics'', as opposed to traditionnal [[Semantics|denotational semantics]].

The first instance of the GoI was restricted to the <math>MELL</math> fragment of linear logic (Multiplicative and Exponential fragment) which is enough to encode lambda-calculus. Since then Girard proposed several improvements: firstly the extension to the additive connectives known as ''Geometry of Interaction 3'' and more recently a complete reformulation using Von Neumann algebras that allows to deal with some aspects of [[Light linear logics|implicit complexity]]

The GoI has been a source of inspiration for various authors. Danos and Regnier have reformulated the original model exhibiting its combinatorial nature using a theory of reduction of paths in proof-nets and showing the link with abstract machines; the execution formula appears as the composition of two automata interacting through a common interface. Also the execution formula has rapidly been understood as expressing the composition of strategies in game semantics. It has been used in the theory of sharing reduction for lambda-calculus in the Abadi-Gonthier-Lévy reformulation and simplification of Lamping's representation of sharing. Finally the original GoI for the <math>MELL</math> fragment has been reformulated in the framework of traced monoidal categories following an idea originally proposed by Joyal.

= The Geometry of Interaction as operators =

The original construction of GoI by Girard follows a general pattern already mentionned in the section on [[coherent semantics]] under the name ''symmetric reducibility'' and that was first put to use in [[phase semantics]]. First set a general space <math>P</math> called the ''proof space'' because this is where the interpretations of proofs will live. Make sure that <math>P</math> is a (not necessarily commutative) monoid. In the case of GoI, the proof space is a subset of the space of bounded operators on <math>\ell^2</math>.

Second define a particular subset of <math>P</math> that will be denoted by <math>\bot</math>; then derive a duality on <math>P</math>: for <math>u,v\in P</math>, <math>u</math> and <math>v</math> are dual<ref>In modern terms one says that <math>u</math> and <math>v</math> are ''polar''.</ref>iff <math>uv\in\bot</math>.

Such a duality defines an [[orthogonality relation]], with the usual derived definitions and properties.

For the GoI, two dualities have proved to work; we will consider the first one: nilpotency, ''ie'', <math>\bot</math> is the set of nilpotent operators in <math>P</math>. Let us explicit this: two operators <math>u</math> and <math>v</math> are dual if there is a nonnegative integer <math>n</math> such that <math>(uv)^n = 0</math>. This duality is symmetric: if <math>uv</math> is nilpotent then <math>vu</math> is nilpotent also.

Last define a ''type'' as a subset <math>T</math> of the proof space that is equal to its bidual: <math>T = T\biorth</math>. This means that <math>u\in T</math> iff for all operator <math>v\in T\orth</math>, that is such that <math>u'v\in\bot</math> for all <math>u'\in T</math>, we have <math>uv\in\bot</math>.

The real work<ref>The difficulty is to find the right duality that will make logical operations interpretable. General conditions that allows to achieve this have been formulated by Hyland and Schalk thanks to their theory of ''[[double glueing]]''.</ref>is now to interpret logical operations, that is to associate a type to each formula, an object to each proof and show the ''adequacy lemma'': if <math>u</math> is the interpretation of a proof of the formula <math>A</math> then <math>u</math> belongs to the type associated to <math>A</math>.

== [[GoI for MELL: partial isometries|Partial isometries]] ==

The first step is to build the proof space. This is constructed as a special set of partial isometries on a separable Hilbert space <math>H</math> which turns out to be generated by partial permutations on the canonical basis of <math>H</math>.

These so-called ''<math>p</math>-isometries'' enjoy some nice properties, the most important one being that a <math>p</math>-isometry is a sum of <math>p</math>-isometries iff all the terms of the sum have disjoint domains and disjoint codomains. As a consequence we get that a sum of <math>p</math>-isometries is null iff each term of the sum is null.

A second important property is that operators on <math>H</math> can be ''externalized'' using <math>p</math>-isometries into operators acting on <math>H\oplus H</math>, and conversely operators on <math>H\oplus H</math> may be ''internalized'' into operators on <math>H</math>. This is widely used in the sequel.

== [[GoI for MELL: the *-autonomous structure|The *-autonomous structure]] ==

The second step is to interpret the linear logic multiplicative operations, most importantly the cut rule.

Internalization/externalization is the key for this: typically the type <math>A\tens B</math> is interpreted by a set of <math>p</math>-isometries which are internalizations of operators acting on <math>H\oplus H</math>.

The (interpretation of) the cut-rule is defined in two steps: firstly we use nilpotency to define an operation corresponding to lambda-calculus application which given two <math>p</math>-isometries in respectively <math>A\limp B</math> and <math>A</math> produces an operator in <math>B</math>. From this we deduce the composition and finally obtain a structure of *-autonomous category, that is a model of multiplicative linear logic.

== [[GoI for MELL: exponentials|The exponentials]] ==

Finally we turn to define exponentials, that is connectives managing duplication. To do this we introduce an isomorphism (induced by a <math>p</math>-isometry) between <math>H</math> and <math>H\tens H</math>: the first component of the tensor is intended to hold the address of the the copy whereas the second component contains the content of the copy.

We eventually get a quasi-model of full MELL; quasi in the sense that if we can construct <math>p</math>-isometries for usual structural operations in MELL (contraction, dereliction, digging), the interpretation of linear logic proofs is not invariant w.r.t. cut elimination in general. It is however invariant in some good cases, which are enough to get a correction theorem for the interpretation.

= The Geometry of Interaction as an abstract machine =


= GoI for MELL: exponentials =


= The tensor product of Hilbert spaces =

Recall that we work in the Hilbert space <math>H=\ell^2(\mathbb{N})</math> endowed with its canonical hilbertian basis denoted by <math>(e_k)_{k\in\mathbb{N}}</math>.

The space <math>H\tens H</math> is the collection of sequences <math>(x_{np})_{n,p\in\mathbb{N}}</math> of complex numbers such that <math>\sum_{n,p}|x_{np}|^2</math> converges. The scalar product is defined just as before:
: <math>\langle (x_{np}), (y_{np})\rangle = \sum_{n,p} x_{np}\bar y_{np}</math>.

If <math>x = (x_n)_{n\in\mathbb{N}}</math> and <math>y = (y_p)_{p\in\mathbb{N}}</math> are vectors in <math>H</math> then their tensor is the sequence:
: <math>x\tens y = (x_ny_p)_{n,p\in\mathbb{N}}</math>.

We define: <math>e_{np} = e_n\tens e_p</math> so that <math>e_{np}</math> is the sequence <math>(e_{npij})_{i,j\in\mathbb{N}}</math> of complex numbers given by <math>e_{npij} = \delta_{ni}\delta_{pj}</math>. By bilinearity of tensor we have:
: <math>x\tens y = \left(\sum_n x_ne_n\right)\tens\left(\sum_p y_pe_p\right) = 
  \sum_{n,p} x_ny_p\, e_n\tens e_p = \sum_{n,p} x_ny_p\,e_{np}</math>

Furthermore the system of vectors <math>(e_{np})</math> is a hilbertian basis of <math>H\tens H</math>: the sequence <math>x=(x_{np})_{n,p\in\mathbb{N}}</math> may be written:
: <math>x = \sum_{n,p\in\mathbb{N}}x_{np}\,e_{np}
          = \sum_{n,p\in\mathbb{N}}x_{np}\,e_n\tens e_p</math>.

== An algebra isomorphism ==

Being both separable Hilbert spaces, <math>H</math> and <math>H\tens H</math> are isomorphic. We will now define explicitely an iso based on partial permutations.

We fix, once for all, a bijection from couples of natural numbers to natural
numbers that we will denote by <math>(n,p)\mapsto\langle n,p\rangle</math>. For
example set <math>\langle n,p\rangle = 2^n(2p+1) - 1</math>. Conversely, given
<math>n\in\mathbb{N}</math> we denote by <math>n_{(1)}</math> and
<math>n_{(2)}</math> the unique integers such that <math>\langle n_{(1)},
n_{(2)}\rangle = n</math>.

{{Remark|
just as it was convenient but actually not necessary to choose <math>p</math> and <math>q</math> so that <math>pp^* + qq^* = 1</math> it is actually not necessary to have a ''bijection'', a one-to-one mapping from <math>\mathbb{N}^2</math> ''into'' <math>\mathbb{N}</math> would be sufficient for our purpose.
}}

This bijection can be extended into a Hilbert space isomorphism <math>\Phi:H\tens H\rightarrow H</math> by defining:
: <math>e_n\tens e_p = e_{np} \mapsto e_{\langle n,p\rangle}</math>.

Now given an operator <math>u</math> on <math>H</math> we define the operator <math>!u</math> on <math>H</math> by:
: <math>!u(e_{\langle n,p\rangle}) = \Phi(e_n\tens u(e_p))</math>.

{{Remark|
The operator <math>!u</math> is defined by:
: <math>!u = \Phi\circ (1\tens u)\circ \Phi^{-1}</math>
where <math>1\tens u</math> denotes the operator on <math>H\tens H</math> defined by <math>(1\tens u)(x\tens y) = x\tens u(y)</math> for any <math>x,y</math> in <math>H</math>. However this notation must not be confused with the [[GoI for MELL: the *-autonomous structure#The tensor rule|tensor of operators]] that was defined in the previous section in order to interpret the tensor rule of linear logic; we therefore will not use it.
}}

One can check that given two operators <math>u</math> and <math>v</math> we have:
* <math>!u!v = {!(uv)}</math>;
* <math>!(u^*) = (!u)^*</math>.

Due to the fact that <math>\Phi</math> is an isomorphism ''onto'' we also have <math>!1=1</math>; this however will not be used.

We therefore have that <math>!</math> is a morphism on <math>\mathcal{B}(H)</math>; it is easily seen to be an iso (not ''onto'' though). As this is the crucial ingredient for interpreting the structural rules of linear logic, we will call it the ''copying iso''.

== Interpretation of exponentials ==

If we suppose that <math>u = u_\varphi</math> is a <math>p</math>-isometry generated by the partial permutation <math>\varphi</math> then we have:
: <math>!u(e_{\langle n,p\rangle}) = \Phi(e_n\tens u(e_p)) = \Phi(e_n\tens e_{\varphi(p)}) = e_{\langle n,\varphi(p)\rangle}</math>.
Thus <math>!u_\varphi</math> is itself a <math>p</math>-isometry generated by the
partial permutation <math>!\varphi:n\mapsto \langle n_{(1)}, \varphi(n_{(2)})\rangle</math>, which shows that the proof space is stable under the copying iso.

Given a type <math>A</math> we define the type <math>!A</math> by:
: <math>!A = \{!u, u\in A\}\biorth</math>


= GoI for MELL: partial isometries =


= Operators, partial isometries =

We will denote by <math>H</math> the Hilbert space <math>\ell^2(\mathbb{N})</math> of sequences <math>(x_n)_{n\in\mathbb{N}}</math> of complex numbers such that the series <math>\sum_{n\in\mathbb{N}}|x_n|^2</math> converges. If <math>x = (x_n)_{n\in\mathbb{N}}</math> and <math>y = (y_n)_{n\in\mathbb{N}}</math> are two vectors of <math>H</math> their ''scalar product'' is:
: <math>\langle x, y\rangle = \sum_{n\in\mathbb{N}} x_n\bar y_n</math>.

Two vectors of <math>H</math> are ''othogonal'' if their scalar product is nul. We will say that two subspaces are ''disjoint'' when any two vectors taken in each subspace are orthorgonal. Note that this notion is different from the set theoretic one, in particular two disjoint subspaces always have exactly one vector in common: <math>0</math>.

The ''norm'' of a vector is the square root of the scalar product with itself:
: <math>\|x\| = \sqrt{\langle x, x\rangle}</math>.

Let us denote by <math>(e_k)_{k\in\mathbb{N}}</math> the canonical ''hilbertian basis'' of <math>H</math>: <math>e_k = (\delta_{kn})_{n\in\mathbb{N}}</math> where <math>\delta_{kn}</math> is the Kroenecker symbol: <math>\delta_{kn}=1</math> if <math>k=n</math>, <math>0</math> otherwise. Thus if <math>x=(x_n)_{n\in\mathbb{N}}</math> is a sequence in <math>H</math> we have:
: <math> x = \sum_{n\in\mathbb{N}} x_ne_n</math>.

An ''operator'' on <math>H</math> is a ''continuous'' linear map from <math>H</math> to <math>H</math>.<ref>Continuity is equivalent to the fact that operators are ''bounded'', which means that one may define the ''norm'' of an operator <math>u</math> as the sup on the unit ball of the norms of its values:
: <math>\|u\| = \sup_{\{x\in H,\, \|x\| = 1\}}\|u(x)\|</math>.</ref>The set of (bounded) operators is denoted by <math>\mathcal{B}(H)</math>.

The ''range'' or ''codomain'' of the operator <math>u</math> is the set of images of vectors; the ''kernel'' of <math>u</math> is the set of vectors that are anihilated by <math>u</math>; the ''domain'' of <math>u</math> is the set of vectors orthogonal to the kernel, ''ie'', the maximal subspace disjoint with the kernel:

* <math>\mathrm{Codom}(u) = \{u(x),\, x\in H\}</math>;
* <math>\mathrm{Ker}(u) = \{x\in H,\, u(x) = 0\}</math>;
* <math>\mathrm{Dom}(u) = \{x\in H,\, \forall y\in\mathrm{Ker}(u), \langle x, y\rangle = 0\}</math>.

These three sets are closed subspaces of <math>H</math>.

The ''adjoint'' of an operator <math>u</math> is the operator <math>u^*</math> defined by <math>\langle u(x), y\rangle = \langle x, u^*(y)\rangle</math> for any <math>x,y\in H</math>. Adjointness is well behaved w.r.t. composition of operators:
: <math>(uv)^* = v^*u^*</math>.

A ''projector'' is an idempotent operator of norm <math>0</math> (the projector
on the null subspace) or <math>1</math>, that is an operator <math>p</math>
such that <math>p^2 = p</math> and <math>\|p\| = 0</math> or <math>1</math>. A projector is auto-adjoint and its domain is equal to its codomain.

A ''partial isometry'' is an operator <math>u</math> satisfying <math>uu^* u =
u</math>; this condition entails that we also have <math>u^*uu^* =
u^*</math>. As a consequence <math>uu^*</math> and <math>uu^*</math> are both projectors, called respectively the ''initial'' and the ''final'' projector of <math>u</math> because their (co)domains are respectively the domain and the codomain of <math>u</math>:
* <math>\mathrm{Dom}(u^*u) = \mathrm{Codom}(u^*u) = \mathrm{Dom}(u)</math>;
* <math>\mathrm{Dom}(uu^*) = \mathrm{Codom}(uu^*) = \mathrm{Codom}(u)</math>.

The restriction of <math>u</math> to its domain is an isometry. Projectors are particular examples of partial isometries.

If <math>u</math> is a partial isometry then <math>u^*</math> is also a partial isometry the domain of which is the codomain of <math>u</math> and the codomain of which is the domain of <math>u</math>.

If the domain of <math>u</math> is <math>H</math> that is if <math>u^* u = 1</math> we say that <math>u</math> has ''full domain'', and similarly for codomain. If <math>u</math> and <math>v</math> are two partial isometries then we have:
* <math>uv^* = 0</math> iff <math>u^*uv^*v = 0</math> iff the domains of <math>u</math> and <math>v</math> are disjoint;
* <math>u^*v = 0</math> iff <math>uu^*vv^* = 0</math> iff the codomains of <math>u</math> and <math>v</math> are disjoint;
* <math>uu^* + vv^* = 1</math> iff the codomains of <math>u</math> and <math>v</math> are disjoint and their their direct sum is <math>H</math>.

= Partial permutations =

We will now define our proof space which turns out to be the set of partial isometries acting as permutations on the canonical basis <math>(e_n)_{n\in\mathbb{N}}</math>.

More precisely a ''partial permutation'' <math>\varphi</math> on <math>\mathbb{N}</math> is a one-to-one map defined on a subset <math>D_\varphi</math> of <math>\mathbb{N}</math> onto a subset <math>C_\varphi</math> of <math>\mathbb{N}</math>. <math>D_\varphi</math> is called the ''domain'' of <math>\varphi</math> and <math>C_\varphi</math> its ''codomain''. Partial permutations may be composed: if <math>\psi</math> is another partial permutation on <math>\mathbb{N}</math> then <math>\varphi\circ\psi</math> is defined by:

* <math>n\in D_{\varphi\circ\psi}</math> iff <math>n\in D_\psi</math> and <math>\psi(n)\in D_\varphi</math>;
* if <math>n\in D_{\varphi\circ\psi}</math> then <math>\varphi\circ\psi(n) = \varphi(\psi(n))</math>;
* the codomain of <math>\varphi\circ\psi</math> is the image of the domain: <math>C_{\varphi\circ\psi} = \{\varphi(\psi(n)), n\in D_{\varphi\circ\psi}\}</math>.

Partial permutations are well known to form a structure of ''inverse monoid'' that we detail now.

Given a a subset <math>D</math> of <math>\mathbb{N}</math>, the ''partial identity'' on <math>D</math> is the partial permutation <math>\varphi</math> defined by:
* <math>D_\varphi = D</math>;
* <math>\varphi(n) = n</math> for any <math>n\in D_\varphi</math>.
Thus the codomain of <math>\varphi</math> is <math>D</math>.

The partial identity on <math>D</math> will be denoted by <math>1_D</math>. Partial identities are idempotent for composition.

Among partial identities one finds the identity on the empty subset, that is the empty map, that we will denote by <math>0</math> and the identity on <math>\mathbb{N}</math> that we will denote by <math>1</math>. This latter permutation is the neutral for composition.

If <math>\varphi</math> is a partial permutation there is an inverse partial permutation <math>\varphi^{-1}</math> whose domain is <math>D_{\varphi^{-1}} = C_{\varphi}</math> and who satisfies:

: <math>\varphi^{-1}\circ\varphi = 1_{D_\varphi}</math>
: <math>\varphi\circ\varphi^{-1} = 1_{C_\varphi}</math>

= The proof space =

Given a partial permutation <math>\varphi</math> one defines a partial isometry <math>u_\varphi</math> by:
: <math>u_\varphi(e_n) = 
   \begin{cases}
     e_{\varphi(n)} &amp; \text{ if }n\in D_\varphi,\\
     0              &amp; \text{ otherwise.}
   \end{cases}
  </math>
In other terms if <math>x=(x_n)_{n\in\mathbb{N}}</math> is a sequence in <math>\ell^2</math> then <math>u_\varphi(x)</math> is the sequence <math>(y_n)_{n\in\mathbb{N}}</math> defined by:
: <math>y_n = x_{\varphi^{-1}(n)}</math> if <math>n\in C_\varphi</math>, <math>0</math> otherwise.

We will (not so abusively) write <math>e_{\varphi(n)} = 0</math> when <math>\varphi(n)</math> is undefined so that the definition of <math>u_\varphi</math> reads:
: <math>u_\varphi(e_n) = e_{\varphi(n)}</math>.

The domain of <math>u_\varphi</math> is the subspace spanned by the family <math>(e_n)_{n\in D_\varphi}</math> and the codomain of <math>u_\varphi</math> is the subspace spanned by <math>(e_n)_{n\in C_\varphi}</math>. In particular if <math>\varphi</math> is <math>1_D</math> then <math>u_\varphi</math> is the projector on the subspace spanned by <math>(e_n)_{n\in D}</math>.

{{Definition|
We call ''<math>p</math>-isometry'' a partial isometry of the form <math>u_\varphi</math> where <math>\varphi</math> is a partial permutation on <math>\mathbb{N}</math>. The ''proof space'' <math>\mathcal{P}</math> is the set of all <math>p</math>-isometries.
}}

{{Proposition|
Let <math>\varphi</math> and <math>\psi</math> be two partial permutations. We have:
: <math>u_\varphi u_\psi = u_{\varphi\circ\psi}</math>.

The adjoint of <math>u_\varphi</math> is:
: <math>u_\varphi^* = u_{\varphi^{-1}}</math>.

In particular the initial projector of <math>u_{\varphi}</math> is given by:
: <math>u_\varphi u^*_\varphi = u_{1_{D_\varphi}}</math>.

and the final projector of <math>u_\varphi</math> is:
: <math>u^*_\varphi u_\varphi = u_{1_{C_\varphi}}</math>.

If <math>p</math> is a projector in <math>\mathcal{P}</math> then there is a partial identity <math>1_D</math> such that <math>p= u_{1_D}</math>.

Projectors commute, in particular we have:
: <math>u_\varphi u_\varphi^*u_\psi u_\psi^* = u_\psi u_\psi^*u_\varphi u_\varphi^*</math>.
}}

Note that this entails all the other commutations of projectors: <math>u^*_\varphi u_\varphi u_\psi u^*_\psi = u_\psi u^*_\psi u^*_\varphi u_\varphi</math> and <math>u^*_\varphi u_\varphi u^*_\psi u\psi = u^*_\psi u_\psi u^*_\varphi u_\varphi</math>.

In particular note that <math>0</math> is a <math>p</math>-isometry. The set <math>\mathcal{P}</math> is a submonoid of <math>\mathcal{B}(H)</math> but it is not a subalgebra.<ref><math>\mathcal{P}</math> is the normalizing groupoid of the maximal commutative subalgebra of <math>\mathcal{B}(H)</math> consisiting of all operators ''diagonalizable'' in the canonical basis.</ref>In general given <math>u,v\in\mathcal{P}</math> we don't necessarily have <math>u+v\in\mathcal{P}</math>. However we have:

{{Proposition|
Let <math>u, v\in\mathcal{P}</math>. Then <math>u+v\in\mathcal{P}</math> iff <math>u</math> and <math>v</math> have disjoint domains and disjoint codomains, that is:
: <math>u+v\in\mathcal{P}</math> iff <math>uu^*vv^* = u^*uv^*v = 0</math>.
}}

{{Proof|
Suppose for contradiction that <math>e_n</math> is in the domains of <math>u</math> and <math>v</math>. There are integers <math>p</math> and <math>q</math> such that <math>u(e_n) = e_p</math> and <math>v(e_n) = e_q</math> thus <math>(u+v)(e_n) = e_p + e_q</math> which is not a basis vector; therefore <math>u+v</math> is not a <math>p</math>-permutation.
}}

As a corollary note that if <math>u+v=0</math> then <math>u=v=0</math>.

= From operators to matrices: internalization/externalization =

It will be convenient to view operators on <math>H</math> as acting on <math>H\oplus H</math>, and conversely. For this purpose we define an isomorphism <math>H\oplus H \cong H</math> by <math>x\oplus y\rightsquigarrow p(x)+q(y)</math> where <math>p:H\mapsto H</math> and <math>q:H\mapsto H</math> are partial isometries given by:

: <math>p(e_n) = e_{2n}</math>,
: <math>q(e_n) = e_{2n+1}</math>.

From the definition <math>p</math> and <math>q</math> have full domain, that is
satisfy <math>p^* p = q^* q = 1</math>. On the other hand their codomains are
disjoint, thus we have <math>p^*q = q^*p = 0</math>. As the sum of their
codomains is the full space <math>H</math> we also have <math>pp^* + qq^* = 1</math>.

Note that we have choosen <math>p</math> and <math>q</math> in <math>\mathcal{P}</math>. However the choice is arbitrary: any two <math>p</math>-isometries with full domain and disjoint codomains would do the job.

Given an operator <math>u</math> on <math>H</math> we may ''externalize'' it obtaining an operator <math>U</math> on <math>H\oplus H</math> defined by the matrix:
: <math>U = \begin{pmatrix}
  u_{11} &amp; u_{12}\\
  u_{21} &amp; u_{22}
  \end{pmatrix}</math>
where the <math>u_{ij}</math>'s are given by:
: <math>u_{11} = p^*up</math>;
: <math>u_{12} = p^*uq</math>;
: <math>u_{21} = q^*up</math>;
: <math>u_{22} = q^*uq</math>.

The <math>u_{ij}</math>'s are called the ''external components'' of <math>u</math>. The externalization is functorial in the sense that if <math>v</math> is another operator externalized as:
: <math>V = \begin{pmatrix}
  v_{11} &amp; v_{12}\\
  v_{21} &amp; v_{22}
  \end{pmatrix} 
= \begin{pmatrix}
  p^*vp &amp; p^*vq\\
  q^*vp &amp; q^*vq
  \end{pmatrix}
</math>
then the externalization of <math>uv</math> is the matrix product <math>UV</math>.

As <math>pp^* + qq^* = 1</math> we have:
: <math>u = (pp^*+qq^*)u(pp^*+qq^*) = pu_{11}p^* + pu_{12}q^* + qu_{21}p^* + qu_{22}q^*</math>
which entails that externalization is reversible, its converse being called ''internalization''.

If we suppose that <math>u</math> is a <math>p</math>-isometry then so are the components <math>u_{ij}</math>'s. Thus the formula above entails that the four terms of the sum have pairwise disjoint domains and pairwise disjoint codomains from which we deduce:

{{Proposition|
If <math>u</math> is a <math>p</math>-isometry and <math>u_{ij}</math> are its external components then:
* <math>u_{1j}</math> and <math>u_{2j}</math> have disjoint domains, that is <math>u_{1j}^*u_{1j}u_{2j}^*u_{2j} = 0</math> for <math>j=1,2</math>;
* <math>u_{i1}</math> and <math>u_{i2}</math> have disjoint codomains, that is <math>u_{i1}u_{i1}^*u_{i2}u_{i2}^* = 0</math> for <math>i=1,2</math>.
}}

As an example of computation in <math>\mathcal{P}</math> let us check that the product of the final projectors of <math>pu_{11}p^*</math> and <math>pu_{12}q^*</math> is null:
: <math>\begin{align}
    (pu_{11}p^*)(pu^*_{11}p^*)(pu_{12}q^*)(qu_{12}^*p^*)
    &amp;= pu_{11}u_{11}^*u_{12}u_{12}^*p^*\\
    &amp;= pp^*upp^*u^*pp^*uqq^*u^*pp^*\\
    &amp;= pp^*u(pp^*)(u^*pp^*u)qq^*u^*pp^*\\
    &amp;= pp^*u(u^*pp^*u)(pp^*)qq^*u^*pp^*\\
    &amp;= pp^*uu^*pp^*u(pp^*)(qq^*)u^*pp^*\\
    &amp;= 0
  \end{align}</math>
where we used the fact that all projectors in <math>\mathcal{P}</math> commute, which is in particular the case of <math>pp^*</math> and <math>u^*pp^*u</math>.


= GoI for MELL: the *-autonomous structure =


Recall that when <math>u</math> and <math>v</math> are <math>p</math>-isometries we say they are dual when <math>uv</math> is nilpotent, and that <math>\bot</math> denotes the set of nilpotent operators. A ''type'' is a subset of <math>\mathcal{P}</math> that is equal to its bidual. In particular <math>X\orth</math> is a type for any <math>X\subset\mathcal{P}</math>. We say that <math>X</math> ''generates'' the type <math>X\biorth</math>.

= The tensor and the linear application =

If <math>u</math> and <math>v</math> are two <math>p</math>-isometries summing them doesn't in general produces a <math>p</math>-isometry. However as <math>pup^*</math> and <math>qvq^*</math> have disjoint domains and disjoint codomains it is true that <math>pup^* + qvq^*</math> is a <math>p</math>-isometry. Given two types <math>A</math> and <math>B</math>, we thus define their ''tensor'' by:

: <math>A\tens B = \{pup^* + qvq^*, u\in A, v\in B\}\biorth</math>

Note the closure by bidual to make sure that we obtain a type.

From what precedes we see that <math>A\tens B</math> is generated by the internalizations of operators on <math>H\oplus H</math> of the form:
: <math>\begin{pmatrix}
   u &amp; 0\\
   0 &amp; v
  \end{pmatrix}</math>

{{Remark|
This so-called tensor resembles a sum rather than a product. We will stick to this terminology though because it defines the interpretation of the tensor connective of linear logic.
}}

The linear implication is derived from the tensor by duality: given two types <math>A</math> and <math>B</math> the type <math>A\limp B</math> is defined by:
: <math>A\limp B = (A\tens B\orth)\orth</math>.

Unfolding this definition we get:
: <math>A\limp B = \{u\in\mathcal{P}\text{ s.t. } \forall v\in A, \forall w\in B\orth,\, u.(pvp^* + qwq^*) \in\bot\}</math>.

= The identity =

Given a type <math>A</math> we are to find an operator <math>\iota</math> in type <math>A\limp A</math>, thus satisfying:
: <math>\forall u\in A, v\in A\orth,\, \iota(pup^* + qvq^*)\in\bot</math>.

An easy solution is to take <math>\iota = pq^* + qp^*</math>. In this way we get <math>\iota(pup^* + qvq^*) = qup^* + pvq^*</math>. Therefore <math>(\iota(pup^* + qvq^*))^2 = quvq^* + pvup^*</math>, from which one deduces that this operator is nilpotent iff <math>uv</math> is nilpotent. It is the case since <math>u</math> is in <math>A</math> and <math>v</math> in <math>A\orth</math>.

It is interesting to note that the <math>\iota</math> thus defined is actually the internalization of the operator on <math>H\oplus H</math> given by the matrix:
: <math>\begin{pmatrix}0 &amp; 1\\1 &amp; 0\end{pmatrix}</math>.

We will see once the composition is defined that the <math>\iota</math> operator is the interpretation of the identity proof, as expected.

= The execution formula, version 1: application =

{{Definition|
Let <math>u</math> and <math>v</math> be two operators; as above denote by <math>u_{ij}</math> the external components of <math>u</math>. If <math>u_{11}v</math> is nilpotent we define the ''application of <math>u</math> to <math>v</math>'' by:
: <math>\mathrm{App}(u,v) = u_{22} + u_{21}v\sum_k(u_{11}v)^ku_{12}</math>.
}}

Note that the hypothesis that <math>u_{11}v</math> is nilpotent entails that the sum <math>\sum_k(u_{11}v)^k</math> is actually finite. It would be enough to assume that this sum converges. For simplicity we stick to the nilpotency condition, but we should mention that weak nilpotency would do as well.

{{Theorem|
If <math>u</math> and <math>v</math> are <math>p</math>-isometries such that <math>u_{11}v</math> is nilpotent, then <math>\mathrm{App}(u,v)</math> is also a <math>p</math>-isometry.
}}

{{Proof|
Let us note <math>E_k = u_{21}v(u_{11}v)^ku_{12}</math>. Recall that <math>u_{22}</math> and <math>u_{12}</math> being external components of the <math>p</math>-isometry <math>u</math>, they have disjoint domains. Thus it is also the case of <math>u_{22}</math> and <math>E_k</math>. Similarly <math>u_{22}</math> and <math>E_k</math> have disjoint codomains because <math>u_{22}</math> and <math>u_{21}</math> have disjoint codomains.

Let now <math>k</math> and <math>l</math> be two integers such that <math>k>l</math> and let us compute for example the intersection of the codomains of <math>E_k</math> and <math>E_l</math>:
: <math>
    E_kE^*_kE_lE^*_l = (u_{21}v(u_{11}v)^ku_{12})(u^*_{12}(v^*u^*_{11})^kv^*u^*_{21})(u_{21}v(u_{11}v)^lu_{12})(u^*_{12}(v^*u^*_{11})^lv^*u_{21}^*)
  </math>
As <math>k>l</math> we may write <math>(v^*u_{11}^*)^l = (v^*u^*_{11})^{k-l-1}v^*u^*_{11}(v^*u^*_{11})^l</math>. Let us note <math>E = u^*_{11}(v^*u^*_{11})^lv^*u_{21}^*u_{21}v(u_{11}v)^lu_{12}</math> so that <math>E_kE^*_kE_lE^*_l = u_{21}v(u_{11}v)^ku_{12}u^*_{12}(v^*u^*_{11})^{k-l-1}v^*Eu^*_{12}(v^*u^*_{11})^lv^*u_{21}^*</math>. We have:
: <math>\begin{align}
     E &amp;= u^*_{11}(v^*u^*_{11})^lv^*u_{21}^*u_{21}v(u_{11}v)^lu_{12}\\
       &amp;= (u^*_{11}u_{11}u^*_{11})(v^*u^*_{11})^lv^*u_{21}^*u_{21}v(u_{11}v)^lu_{12}\\
       &amp;= u^*_{11}(u_{11}u^*_{11})\bigl((v^*u^*_{11})^lv^*u_{21}^*u_{21}v(u_{11}v)^l\bigr)u_{12}\\
       &amp;= u^*_{11}\bigl((v^*u^*_{11})^lv^*u_{21}^*u_{21}v(u_{11}v)^l\bigr)(u_{11}u^*_{11})u_{12}\\
       &amp;= u^*_{11}(v^*u^*_{11})^lv^*u_{21}^*u_{21}v(u_{11}v)^lu_{11}u^*_{11}u_{12}\\
       &amp;= 0
  \end{align}</math>
because <math>u_{11}</math> and <math>u_{12}</math> have disjoint codomains, thus <math>u^*_{11}u_{12} = 0</math>. 

Similarly we can show that <math>E_k</math> and <math>E_l</math> have disjoint domains. Therefore we have proved that all terms of the sum <math>\mathrm{App}(u,v)</math> have disjoint domains and disjoint codomains. Consequently <math>\mathrm{App}(u,v)</math> is a <math>p</math>-isometry.
}}

{{Theorem|
Let <math>A</math> and <math>B</math> be two types and <math>u</math> a <math>p</math>-isometry. Then the two following conditions are equivalent:
# <math>u\in A\limp B</math>;
# for any <math>v\in A</math> we have:
#* <math>u_{11}v</math> is nilpotent and
#*  <math>\mathrm{App}(u, v)\in B</math>.
}}

{{Proof|
Let <math>v</math> and <math>w</math> be two <math>p</math>-isometries. If we compute
: <math>(u.(pvp^* + qwq^*))^n = \bigl((pu_{11}p^* + pu_{12}q^* + qu_{21}p^* + qu_{22}q^*)(pvp^* + qwq^*)\bigr)^n</math>
we get a finite sum of monomial operators of the form:
# <math>p(u_{11}v)^{i_0}u_{12}w(u_{22}w)^{i_1}\dots u_{21}v(u_{11}v)^{i_m}p^*</math>
# <math>p(u_{11}v)^{i_0}u_{12}w(u_{22}w)^{i_1}\dots u_{12}w(u_{22}w)^{i_m}q^*</math>,
# <math>q(u_{22}w)^{i_0}u_{21}v(u_{11}v)^{i_1}\dots u_{21}v(u_{11}v)^{i_m}p^*</math> or
# <math>q(u_{22}w)^{i_0}u_{21}v(u_{11}v)^{i_1}\dots u_{12}w(u_{22}w)^{i_m}q^*</math>,
for all tuples of (nonnegative) integers <math>(i_1,\dots, i_m)</math> such that <math>i_0+\cdots+i_m+m = n</math>.

Each of these monomial is a <math>p</math>-isometry. Furthermore they have disjoint domains and disjoint codomains because their sum is the <math>p</math>-isometry <math>(u.(pvp^* + qwq^*))^n</math>. This entails that <math>(u.(pvp^* + qwq^*))^n = 0</math> iff all these monomials are null.

Suppose <math>u_{11}v</math> is nilpotent and consider:
: <math>\bigl(\mathrm{App}(u,v)w\bigr)^n = \biggl(\bigl(u_{22} + u_{21}v\sum_k(u_{11}v)^k u_{12}\bigr)w\biggr)^n</math>.
Developping we get a finite sum of monomials of the form:
: 5. <math>(u_{22}w)^{l_0}u_{21}v(u_{11}v)^{k_1}u_{12}w(u_{22}w)^{l_1}\dots u_{21}v(u_{11}v)^{k_m}u_{12}w(u_{22}w)^{l_m}</math>
for all tuples <math>(l_0, k_1, l_1,\dots, k_m, l_m)</math> such that <math>l_0\cdots l_m + m = n</math> and <math>k_i</math> is less than the degree of nilpotency of <math>u_{11}v</math> for all <math>i</math>.

Again as these monomials are <math>p</math>-isometries and their sum is the <math>p</math>-isometry <math>(\mathrm{App}(u,v)w)^n</math>, they have pairwise disjoint domains and pairwise disjoint codomains. Note that each of these monomial is equal to <math>q^*Mq</math> where <math>M</math> is a monomial of type 4 above.

As before we thus have that <math>\bigl(\mathrm{App}(u,v)w\bigr)^n = 0</math> iff all monomials of type 5 are null.

Suppose now that <math>u\in A\limp B</math> and <math>v\in A</math>. Then, since <math>0\in B\orth</math> (<math>0</math> belongs to any type) <math>u.(pvp^*) = pu_{11}vp^*</math> is nilpotent, thus <math>u_{11}v</math> is nilpotent.

Suppose further that <math>w\in B\orth</math>. Then <math>u.(pvp^*+qwq^*)</math> is nilpotent, thus there is a <math>N</math> such that <math>(u.(pvp^* + qwq^*))^n=0</math> for any <math>n\geq N</math>. This entails that all monomials of type 1 to 4 are null. Therefore all monomials appearing in the developpment of <math>(\mathrm{App}(u,v)w)^N</math> are null which proves that <math>\mathrm{App}(u,v)w</math> is nilpotent. Thus <math>\mathrm{App}(u,v)\in B</math>.

Conversely suppose for any <math>v\in A</math> and <math>w\in B\orth</math>, <math>u_{11}v</math> and <math>\mathrm{App}(u,v)w</math> are nilpotent. Let <math>P</math> and <math>N</math> be their respective degrees of nilpotency and put <math>n=N(P+1)+N</math>. Then we claim that all monomials of type 1 to 4 appearing in the development of <math>(u.(pvp^*+qwq^*))^n</math> are null.

Consider for example a monomial of type 1:
: <math>p(u_{11}v)^{i_0}u_{12}w(u_{22}w)^{i_1}\dots u_{21}v(u_{11}v)^{i_m}p^*</math>
with <math>i_0+\cdots+i_m + m = n</math>. Note that <math>m</math> must be even.

If <math>i_{2k}\geq P</math> for some <math>0\leq k\leq m/2</math> then <math>(u_{11}v)^{i_{2k}}=0</math> thus our monomial is null. Otherwise if <math>i_{2k}<P</math> for all <math>k</math> we have:
: <math>i_1+i_3+\cdots +i_{m-1} + m/2 = n - m/2 - (i_0+i_2+\cdots +i_m)</math>
thus:
: <math>i_1+i_3+\cdots +i_{m-1} + m/2\geq n - m/2 - (1+m/2)P</math>.
Now if <math>m/2\geq N</math> then <math>i_1+\cdots+i_{m-1}+m/2 \geq N</math>. Otherwise <math>1+m/2\leq N</math> thus
: <math>i_1+i_3+\cdots +i_{m-1} + m/2\geq n - N - NP = N</math>.
Since <math>N</math> is the degree of nilpotency of <math>\mathrm{App}(u,v)w</math> we have that the monomial:
: <math>(u_{22}w)^{i_1}u_{21}v(u_{11}v)^{i_2}u_{12}w\dots(u_{11}v)^{i_{m-2}}u_{12}w(u_{22}w)^{i_{m-1}}</math>
is null, thus also the monomial of type 1 we started with.
}}

{{Corollary|
If <math>A</math> and <math>B</math> are types then we have:
: <math>A\limp B = \{u\in\mathcal{P} \text{ such that }\forall v\in A: u_{11}v\in\bot\text{ and } \mathrm{App}(u, v)\in B\}</math>.
}}

As an example if we compute the application of the interpretation of the identity <math>\iota</math> in type <math>A\limp A</math> to the operator <math>v\in A</math> then we have:
: <math>\mathrm{App}(\iota, v) = \iota_{22} + \iota_{21}v\sum(\iota_{11}v)^k\iota_{12}</math>.
Now recall that <math>\iota = pq^* + qp^*</math> so that <math>\iota_{11} = \iota_{22} = 0</math> and <math>\iota_{12} = \iota_{21} = 1</math> and we thus get:
: <math>\mathrm{App}(\iota, v) = v</math>
as expected.

= The tensor rule =

Let now <math>A, A', B</math> and <math>B'</math> be types and consider two operators <math>u</math> and <math>u'</math> respectively in <math>A\limp B</math> and <math>A\limp B'</math>. We define an operator <math>u\tens u'</math> by:
: <math>\begin{align}
    u\tens u' &amp;= ppp^*upp^*p^* + qpq^*upp^*p^* + ppp^*uqp^*q^* + qpq^*uqp^*q^*\\
              &amp;+ pqp^*u'pq^*p^* + qqq^*u'pq^*p^* + pqp^*u'qq^*q^* + qqq^*u'qq^*q^*
  \end{align}</math>

Once again the notation is motivated by linear logic syntax and is contradictory with linear algebra practice since what we denote by <math>u\tens u'</math> actually is the internalization of the direct sum <math>u\oplus u'</math>.

Indeed if we think of <math>u</math> and <math>u'</math> as the internalizations of the matrices:
: <math>
    \begin{pmatrix}u_{11}   &amp; u_{12}\\
                   u_{21}   &amp; u_{22}
    \end{pmatrix}
  </math> and <math>
    \begin{pmatrix}u'_{11} &amp; u'_{12}\\
                   u'_{21} &amp; u'_{22}
    \end{pmatrix}</math>
then we may write:
: <math>\begin{align}
    u\tens u' &amp;= ppu_{11}p^*p^* + qpu_{21}p^*p^* + ppu_{12}p^*q^* + qpu_{22}p^*q^*\\
              &amp;+ pqu'_{11}q^*p^* + qqu'_{21}q^*p^* + pqu'_{12}q^*q^* + qqu'_{22}q^*q^*
  \end{align}</math>

Thus the components of <math>u\tens u'</math> are given by:
: <math>(u\tens u')_{ij} = pu_{ij}p^* + qu'_{ij}q^*</math>.
and we see that <math>u\tens u'</math> is actually the internalization of the matrix:
: <math>
    \begin{pmatrix}
      u_{11} &amp; 0       &amp; u_{12}  &amp; 0       \\
      0      &amp; u'_{11} &amp; 0       &amp; u'_{12} \\
      u_{21} &amp; 0       &amp; u_{22}  &amp; 0       \\
      0      &amp; u'_{21} &amp; 0       &amp; u'_{22} \\
    \end{pmatrix}
  </math>

We are now to show that if we suppose <math>u</math>and <math>u'</math> are in types <math>A\limp B</math> and <math>A'\limp B'</math>, then <math>u\tens u'</math> is in <math>A\tens A'\limp B\tens B'</math>. For this we consider <math>v</math> and <math>v'</math> respectively in <math>A</math> and <math>A'</math>, so that <math>pvp^* + qv'q^*</math> is in <math>A\tens A'</math>, and we show that <math>\mathrm{App}(u\tens u', pvp^* + qv'q^*)\in B\tens B'</math>.

Since <math>u</math> and <math>u'</math> are in <math>A\limp B</math> and <math>A'\limp B'</math> we have that <math>u_{11}v</math> and <math>u'_{11}v'</math> are nilpotent and that <math>\mathrm{App}(u, v)</math> and <math>\mathrm{App}(u', v')</math> are respectively in <math>B</math> and <math>B'</math>, thus:
: <math>p\mathrm{App}(u, v)p^* + q\mathrm{App}(u', v')q^* \in B\tens B'</math>.

But we have:
: <math>\begin{align}
    \bigl((u\tens u')_{11}(pvp^* + qv'q^*)\bigr)^n
      &amp;= \bigl((pu_{11}p^* + qu'_{11}q^*)(pvp^* + qv'q^*)\bigr)^n\\
      &amp;= (pu_{11}vp^* + qu'_{11}v'q^*)^n\\
      &amp;= p(u_{11}v)^np^* + q(u'_{11}v')^nq^*
  \end{align}</math>

Therefore <math>(u\tens u')_{11}(pvp^* + qv'q^*)</math> is nilpotent. So we can compute <math>\mathrm{App}(u\tens u', pvp^* + qv'q^*)</math>:
: <math>\begin{align}
    &amp;\mathrm{App}(u\tens u', pvp^* + qv'q^*)\\
      &amp;= (u\tens u')_{22} + (u\tens u')_{21}(pvp^* + qv'q^*)\sum\bigl((u\tens u')_{11}(pvp^* + qv'q^*)\bigr)^k(u\tens u')_{12}\\
      &amp;= pu_{22}p^* + qu'_{22}q^* + (pu_{21}p^* + qu'_{21}q^*)(pvp^* + qv'q^*)\sum\bigl((pu_{11}p^* + qu'_{11}q^*)(pvp^* + qv'q^*)\bigr)^k(pu_{12}p^* + qu'_{12}q^*)\\
      &amp;= p\bigl(u_{22} + u_{21}v\sum(u_{11}v)^ku_{12}\bigr)p^* + q\bigl(u'_{22} + u'_{21}v'\sum(u'_{11}v')^ku'_{12}\bigr)q^*\\
      &amp;= p\mathrm{App}(u, v)p^* + q\mathrm{App}(u', v')q^*
  \end{align}</math>
thus lives in <math>B\tens B'</math>.

= Other monoidal constructions =

== Contraposition ==

Let <math>A</math> and <math>B</math> be some types; we have:
: <math>A\limp B = A\orth\limpinv B\orth</math>

Indeed, <math>u\in A\limp B</math> means that for any <math>v</math> and <math>w</math> in respectively <math>A</math> and <math>B\orth</math> we have <math>u.(pvp^* + qwq^*)\in\bot</math> which is exactly the definition of <math>A\orth\limpinv B\orth</math>.

We will denote <math>u\orth</math> the operator:
: <math>u\orth = pu_{22}p^* + pu_{12}q^* + qu_{12}p^* + qu_{11}q^*</math>
where <math>u_{ij}</math> is given by externalization. Therefore the externalization of <math>u\orth</math> is:
: <math>(u\orth)_{ij} = u_{\bar i\,\bar j}</math> where <math>\bar .</math> is defined by <math>\bar1 = 2, \bar2 = 1</math>.
From this we deduce that <math>u\orth\in B\orth\limp A\orth</math> and that <math>(u\orth)\orth = u</math>.

== Commutativity ==
Let <math>\sigma</math> be the operator:
: <math>\sigma = ppq^*q^* +pqp^*q^* + qpq^*p^* + qqp^*p^*</math>.
One can check that <math>\sigma</math> is the internalization of the operator <math>S</math> on <math>H\oplus H\oplus H\oplus H</math> defined by: <math>S(x_1\oplus x_2\oplus x_3\oplus x_4) = x_4\oplus x_3\oplus x_2\oplus x_1</math>. In particular the components of <math>\sigma</math> are:
: <math>\sigma_{11} = \sigma_{22} = 0</math>;
: <math>\sigma_{12} = \sigma_{21} = pq^* + qp^*</math>.

Let <math>A</math> and <math>B</math> be types and <math>u</math> and <math>v</math> be operators in <math>A</math> and <math>B</math>. Then <math>pup^* + qvq^*</math> is in <math>A\tens B</math> and as <math>\sigma_{11}.(pup^* + qvq^*) = 0</math> we may compute:
: <math>\begin{align}
    \mathrm{App}(\sigma, pup^* + qvq^*) 
      &amp;= \sigma_{22} + \sigma_{21}(pup^* + qvq^*)\sum(\sigma_{11}(pup^* + qvq^*))^k\sigma_{12}\\
      &amp;= (pq^* + qp^*)(pup^* + qvq^*)(pq^* + qp^*)\\
      &amp;= pvp^* + quq^*
   \end{align}</math>
But <math>pvp^* + quq^*\in B\tens A</math>, thus we have shown that:
: <math>\sigma\in (A\tens B) \limp (B\tens A)</math>.

== Distributivity ==
We get distributivity by considering the operator:
: <math>\delta = ppp^*p^*q^* + pqpq^*p^*q^* + pqqq^*q^* + qppp^*p^* + qpqp^*q^*p^* + qqq^*q^*p^*</math>
that is similarly shown to be in type <math>A\tens(B\tens C)\limp(A\tens B)\tens C</math> for any types <math>A</math>, <math>B</math> and <math>C</math>.


== Weak distributivity ==
Similarly we get weak distributivity thanks to the operators:
: <math>\delta_1 = pppp^*q^* + ppqp^*q^*q^* + pqq^*q^*q^* + qpp^*p^*p^* + qqp q^*p^*p^* + qqq q^*p^*</math> and
: <math>\delta_2 = ppp^*p^*q^* + pqpq^*p^*q^* + pqqq^*q^* + qppp^*p^* + qpqp^*q^*p^* + qqq^*q^*p^*</math>.

Given three types <math>A</math>, <math>B</math> and <math>C</math> then one can show that:
: <math>\delta_1</math> has type <math>((A\limp B)\tens C)\limp A\limp (B\tens C)</math> and
: <math>\delta_2</math> has type <math>(A\tens(B\limp C))\limp (A\limp B)\limp C</math>.

= Execution formula, version 2: composition =

Let <math>A</math>, <math>B</math> and <math>C</math> be types and <math>u</math> and <math>v</math> be operators respectively in types <math>A\limp B</math> and <math>B\limp C</math>.

As usual we will denote <math>u_{ij}</math> and <math>v_{ij}</math> the operators obtained by externalization of <math>u</math> and <math>v</math>, eg, <math>u_{11} = p^*up</math>, ...

As <math>u</math> is in <math>A\limp B</math> we have that <math>\mathrm{App}(u, 0)=u_{22}\in B</math>; similarly as <math>v\in B\limp C</math>, thus <math>v\orth\in C\orth\limp B\orth</math>, we have <math>\mathrm{App}(v\orth, 0) = v_{11}\in B\orth</math>. Thus <math>u_{22}v_{11}</math> is nilpotent.

We define the operator <math>\mathrm{Comp}(u, v)</math> by:
: <math>\begin{align}
    \mathrm{Comp}(u, v) &amp;= p(u_{11} + u_{12}\sum(v_{11}u_{22})^k\,v_{11}u_{21})p^*\\
                        &amp;+ p(u_{12}\sum(v_{11}u_{22})^k\,v_{12})q^*\\
                        &amp;+ q(v_{21}\sum(u_{22}v_{11})^k\,u_{21})p^*\\
			&amp;+ q(v_{22} + v_{21}\sum(u_{22}v_{11})^k\,u_{22}v_{12})q^*
  \end{align}</math>

This is well defined since <math>u_{11}v_{22}</math> is nilpotent. As an example let us compute the composition of <math>u</math> and <math>\iota</math> in type <math>B\limp B</math>; recall that <math>\iota_{ij} = \delta_{ij}</math>, so we get:
: <math>
    \mathrm{Comp}(u, \iota) = pu_{11}p^* + pu_{12}q^* + qu_{21}p^* + qu_{22}q^*  = u
 </math>
Similar computation would show that <math>\mathrm{Comp}(\iota, v) = v</math> (we use <math>pp^* + qq^* = 1</math> here).

Coming back to the general case we claim that <math>\mathrm{Comp}(u, v)</math> is in <math>A\limp C</math>: let <math>a</math> be an operator in <math>A</math>. By computation we can check that:
: <math>\mathrm{App}(\mathrm{Comp}(u, v), a) = \mathrm{App}(v, \mathrm{App}(u, a))</math>.
Now since <math>u</math> is in <math>A\limp B</math>, <math>\mathrm{App}(u, a)</math> is in <math>B</math> and since <math>v</math> is in <math>B\limp C</math>, <math>\mathrm{App}(v, \mathrm{App}(u, a))</math> is in <math>C</math>.

If we now consider a type <math>D</math> and an operator <math>w</math> in <math>C\limp D</math> then we have:
: <math>\mathrm{Comp}(\mathrm{Comp}(u, v), w) = \mathrm{Comp}(u,
\mathrm{Comp}(v, w))</math>.

Putting together the results of this section we finally have:

{{Theorem|
Let GoI(H) be defined by:
* objects are types, ''ie'' sets <math>A</math> of <math>p</math>-isometries satisfying: <math>A\biorth = A</math>;
* morphisms from <math>A</math> to <math>B</math> are <math>p</math>-isometries in type <math>A\limp B</math>;
* composition is given by the formula above.

Then GoI(H) is a star-autonomous category.
}}


= Intuitionistic linear logic =


Intuitionistic Linear Logic (<math>ILL</math>) is the
[[intuitionnistic restriction]] of linear logic: the sequent calculus
of <math>ILL</math> is obtained from the [[Sequent calculus#Sequents and proofs|two-sided sequent calculus of
linear logic]] by constraining sequents to have exactly one formula on
the right-hand side: <math>\Gamma\vdash A</math>.

The connectives <math>\parr</math>, <math>\bot</math> and <math>\wn</math> are
not available anymore, but the linear implication <math>\limp</math> is.

== Sequent Calculus ==

<math>
\LabelRule{\rulename{ax}}
\NulRule{A\vdash A}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,A\vdash C}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,\Delta\vdash C}
\DisplayProof
</math>

<br>

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta\vdash B}
\LabelRule{\tens R}
\BinRule{\Gamma,\Delta\vdash A\tens B}
\DisplayProof
\qquad
\AxRule{\Gamma,A,B\vdash C}
\LabelRule{\tens L}
\UnaRule{\Gamma,A\tens B\vdash C}
\DisplayProof
\qquad
\LabelRule{\one R}
\NulRule{{}\vdash\one}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash C}
\LabelRule{\one L}
\UnaRule{\Gamma,\one\vdash C}
\DisplayProof
</math>

<br>

<math>
\AxRule{\Gamma,A\vdash B}
\LabelRule{\limp R}
\UnaRule{\Gamma\vdash A\limp B}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,B\vdash C}
\LabelRule{\limp L}
\BinRule{\Gamma,\Delta,A\limp B\vdash C}
\DisplayProof
</math>

<br>

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Gamma\vdash B}
\LabelRule{\with R}
\BinRule{\Gamma\vdash A\with B}
\DisplayProof
\qquad
\AxRule{\Gamma,A\vdash C}
\LabelRule{\with_1 L}
\UnaRule{\Gamma,A\with B\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma,B\vdash C}
\LabelRule{\with_2 L}
\UnaRule{\Gamma,A\with B\vdash C}
\DisplayProof
\qquad
\LabelRule{\top R}
\NulRule{\Gamma\vdash\top}
\DisplayProof
</math>

<br>

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\plus_1 R}
\UnaRule{\Gamma\vdash A\plus B}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash B}
\LabelRule{\plus_2 R}
\UnaRule{\Gamma\vdash A\plus B}
\DisplayProof
\qquad
\AxRule{\Gamma,A\vdash C}
\AxRule{\Gamma,B\vdash C}
\LabelRule{\plus L}
\BinRule{\Gamma,A\plus B\vdash C}
\DisplayProof
\qquad
\LabelRule{\zero L}
\NulRule{\Gamma,\zero\vdash C}
\DisplayProof
</math>

<br>

<math>
\AxRule{\oc{\Gamma}\vdash A}
\LabelRule{\oc R}
\UnaRule{\oc{\Gamma}\vdash\oc{A}}
\DisplayProof
\qquad
\AxRule{\Gamma,A\vdash C}
\LabelRule{\oc d L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma,\oc{A},\oc{A}\vdash C}
\LabelRule{\oc c L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash C}
\LabelRule{\oc w L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
</math>

<br>

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\forall R}
\UnaRule{\Gamma\vdash \forall\xi A}
\DisplayProof
\qquad
\AxRule{\Gamma,A[\tau/\xi]\vdash C}
\LabelRule{\forall L}
\UnaRule{\Gamma,\forall\xi A\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash A[\tau/\xi]}
\LabelRule{\exists R}
\UnaRule{\Gamma\vdash\exists\xi A}
\DisplayProof
\qquad
\AxRule{\Gamma,A\vdash C}
\LabelRule{\exists L}
\UnaRule{\Gamma,\exists\xi A\vdash C}
\DisplayProof
</math>

with <math>\xi</math> not free in <math>\Gamma,C</math> in the rules <math>\forall R</math> and <math>\exists L</math>.

== The intuitionistic fragment of linear logic ==

In order to characterize intuitionistic linear logic inside linear logic, we define the intuitionistic restriction of linear formulas:

<math>
J ::= X \mid J\tens J \mid \one \mid J\limp J \mid J\with J \mid \top \mid J\plus J \mid \zero \mid \oc{J} \mid \forall\xi J \mid \exists\xi J
</math>

<math>JLL</math> is the [[fragment]] of linear logic obtained by restriction to intuitionistic formulas.

{{Proposition|title=From <math>ILL</math> to <math>JLL</math>|
If <math>\Gamma\vdash A</math> is provable in <math>ILL_{012}</math>, it is provable in <math>JLL_{012}</math>.
}}

{{Proof|
<math>ILL_{012}</math> is included in <math>JLL_{012}</math>.
}}

{{Theorem|title=From <math>JLL</math> to <math>ILL</math>|
If <math>\Gamma\vdash\Delta</math> is provable in <math>JLL_{12}</math>, it is provable in <math>ILL_{12}</math>.
}}

{{Proof|
We only prove the first order case, a proof of the full result is given in the PhD thesis of Harold Schellinx<ref>{{BibEntry|bibtype=phdthesis|author=Schellinx, Harold|title=The Noble Art of Linear Decorating|type=Dissertation series of the Dutch Institute for Logic, Language and Computation|school=University of Amsterdam|note=ILLC-Dissertation Series, 1994-1|year=1994}}</ref>.

Consider a cut-free proof of <math>\Gamma\vdash\Delta</math> in <math>JLL_{12}</math>, we can prove by induction on the length of such a proof that it belongs to <math>ILL_{12}</math>.
}}

{{Corollary|title=Unique conclusion in <math>JLL</math>|
If <math>\Gamma\vdash\Delta</math> is provable in <math>JLL_{12}</math> then <math>\Delta</math> is a singleton.
}}

The theorem is also valid for formulas containing <math>\one</math> or <math>\top</math> but not anymore with <math>\zero</math>. <math>{}\vdash((X\limp Y)\limp\zero)\limp(X\tens(\zero\limp Z))</math> is provable in <math>JLL_0</math>:

<math>
\LabelRule{\rulename{ax}}
\NulRule{X\vdash X}
\LabelRule{\zero L}
\NulRule{\zero\vdash Y,Z}
\LabelRule{\limp R}
\UnaRule{{}\vdash Y,\zero\limp Z}
\LabelRule{\tens R}
\BinRule{X\vdash Y,X\tens(\zero\limp Z)}
\LabelRule{\limp R}
\UnaRule{{}\vdash X\limp Y,X\tens(\zero\limp Z)}
\LabelRule{\zero L}
\NulRule{\zero\vdash {}}
\LabelRule{\limp L}
\BinRule{(X\limp Y)\limp\zero\vdash X\tens(\zero\limp Z)}
\LabelRule{\limp R}
\UnaRule{{}\vdash((X\limp Y)\limp\zero)\limp(X\tens(\zero\limp Z))}
\DisplayProof
</math>

but not in <math>ILL_0</math>.

== Input / output polarities ==

In order to go to <math>LL</math> without <math>\limp</math>, we consider two classes of formulas: ''input formulas'' (<math>I</math>) and ''output formulas'' (<math>O</math>).

<math>
I ::= X\orth \mid I\parr I \mid \bot \mid I\tens O \mid O\tens I \mid I\plus I \mid \zero \mid I\with I \mid \top \mid \wn{I} \mid \exists\xi I \mid \forall\xi I
</math>
<math>
O ::= X \mid O\tens O \mid \one \mid O\parr I \mid I\parr O \mid O\with O \mid \top \mid O\plus O \mid \zero \mid \oc{O} \mid \forall\xi O \mid \exists\xi O
</math>

By applying the definition of the linear implication <math>A\limp B = A\orth\parr B</math>, any formula of <math>JLL</math> is mapped to an output formula (and the dual of a <math>JLL</math> formula to an input formula). Conversely, any output formula is coming from a <math>JLL</math> formula in this way (up to commutativity of <math>\parr</math>: <math>O\parr I = I\parr O</math>).

The [[fragment]] of linear logic obtained by restriction to input/output formulas is thus equivalent to <math>JLL</math>, but the closure of the set of input/output formulas under orthogonal allows for a one-sided presentation.

<math>
\LabelRule{\rulename{ax}}
\NulRule{\vdash O\orth,O}
\DisplayProof
\qquad
\AxRule{{}\vdash \Gamma,O}
\AxRule{{}\vdash\Delta,O\orth}
\LabelRule{\rulename{cut}}
\BinRule{{}\vdash\Gamma,\Delta}
\DisplayProof
</math>

<br>

<math>
\AxRule{{}\vdash\Gamma,A}
\AxRule{{}\vdash\Delta,B}
\LabelRule{\tens}
\BinRule{{}\vdash\Gamma,\Delta,A\tens B}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma,A,B}
\LabelRule{\parr}
\UnaRule{{}\vdash\Gamma,A\parr B}
\DisplayProof
\qquad
\LabelRule{\one}
\NulRule{{}\vdash\one}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma}
\LabelRule{\bot}
\UnaRule{{}\vdash\Gamma,\bot}
\DisplayProof
</math>

<br>

<math>
\AxRule{{}\vdash\Gamma,A}
\AxRule{{}\vdash\Gamma,B}
\LabelRule{\with}
\BinRule{{}\vdash\Gamma,A\with B}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma,A}
\LabelRule{\plus_1}
\UnaRule{{}\vdash\Gamma,A\plus B}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma,B}
\LabelRule{\plus_2}
\UnaRule{{}\vdash\Gamma,A\plus B}
\DisplayProof
\qquad
\LabelRule{\top}
\NulRule{{}\vdash\Gamma,\top}
\DisplayProof
</math>

<br>

<math>
\AxRule{{}\vdash\wn{\Gamma},O}
\LabelRule{\oc}
\UnaRule{{}\vdash\wn{\Gamma},\oc{O}}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma,I}
\LabelRule{\wn d}
\UnaRule{{}\vdash\Gamma,\wn{I}}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma,\wn{I},\wn{I}}
\LabelRule{\wn c}
\UnaRule{{}\vdash\Gamma,\wn{I}}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma}
\LabelRule{\wn w}
\UnaRule{{}\vdash\Gamma,\wn{I}}
\DisplayProof
</math>

<br>

<math>
\AxRule{{}\vdash\Gamma,A}
\LabelRule{\forall}
\UnaRule{{}\vdash\Gamma,\forall\xi A}
\DisplayProof
\qquad
\AxRule{{}\vdash\Gamma,A[\tau/\xi]}
\LabelRule{\exists}
\UnaRule{{}\vdash\Gamma,\exists\xi A}
\DisplayProof
</math>

with <math>A</math> and <math>B</math> arbitrary input or output formulas (under the condition that the composite formulas containing them are input or output formulas) and <math>\xi</math> not free in <math>\Gamma</math> in the rule <math>\forall</math>.

{{Lemma|title=Output formula|
If <math>{}\vdash\Gamma</math> is provable in <math>LL_{12}</math> and contains only input and output formulas, then <math>\Gamma</math> contains exactly one output formula.
}}

{{Proof|
Assume <math>\Gamma_O</math> is obtained by turning the output formulas of <math>\Gamma</math> into <math>JLL</math> formulas and <math>\Gamma_I</math> is obtained by turning the dual of the input formulas of <math>\Gamma</math> into <math>JLL</math> formulas, <math>\Gamma_I\vdash\Gamma_O</math> is provable in <math>LL_{12}</math> thus in <math>JLL_{12}</math>. By corollary (Unique conclusion in <math>JLL</math>), <math>\Gamma_O</math> is a singleton, thus <math>\Gamma</math> contains exactly one output formula.
}}


= Isomorphism =


{{stub}}

Two formulas <math>A</math> and <math>B</math> are isomorphic (denoted <math>A\cong B</math>), when there are two proofs <math>\pi</math> of <math>A \vdash B</math> and <math>\rho</math> of <math>B \vdash A</math> such that eliminating the cut on <math>A</math> in

<math>\AxRule{}\VdotsRule{\pi}{A \vdash B}\AxRule{}\VdotsRule{\rho}{B \vdash A}\LabelRule{\rulename{cut}}\BinRule{B\vdash B}\DisplayProof</math>

leads to an [[Sequent_calculus#Expansion_of_identities|<math>\eta</math>-expansion]] of 

<math>\LabelRule{\rulename{ax}}\NulRule{B\vdash B}\DisplayProof</math>, 

and eliminating the cut on <math>B</math> in 

<math>\AxRule{}\VdotsRule{\pi}{A \vdash B}\AxRule{}\VdotsRule{\rho}{B \vdash A}\LabelRule{\rulename{cut}}\BinRule{A\vdash A}\DisplayProof</math>

leads to an [[Sequent_calculus#Expansion_of_identities|<math>\eta</math>-expansion]] of 

<math>\LabelRule{\rulename{ax}}\NulRule{A\vdash A}\DisplayProof</math>.

Linear logic admits [[List of isomorphisms|many isomorphisms]], but it is not known wether all of them have been discovered or not.


= Lattice of exponential modalities =


<math>
\xymatrix{
 &amp; &amp; {\wn} \\
 &amp; &amp; &amp; &amp; {\wn\oc\wn}\ar[ull] \\
\varepsilon\ar[uurr] &amp; &amp; &amp; {\oc\wn} \ar[ur] &amp; &amp; {\wn\oc} \ar[ul] \\
 &amp; &amp; &amp; &amp; {\oc\wn\oc} \ar[ul]\ar[ur] \\
 &amp; &amp; {\oc} \ar[uull]\ar[urr]
}
</math>

An ''exponential modality'' is an arbitrary (possibly empty) sequence of the two exponential connectives <math>\oc</math> and <math>\wn</math>. It can be considered itself as a unary connective. This leads to the notation <math>\mu A</math> for applying an exponential modality <math>\mu</math> to a formula <math>A</math>.

There is a preorder relation on exponential modalities defined by <math>\mu\lesssim\nu</math> if and only if for any formula <math>A</math> we have <math>\mu A\vdash \nu A</math>. It induces an [[List of equivalences|equivalence]] relation on exponential modalities by <math>\mu \sim \nu</math> if and only if <math>\mu\lesssim\nu</math> and <math>\nu\lesssim\mu</math>.

{{Lemma|
For any formula <math>A</math>, <math>\oc{A}\vdash A</math> and <math>A\vdash\wn{A}</math>.
}}

{{Lemma|title=Functoriality|
If <math>A</math> and <math>B</math> are two formulas such that <math>A\vdash B</math> then, for any exponential modality <math>\mu</math>, <math>\mu A\vdash \mu B</math>.
}}

{{Lemma|
For any formula <math>A</math>, <math>\oc{A}\vdash \oc{\oc{A}}</math> and <math>\wn{\wn{A}}\vdash\wn{A}</math>.
}}

{{Lemma|
For any formula <math>A</math>, <math>\oc{A}\vdash \oc{\wn{\oc{A}}}</math> and <math>\wn{\oc{\wn{A}}}\vdash\wn{A}</math>.
}}

This allows to prove that any exponential modality is equivalent to one of the following seven modalities: <math>\varepsilon</math> (the empty modality), <math>\oc</math>, <math>\wn</math>, <math>\oc\wn</math>, <math>\wn\oc</math>, <math>\oc\wn\oc</math> or <math>\wn\oc\wn</math>.
Indeed any sequence of consecutive <math>\oc</math> or <math>\wn</math> in a modality can be simplified into only one occurrence, and then any alternating sequence of length at least four can be simplified into a smaller one.

{{Proof|
We obtain <math>\oc{\oc{A}}\vdash\oc{A}</math> by functoriality from <math>\oc{A}\vdash A</math> (and similarly for <math>\wn{A}\vdash\wn{\wn{A}}</math>).
From <math>\oc{A}\vdash \oc{\wn{\oc{A}}}</math>, we deduce <math>\wn{\oc{A}}\vdash \wn{\oc{\wn{\oc{A}}}}</math> by functoriality and <math>\oc{\wn{B}}\vdash \oc{\wn{\oc{\wn{B}}}}</math> (with <math>A=\wn{B}</math>). In a similar way we have <math>\oc{\wn{\oc{\wn{A}}}}\vdash \oc{\wn{A}}</math> and <math>\wn{\oc{\wn{\oc{A}}}}\vdash \wn{\oc{A}}</math>.
}}

The order relation induced on equivalence classes of exponential modalities with respect to <math>\sim</math> can be proved to be the one represented on the picture in the top of this page. All the represented relations are valid.

{{Proof|
We have already seen <math>\oc{A}\vdash A</math> and <math>\oc{A}\vdash \oc{\wn{\oc{A}}}</math>. By functoriality we deduce <math>\oc{\wn{\oc{A}}}\vdash \oc{\wn{A}}</math> and by <math>A=\wn{\oc{B}}</math> we deduce <math>\oc{\wn{\oc{B}}}\vdash \wn{\oc{B}}</math>.

The others are obtained from these ones by duality: <math>A\vdash B</math> entails <math>B\orth\vdash A\orth</math>.
}}

{{Lemma|
If <math>\alpha</math> is an atom, <math>\wn{\alpha}\not\vdash\alpha</math> and 
<math>\alpha\not\vdash\wn{\oc{\wn{\alpha}}}</math>.
}}

We can prove that no other relation between classes is true (by relying on the previous lemma).

{{Proof|
From the lemma and <math>A\vdash\wn{A}</math>, we have <math>\wn{\alpha}\not\vdash\wn{\oc{\wn{\alpha}}}</math>.

Then <math>\wn</math> cannot be smaller than any other of the seven modalities (since they are all smaller than <math>\varepsilon</math> or <math>\wn\oc\wn</math>). For the same reason, <math>\varepsilon</math> cannot be smaller than <math>\oc</math>, <math>\oc\wn</math>, <math>\wn\oc</math> or <math>\oc\wn\oc</math>. This entails that <math>\wn\oc\wn</math> is only smaller than <math>\wn</math> since it is not smaller than <math>\varepsilon</math> (by duality from <math>\varepsilon</math> not smaller than <math>\oc\wn\oc</math>).

From these, <math>\wn{\oc{\alpha}}\not\vdash\oc{\wn{\alpha}}</math> and <math>\oc{\wn{\alpha}}\not\vdash\wn{\oc{\alpha}}</math>, we deduce that no other relation is possible.
}}

The order relation on equivalence classes of exponential modalities is a lattice.


= Light linear logics =


Light linear logics are variants of linear logic characterizing complexity classes. They are designed by defining alternative exponential connectives, which induce a complexity bound on the cut-elimination procedure.<br>
Light linear logics are one of the approaches used in ''implicit computational complexity'',  the area studying the computational complexity of programs without referring to external measuring conditions or particular machine models. 

= Elementary linear logic =
We present here the intuitionistic version of ''elementary linear logic'', ELL. Moreover we restrict to the fragment without additive connectives. <br> The language of formulas is the same one as that of (multiplicative)  [[Intuitionistic linear logic|ILL]]:

<math>
A ::= X \mid A\tens A \mid A\limp A  \mid \oc{A} \mid \forall X A 
</math>
<br>
The sequent calculus rules are the same ones as for [[Intuitionistic linear logic|ILL]], except for the rules
dealing with the exponential connectives:

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\oc\rulename{mf} }
\UnaRule{\oc{\Gamma}\vdash\oc{A}}
\DisplayProof
\qquad
\AxRule{\Gamma,\oc{A},\oc{A}\vdash C}
\LabelRule{\oc c L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash C}
\LabelRule{\oc w L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
</math>

The ''depth'' of a derivation <math>\pi</math> is the maximum number of
<math>(\oc\rulename{mf})</math> rules in a branch of <math>\pi</math>.

We consider the function <math>K(.,.)</math> defined by:<br>
<math>K(0,n)=n, \quad K(k+1,n)=2^{K(k,n)}</math>.

{{Theorem|If <math>\pi</math> is an ELL proof of depth d, and R is the corresponding ELL proof-net, then R can be reduced to its normal form by cut elimination in at most <math> K(d+1,|\pi|)</math> steps, where <math>|\pi|</math>is the size of <math>\pi</math>.}}

A function f on integers is ''elementary recursive'' if there exists an integer h and a  Turing  machine 
which  computes  f in time bounded by  <math>K(h,n)</math>, where n is the size of the input.

{{Theorem|The functions representable in  ELL are exactly the elementary recursive
functions.
}}

One also often considers the ''affine'' variant of ELL, called ''elementary affine logic'' EAL, which is defined by adding unrestricted weakening:
 
<math>
\AxRule{\Gamma\vdash C}
\LabelRule{ w L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
</math>

It enjoys the same properties as ELL.

Elementary linear logic was introduced together with light linear logic
<ref>{{BibEntry|bibtype=journal|author=Girard, Jean-Yves|title=Light linear logic|journal=Information and Computation|volume=143|pages=175-204|year=1998}}</ref>.

= Light linear logic =
We present the intuitionistic version of ''light linear logic''  LLL, without additive connectives. The language of formulas is:

<math>
A ::= X \mid A\tens A \mid A\limp A  \mid \oc{A} \mid \pg{A} \mid \forall X A 
</math>
<br>
The sequent calculus rules are the same ones as for ILL, except for the rules
dealing with the exponential connectives:

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\oc\rulename{f} }
\UnaRule{\oc{\Gamma}\vdash\oc{A}}
\DisplayProof
\qquad
\AxRule{\Gamma, \Delta\vdash A}
\LabelRule{\pg }
\UnaRule{\oc{\Gamma}, \pg \Delta\vdash\pg{A}}
\DisplayProof
\qquad
\AxRule{\Gamma,\oc{A},\oc{A}\vdash C}
\LabelRule{\oc c L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash C}
\LabelRule{\oc w L}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
</math>

In the <math>(\oc\rulename{f})</math> rule, <math>\Gamma</math> must contain ''at most one'' formula.


The ''depth'' of a derivation <math>\pi</math> is the maximum number of
<math>(\oc\rulename{f})</math> and <math>(\pg)</math> rules in a branch of <math>\pi</math>.


{{Theorem|If <math>\pi</math> is an LLL proof of depth d, and R is the corresponding LLL proof-net, then R can be reduced to its normal form by cut elimination in 
<math> O((d+1)|\pi|^{2^{d+1}})</math> steps, where <math>|\pi|</math>is the size of <math>\pi</math>.}}

The class FP is the class of functions on binary lists which are computable in polynomial time on a Turing machine.

{{Theorem|The class of functions on binary lists representable in LLL is exactly FP.
}}

In the literature one also often considers the ''affine'' variant of LLL, called ''light affine logic'', LAL.

= Soft linear logic =
We consider the intuitionistic version of ''soft linear logic'', SLL.

The language of formulas is the same one as that of ILL:
<math>
A ::= X \mid A\tens A \mid A\limp A \mid A\with A \mid  A\plus A   \mid \oc{A} \mid \forall X A 
</math>
<br>
The sequent calculus rules are the same ones as for ILL, except for the rules
dealing with the exponential connectives:

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\oc\rulename{mf} }
\UnaRule{\oc{\Gamma}\vdash\oc{A}}
\DisplayProof
\qquad
\AxRule{\Gamma,A^{(n)}\vdash C}
\LabelRule{\rulename{mplex}}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
</math>

The rule mplex is the ''multiplexing'' rule. In its premise, <math>A^{(n)}</math> stands for
n occurrences of formula <math> A </math>. As particular instances of mplex for <math>n=0</math> and 1 respectively, we get weakening and dereliction:
 
<math>
\AxRule{\Gamma \vdash C}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
\qquad
\AxRule{\Gamma,A\vdash C}
\UnaRule{\Gamma,\oc{A}\vdash C}
\DisplayProof
</math>

The ''depth'' of a derivation <math>\pi</math> is the maximum number of
<math>(\oc\rulename{mf})</math> rules in a branch of <math>\pi</math>.


{{Theorem|If <math>\pi</math> is an SLL proof of depth d, and R is the corresponding SLL proof-net, then R can be reduced to its normal form by cut elimination in  
<math> O(|\pi|^d)</math> steps, where <math>|\pi|</math>is the size of <math>\pi</math>.}}


{{Theorem|The class of functions on binary lists representable in SLL is exactly FP.
}}

Soft linear logic was introduced  in
<ref>{{BibEntry|bibtype=journal|author=Lafont, Yves|title=Soft linear logic and polynomial time|journal=Theoretcal Computer Science|volume=318(1-2)|pages=163-180|year=2004}}</ref>.


= List of equivalences =


Each [[List of isomorphisms|isomorphism]] gives an equivalence of formulas.
The following equivalences are ''not'' isomorphisms.

== Multiplicatives ==

<math>
\begin{array}{rcccl}
A &amp;\linequiv&amp; A \tens (A\orth\parr A) &amp;\linequiv&amp; (A\tens A\orth)\parr A \\
&amp; &amp; A\parr A\orth &amp;\linequiv&amp; (A\parr A\orth)\tens(A\parr A\orth)
\end{array}
</math>

== Additives ==

<math>
\begin{array}{rclcrcl}
A \with A &amp;\linequiv&amp; A \\
A \plus A &amp;\linequiv&amp; A \\
\\
  A \with (A \plus B) &amp;\linequiv&amp; A &amp;\quad&amp; A \plus \top &amp;\linequiv&amp; \top \\
  A \plus (A \with B) &amp;\linequiv&amp; A &amp;\quad&amp; A \with \zero &amp;\linequiv&amp; \zero
\end{array}
</math>

== Quantifiers ==

<math>
\begin{array}{rcll}
  \forall X.A &amp;\linequiv&amp; A &amp;\quad (X\notin A) \\
  \exists X.A &amp;\linequiv&amp; A &amp;\quad (X\notin A)
\end{array}
</math>

== Exponentials ==

<math>
\begin{array}{rclcrcl}
  \oc A &amp;\linequiv&amp; \oc A\tens\oc A &amp;\quad&amp; 
  \wn A &amp;\linequiv&amp; \wn A\parr\wn A\\
  \oc A &amp;\linequiv&amp; \oc\oc A &amp;\quad&amp; \wn A &amp;\linequiv&amp; \wn\wn A\\
  \oc\wn A &amp;\linequiv&amp; \oc\wn\oc\wn A &amp;\quad&amp; \wn\oc A &amp;\linequiv&amp; \wn\oc\wn\oc A\\
\end{array}
</math>

Some of these equivalences are related with the [[lattice of exponential modalities]].

== Polarities ==

{|
| <math> \wn N \linequiv N </math>
| &amp;nbsp;&amp;nbsp;(N [[Negative formula|negative]])
|-
| <math> \oc P \linequiv P </math>
| &amp;nbsp;&amp;nbsp;(P [[Positive formula|positive]])
|-
| <math> \wn\oc R \linequiv R </math>
| &amp;nbsp;&amp;nbsp;(R [[Regular formula|regular]])
|-
| <math> \oc\wn L \linequiv L </math>
| &amp;nbsp;&amp;nbsp;(L [[Co-regular formula|co-regular]])
|}

== Second order encodings ==

<math>
\begin{array}{rclcrcl}
  A &amp;\linequiv &amp;\forall X . (A \tens X\orth) \parr X \\
  A &amp;\linequiv &amp;\exists X . (A \parr X\orth) \tens X \\
\\
  A \with B &amp;\linequiv&amp; \exists X . \oc{(A \parr X\orth)} \tens \oc{(B \parr X\orth)} \tens X &amp;\quad&amp; \top &amp;\linequiv&amp; \exists X . X \\
  A \plus B &amp;\linequiv&amp; \forall X . \wn{(A \tens X\orth)} \parr \wn{(B \tens X\orth)} \parr X &amp;\quad&amp; \zero &amp;\linequiv&amp; \forall X . X \\
\\
 \bot &amp;\linequiv&amp; \exists X . X\tens X\orth \\
 \one &amp;\linequiv&amp; \forall X . X\orth\parr X \\
\\
  \forall \xi . A &amp;\linequiv&amp; \exists X . (\forall \xi . (A \parr X\orth)) \tens X \\
  \exists \xi . A &amp;\linequiv&amp; \forall X . (\exists \xi . (A \tens X\orth)) \parr X
\end{array}
</math>

== Miscellaneous ==

<math>
\begin{array}{rcl}
  \one &amp;\linequiv&amp; \oc{(A\orth\parr A)} \\
  \bot &amp;\linequiv&amp; \wn{(A\orth\tens A)} \\
\\
  \oc{\wn{(\oc{A}\with\oc{B})}} &amp;\linequiv&amp; \oc{(\wn{\oc{A}}\with\wn{\oc{B}})} \\
  \wn{\oc{(\wn{A}\plus\wn{B})}} &amp;\linequiv&amp; \wn{(\oc{\wn{A}}\plus\oc{\wn{B}})}
\end{array}
</math>


= List of isomorphisms =


== Linear negation ==

<math>
\begin{array}{rclcrcl}
  A\biorth &amp;\cong&amp; A\\
  (A\tens B)\orth &amp;\cong&amp; A\orth\parr B\orth &amp;\quad&amp; \one\orth  &amp;\cong&amp; \bot\\
  (A\parr B)\orth &amp;\cong&amp; A\orth\tens B\orth &amp;\quad&amp; \bot\orth  &amp;\cong&amp; \one\\
  (A\with B)\orth &amp;\cong&amp; A\orth\plus B\orth &amp;\quad&amp; \top\orth  &amp;\cong&amp; \zero\\
  (A\plus B)\orth &amp;\cong&amp; A\orth\with B\orth &amp;\quad&amp; \zero\orth &amp;\cong&amp; \top\\
  (\oc A)\orth &amp;\cong&amp; \wn A\orth\\
  (\wn A)\orth &amp;\cong&amp; \oc A\orth\\
\end{array}
</math>

== Neutrals ==

<math>
\begin{array}{rcl}
  A\tens\one  &amp;\cong&amp; \one\tens A\cong A\\
  A\parr\bot  &amp;\cong&amp; \bot\parr A\cong A\\
  A\with\top  &amp;\cong&amp; \top\with A\cong A\\
  A\plus\zero &amp;\cong&amp;\zero\plus A\cong A\\
\end{array}
</math>

== Commutativity ==

<math>
\begin{array}{rcl}
  A\tens B &amp;\cong&amp; B\tens A\\
  A\parr B &amp;\cong&amp; B\parr A\\
  A\with B &amp;\cong&amp; B\with A\\
  A\plus B &amp;\cong&amp; B\plus A\\
\end{array}
</math>

== Associativity ==

<math>
\begin{array}{rcl}
  (A\tens B)\tens C &amp;\cong&amp; A\tens(B\tens C)\\
  (A\parr B)\parr C &amp;\cong&amp; A\parr(B\parr C)\\
  (A\with B)\with C &amp;\cong&amp; A\with(B\with C)\\
  (A\plus B)\plus C &amp;\cong&amp; A\plus(B\plus C)\\
\end{array}
</math>

== Multiplicative-additive distributivity ==

<math>
\begin{array}{rclcrcl}
  A\tens(B\plus C) &amp;\cong&amp; (A\tens B)\plus(A\tens C) &amp;\quad&amp;
  A\tens\zero &amp;\cong&amp; \zero\\
  A\parr(B\with C) &amp;\cong&amp; (A\parr B)\with(A\parr C) &amp;\quad&amp;
  A\parr\top &amp;\cong&amp; \top\\
\end{array}
</math>

== Linear implication ==

<math>
\begin{array}{rclcrcl}
  A\limp B &amp;\cong&amp; A\orth\parr B\\
  A\limp B &amp;\cong&amp; B\orth\limp A\orth\\
  A\tens B \limp C &amp;\cong&amp; A\limp B \limp C\\
\end{array}
</math>

== The exponential isomorphisms ==

<math>
\begin{array}{rclcrcl}
  \oc(A\with B) &amp;\cong&amp; \oc A\tens\oc B &amp;\quad&amp; \oc\top &amp;\cong&amp; \one\\
  \wn(A\plus B) &amp;\cong&amp; \wn A\parr\wn B &amp;\quad&amp; \wn\zero &amp;\cong&amp; \bot\\
\end{array}
</math>

== Quantifiers ==


<math>
\begin{array}{rclcrcl}
  \forall \xi_1. \forall\xi_2. A &amp;\cong&amp; \forall\xi_2. \forall\xi_1. A\\
  \exists \xi_1. \exists\xi_2.A &amp;\cong&amp; \exists\xi_2.\exists\xi_1.A\\
\\
  \forall \xi . (A \parr B) &amp;\cong&amp; A \parr \forall \xi.B \quad (\xi\notin A) \\
  \exists \xi . (A \tens B) &amp;\cong&amp; A \tens \exists \xi.B \quad (\xi\notin A) \\
\\
  \forall \xi . (A \with B) &amp;\cong&amp; (\forall \xi . A) \with (\forall \xi . B) &amp; &amp; \forall \xi . \top &amp;\cong&amp; \top \\
  \exists \xi . (A \plus B) &amp;\cong&amp; (\exists \xi . A) \plus (\exists \xi . B) &amp; &amp; \exists \xi . \zero &amp;\cong&amp; \zero
\end{array}
</math>


= Mix =


The usual notion of <math>\rulename{Mix}</math> is the binary version of the rule but a nullary version also exists.

== Binary <math>\rulename{Mix}</math> rule ==

<math>
\AxRule{\vdash\Gamma}
\AxRule{\vdash\Delta}
\LabelRule{Mix_2}
\BinRule{\vdash\Gamma,\Delta}
\DisplayProof
</math>

The <math>\rulename{Mix_2}</math> rule is equivalent to <math>\bot\vdash\one</math>:

<math>
\LabelRule{\one}
\NulRule{\vdash\one}
\LabelRule{\one}
\NulRule{\vdash\one}
\LabelRule{Mix_2}
\BinRule{\vdash\one,\one}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma}
\LabelRule{\bot}
\UnaRule{\vdash\Gamma,\bot}
\AxRule{\vdash\one,\one}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\Gamma,\one}
\AxRule{\vdash\Delta}
\LabelRule{\bot}
\UnaRule{\vdash\Delta,\bot}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\Gamma,\Delta}
\DisplayProof
</math>

They are also equivalent to the principle <math>A\tens B \vdash A\parr B</math>:

<math>
\LabelRule{\one}
\NulRule{\vdash\one}
\LabelRule{\one}
\NulRule{\vdash\one}
\LabelRule{\tens}
\BinRule{\vdash\one\tens\one}
\AxRule{\vdash\bot\parr\bot,\one\parr\one}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\one\parr\one}
\LabelRule{\rulename{ax}}
\NulRule{\vdash\bot,\one}
\LabelRule{\rulename{ax}}
\NulRule{\vdash\bot,\one}
\LabelRule{\tens}
\BinRule{\vdash\bot\tens\bot,\one,\one}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\one,\one}
\DisplayProof
\qquad
\LabelRule{\rulename{ax}}
\NulRule{\vdash A\orth,A}
\LabelRule{\rulename{ax}}
\NulRule{\vdash B\orth,B}
\LabelRule{Mix_2}
\BinRule{\vdash A\orth,A,B\orth,B}
\LabelRule{\parr}
\UnaRule{\vdash A\orth,B\orth,A\parr B}
\LabelRule{\parr}
\UnaRule{\vdash A\orth\parr B\orth,A\parr B}
\DisplayProof
</math>

== Nullary <math>\rulename{Mix}</math> rule ==

<math>
\LabelRule{Mix_0}
\NulRule{\vdash}
\DisplayProof
</math>

The <math>\rulename{Mix_0}</math> rule is equivalent to <math>\one\vdash\bot</math>:

<math>
\LabelRule{Mix_0}
\NulRule{\vdash}
\LabelRule{\bot}
\UnaRule{\vdash\bot}
\LabelRule{\bot}
\UnaRule{\vdash\bot,\bot}
\DisplayProof
\qquad
\LabelRule{\one}
\NulRule{\vdash\one}
\AxRule{\vdash\bot,\bot}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\bot}
\LabelRule{\one}
\NulRule{\vdash\one}
\LabelRule{\rulename{cut}}
\BinRule{\vdash}
\DisplayProof
</math>

The nullary <math>\rulename{Mix}</math> acts as a unit for the binary one:

<math>
\AxRule{\vdash\Gamma}
\LabelRule{Mix_0}
\NulRule{\vdash}
\LabelRule{Mix_2}
\BinRule{\vdash\Gamma}
\DisplayProof
</math>

If <math>\pi</math> is a proof which uses no <math>\bot</math> rule and no weakening rule, then (up to the simplification of the pattern <math>\rulename{Mix_0}/\rulename{Mix_2}</math> above into nothing) <math>\pi</math> is either reduced to a <math>\rulename{Mix_0}</math> rule or does not contain any <math>\rulename{Mix_0}</math> rule.


= Negative formula =


A ''negative formula'' is a formula <math>N</math> such that <math>\wn N\limp N</math> (thus a [[Wikipedia:F-algebra|algebra]] for the [[Wikipedia:Monad (category theory)|monad]] <math>\wn</math>). As a consequence <math>N</math> and <math>\wn N</math> are [[Sequent calculus#Equivalences|equivalent]].

A formula <math>N</math> is negative if and only if <math>N\orth</math> is [[Positive formula|positive]].

== Negative connectives ==

A connective <math>c</math> of arity <math>n</math> is ''negative'' if for any negative formulas <math>N_1</math>,...,<math>N_n</math>, <math>c(N_1,\dots,N_n)</math> is negative.

{{Proposition|title=Negative connectives|
<math>\parr</math>, <math>\bot</math>, <math>\with</math>, <math>\top</math>, <math>\wn</math> and <math>\forall</math> are negative connectives.
}}

{{Proof|
This is equivalent to the fact that <math>\tens</math>, <math>\one</math>, <math>\plus</math>, <math>\zero</math>, <math>\oc</math> and <math>\exists</math> are [[Positive formula#Positive connectives|positive connectives]].
}}

More generally, <math>\wn A</math> is negative for any formula <math>A</math>.


The notion of negative connective is related with but different from the notion of [[synchronous connective]].

== Generalized structural rules ==

Negative formulas admit generalized right structural rules corresponding to a structure of [[Wikipedia:Monoid (category theory)|<math>\parr</math>-monoid]]: <math>N\parr N\limp N</math> and <math>\bot\limp N</math>. The following rule is derivable:

<math>
\AxRule{\Gamma\vdash N,N,\Delta}
\LabelRule{- c R}
\UnaRule{\Gamma\vdash N,\Delta}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash\Delta}
\LabelRule{- w R}
\UnaRule{\Gamma\vdash N,\Delta}
\DisplayProof
</math>

{{Proof|
This is equivalent to the [[Positive formula#Generalized structural rules|generalized left structural rules]] for positive formulas.
}}

Negative formulas are also acceptable in the context of the promotion rule. The following rule is derivable:

<math>
\AxRule{\vdash A,N_1,\dots,N_n}
\LabelRule{- \oc R}
\UnaRule{\vdash \oc{A},N_1,\dots,N_n}
\DisplayProof
</math>

{{Proof|
This is equivalent to the possibility of having positive formulas in the [[Positive formula#Generalized structural rules|left-hand side context of the promotion rule]].
}}


= Non provable formulas =


{{stub}}

<math>A \with (B\plus C) \not\limp (A\with B)\plus (A\with C)</math>

<math>(A\plus B)\with (A\plus C) \not\limp A\plus (B\with C)</math>

<math>(A\tens B)\parr C \not\limp A\tens (B\parr C)</math>

<math>A \not\limp \oc{A}</math>

<math>\oc{\wn{\oc{A}}} \not\limp A</math>

<math>\oc{\wn{\oc{A}}} \not\limp \oc{A}</math>

<math>\oc{\wn{A}} \not\limp \wn{\oc{A}}</math>

<math>\wn{\oc{A}} \not\limp \oc{\wn{A}}</math>


= Orthogonality relation =


'''Orthogonality relations''' are used pervasively throughout linear logic models, being often used to define somehow the duality operator <math>(-)\orth</math>.

{{Definition|title=Orthogonality relation|Let <math>A</math> and <math>B</math> be two sets. An '''orthogonality relation''' on <math>A</math> and <math>B</math> is a binary relation <math>\mathcal{R}\subseteq A\times B</math>. We say that <math>a\in A</math> and <math>b\in B</math> are '''orthogonal''', and we note <math>a\perp b</math>, whenever <math>(a, b)\in\mathcal{R}</math>.}}

Let us now assume an orthogonality relation over <math>A</math> and <math>B</math>.

{{Definition|title=Orthogonal sets|Let <math>\alpha\subseteq A</math>. We define its orthogonal set <math>\alpha\orth</math> as <math>\alpha\orth:=\{b\in B \mid \forall a\in \alpha, a\perp b\}</math>.

Symmetrically, for any <math>\beta\subseteq B</math>, we define <math>\beta\orth:=\{a\in A \mid \forall b\in \beta, a\perp b\}</math>.
}}

Orthogonal sets define Galois connections and share many common properties.

{{Proposition|For any sets <math>\alpha, \alpha'\subseteq A</math>:

* <math>\alpha\subseteq \alpha\biorth</math>
* If <math>\alpha\subseteq\alpha'</math>, then <math>{\alpha'}\orth\subseteq\alpha\orth</math>
* <math>\alpha\triorth = \alpha\orth</math>
}}


= Phase semantics =


==Introduction==

The semantics given by phase spaces is a kind of &quot;formula and provability semantics&quot;, and is thus quite different in spirit from the more usual denotational semantics of linear logic. (Those are rather some &quot;formulas and ''proofs'' semantics&quot;.)

  --- probably a whole lot more of blabla to put here... ---

==Preliminaries: relation and closure operators==

Part of the structure obtained from phase semantics works in a very general framework and relies solely on the notion of relation between two sets.


===Relations and operators on subsets===

The starting point of phase semantics is the notion of ''duality''. The structure needed to talk about duality is very simple: one just needs a relation <math>R</math> between two sets <math>X</math> and <math>Y</math>. Using standard mathematical practice, we can write either <math>(a,b) \in R</math> or <math>a\mathrel{R} b</math> to say that <math>a\in X</math> and <math>b\in Y</math> are related.

{{Definition|
If <math>R\subseteq X\times Y</math> is a relation, we write <math>R^\sim\subseteq Y\times X</math> for the converse relation: <math>(b,a)\in R^\sim</math> iff <math>(a,b)\in R</math>.
}}

Such a relation yields three interesting operators sending subsets of <math>X</math> to subsets of <math>Y</math>:

{{Definition|
Let <math>R\subseteq X\times Y</math> be a relation, define the operators <math>\langle R\rangle</math>, <math>[R]</math> and <math>\_^R</math> taking subsets of <math>X</math> to subsets of <math>Y</math> as follows:
# <math>b\in\langle R\rangle(x)</math> iff <math>\exists a\in x,\ (a,b)\in R</math>
# <math>b\in[R](x)</math> iff <math>\forall a\in X,\ (a,b)\in R \implies a\in x</math>
# <math>b\in x^R</math> iff <math>\forall a\in x, (a,b)\in R</math>
}}

The operator <math>\langle R\rangle</math> is usually called the ''direct image'' of the relation, <math>[R]</math> is sometimes called the ''universal image'' of the relation.

It is trivial to check that <math>\langle R\rangle</math> and <math>[R]</math> are covariant (increasing for the <math>\subseteq</math> relation) while <math>\_^R</math> is contravariant (decreasing for the <math>\subseteq</math> relation). More interesting:

{{Lemma|title=Galois Connections|
# <math>\langle R\rangle</math> is right-adjoint to <math>[R^\sim]</math>: for any <math>x\subseteq X</math> and <math>y\subseteq Y</math>, we have <math>[R^\sim]y \subseteq x</math> iff <math>y\subseteq \langle R\rangle(x)</math>
# we have <math>y\subseteq x^R</math> iff <math>x\subseteq y^{R^\sim}</math>
}}

This implies directly that <math>\langle R\rangle</math> commutes with arbitrary unions and <math>[R]</math> commutes with arbitrary intersections. (And in fact, any operator commuting with arbitrary unions (resp. intersections) is of the form <math>\langle R\rangle</math> (resp. <math>[R]</math>).

{{Remark|the operator <math>\_^R</math> sends unions to intersections because <math>\_^R : \mathcal{P}(X) \to \mathcal{P}(Y)^\mathrm{op}</math> is right adjoint to <math>\_^{R^\sim} : \mathcal{P}(Y)^{\mathrm{op}} \to \mathcal{P}(X)</math>...}}

===Closure operators===

{{Definition|
A closure operator on <math>\mathcal{P}(X)</math> is a monotonic operator <math>P</math> on the subsets of <math>X</math> which satisfies:
# for all <math>x\subseteq X</math>, we have <math>x\subseteq P(x)</math>
# for all <math>x\subseteq X</math>, we have <math>P(P(x))\subseteq P(x)</math>
}}

Closure operators are quite common in mathematics and computer science. They correspond exactly to the notion of ''monad'' on a preorder...

It follows directly from the definition that for any closure operator <math>P</math>, the image <math>P(x)</math> is a fixed point of <math>P</math>. Moreover:

{{Lemma|
<math>P(x)</math> is the smallest fixed point of <math>P</math> containing <math>x</math>.
}}

One other important property is the following:

{{Lemma|
Write <math>\mathcal{F}(P) = \{x\ |\ P(x)\subseteq x\}</math> for the collection of fixed points of a closure operator <math>P</math>. We have that <math>\left(\mathcal{F}(P),\bigcap\right)</math> is a complete inf-lattice.
}}

{{Remark|
A closure operator is in fact determined by its set of fixed points: we have <math>P(x) = \bigcup \{ y\ |\ y\in\mathcal{F}(P),\,y\subseteq x\}</math>}}

Since any complete inf-lattice is automatically a complete sup-lattice, <math>\mathcal{F}(P)</math> is also a complete sup-lattice. However, the sup operation isn't given by plain union:

{{Lemma|
If <math>P</math> is a closure operator on <math>\mathcal{P}(X)</math>, and if <math>(x_i)_{i\in I}</math> is a (possibly infinite) family of subsets of <math>X</math>, we write <math>\bigvee_{i\in I} x_i = P\left(\bigcup_{i\in I} x_i\right)</math>.

We have <math>\left(\mathcal{F}(P),\bigcap,\bigvee\right)</math> is a complete lattice.
}}

{{Proof|
easy.
}}


A rather direct consequence of the Galois connections of the previous section is:

{{Lemma|
The operator and <math>\langle R\rangle \circ [R^\sim]</math> and the operator <math>x\mapsto {x^R}^{R^\sim}</math> are closures.
}}

A last trivial lemma:

{{Lemma|
We have <math>x^R = {{x^R}^{R^\sim}}^{R}</math>.

As a consequence, a subset <math>x\subseteq X</math> is in <math>\mathcal{F}({\_^R}^{R^\sim})</math> iff it is of the form <math>y^{R^\sim}</math>.
}}

{{Remark|everything gets a little simpler when <math>R</math> is a symmetric relation on <math>X</math>.}}

==Phase Semantics==

===Phase spaces===

{{Definition|title=monoid|
A monoid is simply a set <math>X</math> equipped with a binary operation <math>\_\cdot\_</math> s.t.:
# the operation is associative
# there is a neutral element <math>1\in X</math>
The monoid is ''commutative'' when the binary operation is commutative.
}}

{{Definition|title=Phase space|
A phase space is given by:
# a commutative monoid <math>(X,1,\cdot)</math>,
# together with a subset <math>\Bot\subseteq X</math>.
The elements of <math>X</math> are called ''phases''.

We write <math>\bot</math> for the relation <math>\{(a,b)\ |\ a\cdot b \in \Bot\}</math>. This relation is symmetric.

A ''fact'' in a phase space is simply a fixed point for the closure operator <math>x\mapsto x\biorth</math>.
}}


Thanks to the preliminary work, we have:

{{Corollary|
The set of facts of a phase space is a complete lattice where:
# <math>\bigwedge_{i\in I} x_i</math> is simply <math>\bigcap_{i\in I} x_i</math>,
# <math>\bigvee_{i\in I} x_i</math> is <math>\left(\bigcup_{i\in I} x_i\right)\biorth</math>.
}}

===Additive connectives===

The previous corollary makes the following definition correct:

{{Definition|title=additive connectives|
If <math>(X,1,\cdot,\Bot)</math> is a phase space, we define the following facts and operations on facts:
# <math>\top = X = \emptyset\orth</math>
# <math>\zero = \emptyset\biorth = X\orth</math>
# <math>x\with y = x\cap y</math>
# <math>x\plus y = (x\cup y)\biorth</math>
}}

Once again, the next lemma follows from previous observations:

{{Lemma|title=additive de Morgan laws|
We have
# <math>\zero\orth = \top</math>
# <math>\top\orth = \zero</math>
# <math>(x\with y)\orth = x\orth \plus y\orth</math>
# <math>(x\plus y)\orth = x\orth \with y\orth</math>
}}

===Multiplicative connectives===

In order to define the multiplicative connectives, we actually need to use the monoid structure of our phase space. One interpretation that is reminiscent in phase semantics is that our spaces are collections of ''tests'' / programs / proofs / ''strategies'' that can interact with each other. The result of the interaction between <math>a</math> and <math>b</math> is simply <math>a\cdot b</math>.

The set <math>\Bot</math> can be thought of as the set of &quot;good&quot; things, and we thus have <math>a\in x\orth</math> iff &quot;<math>a</math> interacts correctly with all the elements of <math>x</math>&quot;.


{{Definition|
If <math>x</math> and <math>y</math> are two subsets of a phase space, we write <math>x\cdot y</math> for the set <math>\{a\cdot b\ |\ a\in x, b\in y\}</math>.
}}
Thus <math>x\cdot y</math> contains all the possible interactions between one element of <math>x</math> and one element of <math>y</math>.


The tensor connective of linear logic is now defined as:

{{Definition| title=multiplicative connectives|
If <math>x</math> and <math>y</math> are facts in a phase space, we define
* <math>\one = \{1\}\biorth</math>;
* <math>\bot = \one\orth</math>;
* the tensor <math>x\tens y</math> to be the fact <math>(x\cdot y)\biorth</math>;
* the par connective is the de Morgan dual of the tensor: <math>x\parr y = (x\orth \tens y\orth)\orth</math>;
* the linear arrow is just <math>x\limp y = x\orth\parr y = (x\tens y\orth)\orth</math>.
}}

Note that by unfolding the definition of <math>\limp</math>, we have the following, &quot;intuitive&quot; definition of <math>x\limp y</math>:

{{Lemma|
If <math>x</math> and <math>y</math> are facts, we have <math>a\in x\limp y</math> iff <math>\forall b\in x,\,a\cdot b\in y</math>
}}

{{Proof|
easy exercise. }}

Readers familiar with realisability will appreciate...

{{Remark|
Some people say that this idea of orthogonality was implicitly present in Tait's proof of strong normalisation. More recently, Jean-Louis Krivine and Alexandre Miquel have used the idea explicitly to do realisability...}}


===Properties===

All the expected properties hold:

{{Lemma|
* The operations <math>\tens</math>, <math>\parr</math>, <math>\plus</math> and <math>\with</math> are commutative and associative,
* They have respectively <math>\one</math>, <math>\bot</math>, <math>\zero</math> and <math>\top</math> as neutral element,
* <math>\zero</math> is absorbing for <math>\tens</math>,
* <math>\top</math> is absorbing for <math>\parr</math>,
* <math>\tens</math> distributes over <math>\plus</math>,
* <math>\parr</math> distributes over <math>\with</math>.
}}

===Exponentials===

{{Definition|title=Exponentials|
Write <math>I</math> for the set of idempotents of a phase space: <math>I=\{a\ |\ a\cdot a=a\}</math>. We put:
# <math>\oc x = (x\cap I\cap \one)\biorth</math>,
# <math>\wn x = (x\orth\cap I\cap\one)\orth</math>.
}}

This definition captures precisely the intuition behind the exponentials:
* we need to have contraction, hence we restrict to indempotents in <math>x</math>,
* and weakening, hence we restrict to <math>\one</math>.
Since <math>I</math> isn't necessarily a fact, we then take the biorthogonal to get a fact...

== Soundness ==

{{Definition|Let <math>(X, 1, \cdot)</math> be a commutative monoid.

Given a formula <math>A</math> of linear logic and an assignation <math>\rho</math> that associate a fact to any variable, we can inductively define the interpretation <math>\sem{A}_\rho</math> of <math>A</math> in <math>X</math> as one would expect. Interpretation is lifted to sequents as <math>\sem{A_1, \dots, A_n}_\rho = \sem{A_1}_\rho \parr \cdots \parr \sem{A_n}_\rho</math>.}}

{{Theorem|Let <math>\Gamma</math> be a provable sequent in linear logic. Then <math>1_X \in \sem{\Gamma}</math>.}}

{{Proof|By induction on <math>\vdash\Gamma</math>.}}

== Completeness ==

Phase semantics is complete w.r.t. linear logic. In order to prove this, we need to build a particular commutative monoid.

{{Definition|We define the '''syntactic monoid''' as follows:

* Its elements are sequents <math>\Gamma</math> quotiented by the equivalence relation <math>\cong</math> generated by the rules:
*# <math>\Gamma \cong \Delta</math> if <math>\Gamma</math> is a permutation of <math>\Delta</math>
*# <math>\wn{A}, \wn{A}, \Gamma \cong \wn{A}, \Gamma</math>

* Product is concatenation: <math>\Gamma \cdot \Delta := \Gamma, \Delta</math>

* Neutral element is the empty sequent: <math>1 := \emptyset</math>.}}

The equivalence relation intuitively means that we do not care about the multiplicity of <math>\wn</math>-formulae.

{{Lemma|The syntactic monoid is indeed a commutative monoid.}}

{{Definition|The '''syntactic assignation''' is the assignation that sends any variable <math>\alpha</math> to the fact <math>\{\alpha\}\orth</math>.}}

We instantiate the pole as <math>\Bot := \{\Gamma \mid \vdash\Gamma\}</math>.

{{Theorem|If <math>\Gamma\in\sem{\Gamma}\orth</math>, then <math>\vdash\Gamma</math>.}}

{{Proof|By induction on <math>\Gamma</math>.}}

== Cut elimination ==

Actually, the completeness result is stronger, as the proof does not use the cut-rule in the reconstruction of <math>\vdash\Gamma</math>. By refining the pole as the set of ''cut-free'' provable formulae, we get:

{{Theorem|If <math>\Gamma\in\sem{\Gamma}\orth</math>, then <math>\Gamma</math> is cut-free provable.}}

From soundness, one can retrieve the cut-elimination theorem.

{{Corollary|Linear logic enjoys the cut-elimination property.}}

==The Rest==


= Polarized linear logic =


'''Polarized linear logic''' (LLP) is a logic close to plain linear logic in which structural rules, usually restricted to <math>\wn</math>-formulas, have been [[Positive_formula#Generalized_structural_rules|extended]] to the whole class of so called ''negative'' formulae.

== Polarization ==

LLP relies on the notion of ''polarization'', that is, it discriminates between two types of formulae, ''negative'' (noted <math>M, N...</math>) vs. ''[[Positive formula|positive]]'' (<math>P, Q...</math>). They are mutually defined as follows:

<math>M, N ::= X \mid M \parr N \mid \bot \mid M \with N \mid \top \mid \wn{P}</math>

<math>P, Q ::= X\orth \mid P \otimes Q \mid 1 \mid P \oplus Q \mid 0 \mid \oc{N}</math>

The dual operation <math>(-)\orth</math> extended to propositions exchanges the roles of connectors and reverses the polarity of formulae.

== Deduction rules ==

They are several design choices for the structure of sequents. In particular, LLP proofs are ''focalized'', i.e. they contain at most one positive formula. We choose to represent this explicitly using sequents of the form <math>\vdash\Gamma\mid\Delta</math>, where <math>\Gamma</math> is a multiset of negative formulae, and <math>\Delta</math> is a ''stoup'' that contains at most one positive formula (though it may be empty).

<math>
\LabelRule{\rulename{ax}}
\NulRule{\vdash P\orth \mid P}
\DisplayProof
\qquad
\AxRule{\vdash \Gamma_1, N \mid \Delta}
\AxRule{\vdash \Gamma_2 \mid N\orth}
\LabelRule{\rulename{cut}}
\BinRule{\vdash\Gamma_1, \Gamma_2 \mid \Delta}
\DisplayProof
</math>

<br/>

<math>
\AxRule{\vdash\Gamma, N\mid\cdot}
\LabelRule{p}
\UnaRule{\vdash\Gamma\mid \oc{N}}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma\mid P}
\LabelRule{d}
\UnaRule{\vdash\Gamma,\wn{P}\mid \cdot}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma,N,N\mid \Delta}
\LabelRule{c}
\UnaRule{\vdash\Gamma, N\mid\Delta}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma\mid \Delta}
\LabelRule{w}
\UnaRule{\vdash\Gamma,N\mid\Delta}
\DisplayProof
</math>

<br/>

<math>
\AxRule{\vdash\Gamma_1\mid P}
\AxRule{\vdash\Gamma_2\mid Q}
\LabelRule{\tens}
\BinRule{\vdash\Gamma_1,\Gamma_2\mid P\otimes Q}
\DisplayProof
\qquad
\LabelRule{\one}
\NulRule{\vdash\cdot\mid\one}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma, M, N\mid \Delta}
\LabelRule{\parr}
\UnaRule{\vdash\Gamma, M\parr N\mid \Delta}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma\mid \Delta}
\LabelRule{\bot}
\UnaRule{\vdash\Gamma, \bot\mid\Delta}
\DisplayProof
</math>

<br/>

<math>
\AxRule{\vdash\Gamma\mid P}
\LabelRule{\plus_1}
\UnaRule{\vdash\Gamma\mid P\plus Q}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma\mid Q}
\LabelRule{\plus_2}
\UnaRule{\vdash\Gamma\mid P\plus Q}
\DisplayProof
\qquad
\AxRule{\vdash\Gamma,M\mid \Delta}
\AxRule{\vdash\Gamma,N\mid \Delta}
\LabelRule{\with}
\BinRule{\vdash\Gamma,M\with N\mid \Delta}
\DisplayProof
\qquad
\LabelRule{\top}
\NulRule{\vdash\Gamma,\top\mid \Delta}
\DisplayProof
</math>


= Positive formula =


A ''positive formula'' is a formula <math>P</math> such that <math>P\limp\oc P</math> (thus a [[Wikipedia:F-coalgebra|coalgebra]] for the [[Wikipedia:Comonad|comonad]] <math>\oc</math>). As a consequence <math>P</math> and <math>\oc P</math> are [[Sequent calculus#Equivalences|equivalent]].

A formula <math>P</math> is positive if and only if <math>P\orth</math> is [[Negative formula|negative]].

== Positive connectives ==

A connective <math>c</math> of arity <math>n</math> is ''positive'' if for any positive formulas <math>P_1</math>,...,<math>P_n</math>, <math>c(P_1,\dots,P_n)</math> is positive.

{{Proposition|title=Positive connectives|
<math>\tens</math>, <math>\one</math>, <math>\plus</math>, <math>\zero</math>, <math>\oc</math> and <math>\exists</math> are positive connectives.
}}

{{Proof|
<math>
\AxRule{P_2\vdash\oc{P_2}}
\AxRule{P_1\vdash\oc{P_1}}
\LabelRule{\rulename{ax}}
\NulRule{P_1\vdash P_1}
\LabelRule{\rulename{ax}}
\NulRule{P_2\vdash P_2}
\LabelRule{\tens R}
\BinRule{P_1,P_2\vdash P_1\tens P_2}
\LabelRule{\oc d L}
\UnaRule{\oc{P_1},P_2\vdash P_1\tens P_2}
\LabelRule{\oc d L}
\UnaRule{\oc{P_1},\oc{P_2}\vdash P_1\tens P_2}
\LabelRule{\oc R}
\UnaRule{\oc{P_1},\oc{P_2}\vdash\oc{(P_1\tens P_2)}}
\LabelRule{\rulename{cut}}
\BinRule{P_1,\oc{P_2}\vdash\oc{(P_1\tens P_2)}}
\LabelRule{\rulename{cut}}
\BinRule{P_1,P_2\vdash\oc{(P_1\tens P_2)}}
\LabelRule{\tens L}
\UnaRule{P_1\tens P_2\vdash\oc{(P_1\tens P_2)}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{\one R}
\NulRule{\vdash\one}
\LabelRule{\oc R}
\UnaRule{\vdash\oc{\one}}
\LabelRule{\one L}
\UnaRule{\one\vdash\oc{\one}}
\DisplayProof
</math>

<br />

<math>
\AxRule{P_1\vdash\oc{P_1}}
\LabelRule{\rulename{ax}}
\NulRule{P_1\vdash P_1}
\LabelRule{\plus_1 R}
\UnaRule{P_1\vdash P_1\plus P_2}
\LabelRule{\oc d L}
\UnaRule{\oc{P_1}\vdash P_1\plus P_2}
\LabelRule{\oc R}
\UnaRule{\oc{P_1}\vdash\oc{(P_1\plus P_2)}}
\LabelRule{\rulename{cut}}
\BinRule{P_1\vdash\oc{(P_1\plus P_2)}}
\AxRule{P_2\vdash\oc{P_2}}
\LabelRule{\rulename{ax}}
\NulRule{P_2\vdash P_2}
\LabelRule{\plus_2 R}
\UnaRule{P_2\vdash P_1\plus P_2}
\LabelRule{\oc d L}
\UnaRule{\oc{P_2}\vdash P_1\plus P_2}
\LabelRule{\oc R}
\UnaRule{\oc{P_2}\vdash\oc{(P_1\plus P_2)}}
\LabelRule{\rulename{cut}}
\BinRule{P_2\vdash\oc{(P_1\plus P_2)}}
\LabelRule{\plus L}
\BinRule{P_1\plus P_2\vdash\oc{(P_1\plus P_2)}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{\zero L}
\NulRule{\zero\vdash\oc{\zero}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{\rulename{ax}}
\NulRule{\oc{P}\vdash\oc{P}}
\LabelRule{\oc R}
\UnaRule{\oc{P}\vdash\oc{\oc{P}}}
\DisplayProof
</math>

<br />

<math>
\AxRule{P\vdash\oc{P}}
\LabelRule{\rulename{ax}}
\NulRule{P\vdash P}
\LabelRule{\exists R}
\UnaRule{P\vdash \exists\xi P}
\LabelRule{\oc d L}
\UnaRule{\oc{P}\vdash \exists\xi P}
\LabelRule{\oc R}
\UnaRule{\oc{P}\vdash\oc{\exists\xi P}}
\LabelRule{\rulename{cut}}
\BinRule{P\vdash\oc{\exists\xi P}}
\LabelRule{\exists L}
\UnaRule{\exists\xi P\vdash\oc{\exists\xi P}}
\DisplayProof
</math>
}}

More generally, <math>\oc A</math> is positive for any formula <math>A</math>.


The notion of positive connective is related with but different from the notion of [[asynchronous connective]].

== Generalized structural rules ==

Positive formulas admit generalized left structural rules corresponding to a structure of [[Wikipedia:Comonoid|<math>\tens</math>-comonoid]]: <math>P\limp P\tens P</math> and <math>P\limp\one</math>. The following rule is derivable:

<math>
\AxRule{\Gamma,P,P\vdash\Delta}
\LabelRule{+ c L}
\UnaRule{\Gamma,P\vdash\Delta}
\DisplayProof
\qquad
\AxRule{\Gamma\vdash\Delta}
\LabelRule{+ w L}
\UnaRule{\Gamma,P\vdash\Delta}
\DisplayProof
</math>

{{Proof|
<math>
\AxRule{P\vdash\oc{P}}
\AxRule{\Gamma,P,P\vdash\Delta}
\LabelRule{\oc L}
\UnaRule{\Gamma,P,\oc P\vdash\Delta}
\LabelRule{\oc L}
\UnaRule{\Gamma,\oc P,\oc P\vdash\Delta}
\LabelRule{\oc c L}
\UnaRule{\Gamma,\oc P\vdash\Delta}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,P\vdash\Delta}
\DisplayProof
</math>

<br />

<math>
\AxRule{P\vdash\oc{P}}
\AxRule{\Gamma\vdash\Delta}
\LabelRule{\oc w L}
\UnaRule{\Gamma,\oc P\vdash\Delta}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,P\vdash\Delta}
\DisplayProof
</math>
}}

Positive formulas are also acceptable in the left-hand side context of the promotion rule. The following rule is derivable:

<math>
\AxRule{\oc\Gamma,P_1,\dots,P_n\vdash A,\wn\Delta}
\LabelRule{+ \oc R}
\UnaRule{\oc\Gamma,P_1,\dots,P_n\vdash \oc{A},\wn\Delta}
\DisplayProof
</math>

{{Proof|
<math>
\AxRule{P_1\vdash\oc{P_1}}
\AxRule{P_n\vdash\oc{P_n}}
\AxRule{\oc\Gamma,P_1,\dots,P_n\vdash A,\wn\Delta}
\LabelRule{\oc L}
\UnaRule{\oc\Gamma,P_1,\dots,P_{n-1},\oc{P_n}\vdash A,\wn\Delta}
\VdotsRule{}{\oc\Gamma,P_1,\oc{P_2},\dots,\oc{P_n}\vdash A,\wn\Delta}
\LabelRule{\oc L}
\UnaRule{\oc\Gamma,\oc{P_1},\dots,\oc{P_n}\vdash A,\wn\Delta}
\LabelRule{\oc R}
\UnaRule{\oc\Gamma,\oc{P_1},\dots,\oc{P_n}\vdash \oc{A},\wn\Delta}
\LabelRule{\rulename{cut}}
\BinRule{\oc\Gamma,\oc{P_1},\dots,\oc{P_{n-1}},P_n\vdash \oc{A},\wn\Delta}
\VdotsRule{}{\oc\Gamma,\oc{P_1},P_2,\dots,P_n\vdash \oc{A},\wn\Delta}
\LabelRule{\rulename{cut}}
\BinRule{\oc\Gamma,P_1,\dots,P_n\vdash \oc{A},\wn\Delta}
\DisplayProof
</math>
}}


= Proof-nets =


We provide [[a formal account of nets]], but it is probably not the best way to learn about proof-nets if you have never seen them before.


= Provable formulas =


Important provable formulas are given by [[List of isomorphisms|isomorphisms]] and by [[List of equivalences|equivalences]].

In many of the cases below the [[Non provable formulas|converse implication does not hold]].

== Distributivities ==

=== Standard distributivities ===

<math>A\plus (B\with C) \limp (A\plus B)\with (A\plus C)</math>

<math>A\tens (B\with C) \limp (A\tens B)\with (A\tens C)</math>

<math>\exists \xi . (A \with B) \limp (\exists \xi . A) \with (\exists \xi . B)</math>

=== Linear distributivities ===

<math>A\tens (B\parr C) \limp (A\tens B)\parr C</math>

<math>\exists \xi. (A \parr B) \limp A \parr \exists \xi.B  \quad  (\xi\notin A)</math>

<math>A \tens \forall \xi.B \limp \forall \xi. (A \tens B) \quad  (\xi\notin A)</math>

== Factorizations ==

<math>(A\with B)\plus (A\with C) \limp A\with (B\plus C)</math>

<math>(A\parr B)\plus (A\parr C) \limp A\parr (B\plus C)</math>

<math>(\forall \xi . A) \plus (\forall \xi . B) \limp \forall \xi . (A \plus B)</math>

== Identities ==

<math>\one \limp A\orth\parr A</math>

<math>A\tens A\orth \limp\bot</math>

== Additive structure ==

<math>
\begin{array}{rclcrclcrcl}
  A\with B &amp;\limp&amp; A &amp;\quad&amp; A\with B &amp;\limp&amp; B &amp;\quad&amp; A &amp;\limp&amp; \top\\
  A &amp;\limp&amp; A\plus B &amp;\quad&amp; B &amp;\limp&amp; A\plus B &amp;\quad&amp; \zero &amp;\limp&amp; A
\end{array}
</math>

== Quantifiers ==

<math>
\begin{array}{rcll}
  A &amp;\limp&amp; \forall \xi.A  &amp;\quad  (\xi\notin A) \\
  \exists \xi.A &amp;\limp&amp; A  &amp;\quad  (\xi\notin A)
\end{array}
</math>

<br />

<math>
\begin{array}{rcl}
  \forall \xi_1.\forall \xi_2. A &amp;\limp&amp; \forall \xi. A[^\xi/_{\xi_1},^\xi/_{\xi_2}] \\
  \exists \xi.A[^\xi/_{\xi_1},^\xi/_{\xi_2}] &amp;\limp&amp; \exists \xi_1. \exists \xi_2.A
\end{array}
</math>

== Exponential structure ==

Provable formulas involving exponential connectives only provide us with the [[lattice of exponential modalities]].

<math>
\begin{array}{rclcrcl}
  \oc A &amp;\limp&amp; A &amp;\quad&amp; A&amp;\limp&amp;\wn A\\
  \oc A &amp;\limp&amp; 1 &amp;\quad&amp; \bot &amp;\limp&amp; \wn A
\end{array}
</math>

== Monoidality of exponentials ==

<math>
\begin{array}{rcl}
  \wn(A\parr B) &amp;\limp&amp; \wn A\parr\wn B \\
  \oc A\tens\oc B &amp;\limp&amp; \oc(A\tens B) \\
\\
 \oc{(A \with B)} &amp;\limp&amp; \oc{A} \with \oc{B} \\
 \wn{A} \plus \wn{B} &amp;\limp&amp; \wn{(A \plus B)} \\
\\
 \wn{(A \with B)} &amp;\limp&amp; \wn{A} \with \wn{B} \\
 \oc{A} \plus \oc{B} &amp;\limp&amp; \oc{(A \plus B)}
\end{array}
</math>

== Promotion principles ==

<math>
\begin{array}{rcl}
 \oc{A} \tens \wn{B} &amp;\limp&amp; \wn{(A \tens B)} \\
 \oc{(A \parr B)} &amp;\limp&amp; \wn{A} \parr \oc{B}
\end{array}
</math>

== Commutations ==

<math>\exists \xi . \wn A \limp \wn{\exists \xi . A}</math>

<math>\oc{\forall \xi . A} \limp \forall \xi . \oc A</math>

<math>\wn{\forall \xi . A} \limp \forall \xi . \wn A</math>

<math>\exists \xi . \oc A \limp \oc{\exists \xi . A}</math>


= Regular formula =


A ''regular formula'' is a formula <math>R</math> such that <math>R\linequiv\wn\oc R</math>.

A formula <math>L</math> is ''co-regular'' if its dual <math>L\orth</math> is regular, that is if <math>L\linequiv\oc\wn L</math>.

== Alternative characterization ==

<math>R</math> is regular if and only if it is [[Sequent calculus#Equivalences|equivalent]] to a formula of the shape <math>\wn P</math> for some [[positive formula]] <math>P</math>.

{{Proof|
If <math>R</math> is regular then <math>R\linequiv\wn\oc R</math> with <math>\oc R</math> positive. If <math>R\linequiv\wn P</math> with <math>P</math> positive then <math>R</math> is regular since <math>P\linequiv\oc P</math>.
}}

== Regular connectives ==

A connective <math>c</math> of arity <math>n</math> is ''regular'' if for any regular formulas <math>R_1</math>,...,<math>R_n</math>, <math>c(R_1,\dots,R_n)</math> is regular.

{{Proposition|title=Regular connectives|
<math>\parr</math>, <math>\bot</math> and <math>\wn\oc</math> define regular connectives.
}}

{{Proof|
If <math>R</math> and <math>S</math> are regular, <math>R\parr S \linequiv \wn\oc R \parr \wn\oc S \linequiv \wn{(\oc R\plus\oc S)}</math> thus it is regular since <math>\oc R\plus\oc S</math> is positive.

<math>\bot\linequiv\wn\zero</math> thus it is regular since <math>\zero</math> is positive.

If <math>R</math> is regular then <math>\wn\oc R</math> is regular, since <math>\wn\oc\wn\oc R\linequiv \wn\oc R</math>.
}}

More generally, <math>\wn\oc A</math> is regular for any formula <math>A</math>.


= Relational semantics =


== Relational semantics ==

This is the simplest denotational semantics of linear logic. It consists in interpreting a formula <math>A</math> as a set <math>A^*</math> and a proof <math>\pi</math> of <math>A</math> as a subset <math>\pi^*</math> of <math>A^*</math>.

=== The category of sets and relations ===

It is the category <math>\mathbf{Rel}</math> whose objects are sets, and such that <math>\mathbf{Rel}(X,Y)=\powerset{X\times Y}</math>. Composition is the ordinary composition of relations: given <math>s\in\mathbf{Rel}(X,Y)</math> and <math>t\in\mathbf{Rel}(Y,Z)</math>, one 
sets <math>t\circ s=\set{(a,c)\in X\times Z}{\exists b\in Y\ (a,b)\in s\ \text{and}\ (b,c)\in t}</math> and the identity morphism is the diagonal relation <math>\mathsf{Id}_X=\set{(a,a)}{a\in X}</math>.

An isomorphism in the category <math>\mathbf{Rel}</math> is a relation which is a bijection, as easily checked.

==== Monoidal structure ====

The tensor product is the usual cartesian product of sets <math>X\tens Y=X\times Y</math> (which <em>is not</em> a cartesian product in the category <math>\mathbf{Rel}</math> in the categorical sense). It is a bifunctor: given <math>s_i\in\mathbf{Rel}(X_i,Y_i)</math> (for <math>i=1,2</math>), one sets <math>s_1\tens s_2=\set{((a_1,a_2),(b_1,b_2))}{(a_i,b_i)\in s_i\ \text{for}\ i=1,2}</math>. The unit of this tensor product is <math>\one=\{*\}</math> where <math>*</math> is an arbitrary element.

For defining a monoidal category, it is not sufficient to provide the definition of the tensor product functor <math>\tens</math> and its unit <math>\one</math>, one has also to provide natural isomorphisms <math>\lambda_X\in\mathbf{Rel}(\one\tens X,X)</math>,
<math>\rho_X\in\mathbf{Rel}(X\tens\one,X)</math> (left and right neutrality of <math>\one</math> for <math>\tens</math>) and <math>\alpha_{X,Y,Z}\in\mathbf{Rel}((X\tens Y)\tens Z,X\tens(Y\tens Z))</math> (associativity of <math>\tens</math>). All these isomorphisms have to satisfy a number of commutations. In the present case, they are defined in the obvious way.

This monoidal category <math>(\mathbf{Rel},\tens,\one,\lambda,\rho)</math> is symmetric, meaning that it is endowed with an additional natural isomorphism <math>\sigma_{X,Y}\in\mathbf{Rel}(X\tens Y,Y\tens X)</math>, also subject to some commutations. Here, again, this isomorphism is defined in the obvious way (symmetry of the cartesian product). So, to be precise, the SMCC (symmetric monoidal closed category) <math>\mathbf{Rel}</math> is the tuple <math>(\mathbf{Rel},\tens,\one,\lambda,\rho,\alpha,\sigma)</math>, but we shall simply denote it as <math>\mathbf{Rel}</math>.

The SMCC <math>\mathbf{Rel}</math> is closed. This means that, given any object <math>X</math> of <math>\mathbf{Rel}</math> (a set), the functor <math>Z\mapsto Z\tens X</math> (from <math>\mathbf{Rel}</math> to <math>\mathbf{Rel}</math>) admits a right adjoint <math>Y\mapsto (X\limp Y)</math> (from <math>\mathbf{Rel}</math> to <math>\mathbf{Rel}</math>). In other words, for any objects <math>X</math> and <math>Y</math>, we are given an object <math>X\limp Y</math> and a morphism <math>\mathsf{ev}_{X,Y}\in\mathbf{Rel}((X\limp Y)\tens X,Y)</math> with the following universal property: for any morphism <math>s\in\mathbf{Rel}(Z\tens X,Y)</math>, there is a unique morphism <math>\mathsf{fun}(s)\in\mathbf{Rel}(Z,X\limp Y)</math> such that <math>\mathsf{ev}_{X,Y}\circ(\mathsf{fun}(s)\tens\mathsf{Id}_X)=s</math>.

The definition of all these data is quite simple in <math>\mathbf{Rel}</math>: <math>X\limp Y=X\times Y</math>, <math>\mathsf{ev}_{X,Y}=\set{(((a,b),a),b)}{(a,b)\in X\times Y}</math> and <math>\mathsf{fun}(s)=\set{(c,(a,b))}{((c,a),b)\in s}</math>.

Let <math>\bot=\one=\{*\}</math>. Then we have <math>\mathsf{ev}\circ\sigma:\mathbf{Rel}(X\tens(X\limp\bot),\bot)</math> and hence <math>\eta_X=\mathsf{fun}(\mathsf{ev}\circ\sigma)\in\mathbf{Rel}(X,(X\limp\bot)\limp\bot)</math>. It is clear that <math>\eta=\set{(a,((a,*),*))}{a\in X}</math> and hence <math>\eta</math> is a natural isomorphism: one says that the SMCC <math>\mathbf{Rel}</math> is a *-autonomous category, with <math>\bot</math> as dualizing object.

==== Additives ====

Given a family <math>(X_i)_{i\in I}</math>, let <math>\with_{i\in I}X_i=\cup_{i\in I}\{i\}\times X_i</math>. Let <math>\pi_j\in\mathbf{Rel}(\with_{i\in I}X_i,X_j)</math> given by <math>\pi_j=\set{((j,a),a)}{a\in X_j}</math>. Then <math>(\with_{i\in I}X_i,(\pi_i)_{i\in I})</math> is the cartesian product of the <math>X_i</math>s in the category <math>\mathbf{Rel}</math>.

==== Exponentials ====

One defines <math>\oc X</math> as the set of all finite multisets of elements of <math>X</math>. Given <math>s\in\mathbf{Rel}(X,Y)</math>, one defines <math>\oc s=\set{([a_1,\dots,a_n],[b_1,\dots,b_n])}{n\in\mathbb N\ \text{and}\ \forall i\ (a_i,b_i)\in s}</math> where <math>[a_1,\dots,a_n]</math> is the multiset containing <math>a_1,\dots,a_n</math>, taking multiplicities into account. This defines a functor <math>\mathbf{Rel}\to\mathbf{Rel}</math>, that we endow with a comonad structure as follows:
* the counit, called dereliction, is the natural transformation <math>\mathsf{der}_X\in\mathbf{Rel}(\oc X,X)</math>, given by <math>\mathsf{der}_X=\set{([a],a)}{a\in X}</math>
* the comultiplication, called digging, is the natural transformation <math>\mathsf{digg}_X\in\mathbf{Rel}(\oc X,\oc\oc X)</math>, given by <math>\mathsf{digg}_X=\set{(m_1+\cdots+m_n,[m_1,\dots,m_n])}{n\in\mathbb N\ \text{and}\ m_1,\dots,m_n\in\oc X}</math>

=== Interpretation of propositional linear logic (<math>LL_0</math>) ===

The structure described above gives rise to the following interpretation of
formulas and proofs of linear logic.

For all propositional variable <math>X</math>, fix a set <math>\web X</math>.
Then with each formula <math>A</math>, we associate a set <math>\web A</math> as follows: 
* <math>\web{A\orth}=\web A</math>;
* <math>\web{A\tens B}=\web{A\parr B}=\web A\times\web B</math>;
* <math>\web{A\with B}=\web{A\plus B}=(\{1\}\times\web A)\cup(\{2\}\times\web B)</math>;
* <math>\web{\oc A}=\web{\wn A}=\finmulset{\web A}</math>.

We then interpret the proofs of <math>LL_0</math> as follows: with each proof
<math>\pi</math> of sequent <math>\vdash A_1,\ldots,A_n</math>, we associate a
subset <math>\sem\pi\subseteq\web{A_1}\times\cdots\times\web{A_n}</math>.
* Identity group:
*: <math>\sem{
\LabelRule{ \rulename{axiom} }
\NulRule{ \vdash A\orth, A }
\DisplayProof}=\set{(a,a)}{a\in\web A}
</math> 
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, A }
\AxRule{}
\VdotsRule{ \rho }{ \vdash \Delta, A\orth }
\LabelRule{ \rulename{cut} }
\BinRule{ \vdash \Gamma, \Delta }
\DisplayProof} = \set{(\gamma,\delta)}{\exists a\in\web A,\ (\gamma,a)\in\sem\pi\land(\delta,a)\in\sem\rho}
</math>
* Multiplicative group:
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, A }
\AxRule{}
\VdotsRule{ \rho }{ \vdash \Delta, B }
\LabelRule{ \tens }
\BinRule{ \vdash \Gamma, \Delta, A \tens B }
\DisplayProof} = \set{(\gamma,\delta,a,b)}{(\gamma,a)\in\sem\pi\land(\delta,b)\in\sem\rho}
</math>
*: <math>
\sem{
\AxRule{ }
\VdotsRule{ \pi }{ \vdash \Gamma, A, B }
\LabelRule{ \parr }
\UnaRule{ \vdash \Gamma, A \parr B }
\DisplayProof} =  \set{(\gamma,(a,b))}{(\gamma,a,b)\in\sem\pi}
</math> 
*: <math>
\sem{
\LabelRule{ \one }
\NulRule{ \vdash \one }
\DisplayProof} = \{ * \}
</math>
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma }
\LabelRule{ \bot }
\UnaRule{ \vdash \Gamma, \bot }
\DisplayProof} = \set{(\gamma,*)}{\gamma\in\sem\pi}
</math>
* Additive group:
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, A }
\LabelRule{ \plus_1 }
\UnaRule{ \vdash \Gamma, A \plus B }
\DisplayProof} = \set{(\gamma,(1,a))}{(\gamma,a)\in\sem\pi}
</math>
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, B }
\LabelRule{ \plus_2 }
\UnaRule{ \vdash \Gamma, A \plus B }
\DisplayProof} = \set{(\gamma,(2,b))}{(\gamma,b)\in\sem\pi}
</math>
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, A }
\AxRule{}
\VdotsRule{ \rho }{ \vdash \Gamma, B }
\LabelRule{ \with }
\BinRule{ \vdash \Gamma, A \with B }
\DisplayProof} = \set{(\gamma,(1,a))}{(\gamma,a)\in\sem\pi} \cup \set{(\gamma,(2,b))}{(\gamma,b)\in\sem\rho}
</math>
*: <math>
\sem{
\LabelRule{ \top }
\NulRule{ \vdash \Gamma, \top }
\DisplayProof} = \emptyset
</math>
* Exponential group:
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, A }
\LabelRule{ d }
\UnaRule{ \vdash \Gamma, \wn A }
\DisplayProof} = \set{(\gamma,[a])}{(\gamma,a)\in\sem\pi}
</math>
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma }
\LabelRule{ w }
\UnaRule{ \vdash \Gamma, \wn A }
\DisplayProof} =  \set{(\gamma,[])}{\gamma\in\sem\pi}
</math>
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \Gamma, \wn A, \wn A }
\LabelRule{ c }
\UnaRule{ \vdash \Gamma, \wn A }
\DisplayProof} =  \set{(\gamma,m+n)}{(\gamma,m,n)\in\sem\pi}
</math>
*: <math>
\sem{
\AxRule{}
\VdotsRule{ \pi }{ \vdash \wn A_1,\ldots,\wn A_n,B }
\LabelRule{ \oc }
\UnaRule{ \vdash \wn A_1,\ldots,\wn A_n,\oc B }
\DisplayProof} = \set{
\left(\sum_{i=1}^k m_1^i,\ldots,\sum_{i=1}^k m_n^i,[b_1,\ldots,b_k]\right)}
{k \in \mathbb{N} \ \text{and} \ \forall 1 \leq i \leq k,\ (m_1^i,\ldots,m_n^i,b_i)\in\sem\pi}
</math>

{{Theorem|If proof <math>\pi'</math> is obtained from <math>\pi</math> by eliminating a cut, 
then <math>\sem\pi=\sem{\pi'}</math>.}}


= Reversibility and focalization =


== Reversibility ==

{{Theorem|
Negative connectives are reversible:

* A sequent <math>\vdash\Gamma,A\parr B</math> is provable if and only if <math>\vdash\Gamma,A,B</math> is provable.
* A sequent <math>\vdash\Gamma,A\with B</math> is provable if and only if <math>\vdash\Gamma,A</math> and <math>\vdash\Gamma,B</math> are provable.
* A sequent <math>\vdash\Gamma,\bot</math> is provable if and only if <math>\vdash\Gamma</math> is provable.
* A sequent <math>\vdash\Gamma,\forall\xi A</math> is provable if and only if <math>\vdash\Gamma,A</math> is provable, for some fresh variable <math>\xi</math>.
}}

{{Proof|
We start with the case of the <math>\parr</math> connective.
If <math>\vdash\Gamma,A,B</math> is provable, then by the introduction rule for <math>\parr</math>
we know that <math>\vdash\Gamma,A\parr B</math> is provable.
For the reverse implication we proceed by induction on a proof <math>\pi</math> of
<math>\vdash\Gamma,A\parr B</math>.

* If the last rule of <math>\pi</math> is the introduction of the <math>\parr</math> in <math>A\parr B</math>, then the premiss is exacty <math>\vdash\Gamma,A,B</math> so we can conclude.
* The other case where the last rule introduces <math>A\parr B</math> is when <math>\pi</math> is an axiom rule, hence <math>\Gamma=A\orth\tens B\orth</math>. Then we can conclude with the proof
* Otherwise <math>A\parr B</math> is in the context of the last rule. If the last rule is a tensor, then <math>\pi</math> has the shape
* or the same with <math>A\parr B</math> in the conclusion of <math>\pi_2</math> instead. By induction hypothesis on <math>\pi_1</math> we get a proof <math>\pi'_1</math> of <math>\vdash\Gamma_1,A,B,C</math>, then we can conclude with the proof
* The case of the cut rule has the same structure as the tensor rule.
* In the case of the <math>\with</math> rule, we have <math>A\with B</math> in both premisses and we conclude similarly, using the induction hypothesis on both <math>\pi_1</math> and <math>\pi_2</math>.
* If <math>A\parr B</math> is in the context of a rules for <math>\parr</math>, <math>\plus</math>, <math>\bot</math> or quantifiers, or in the context of a dereliction, weakening or contraction, the situation is similar as for <math>\tens</math> except that we have only one premiss.
* If <math>A\parr B</math> is in the context of <math>\top</math> rules, we can freely change the context of the rule to get the expected one.
* The two remaining cases are if the last rule is the rule for <math>1</math> or a promotion. By the constraints these rules impose on the contexts, these cases cannot happen.
The <math>\with</math> connective is treated in the same way.
In this cases where <math>A\with B</math> is in the context of a rule with two
premisses, the premiss where this formula is not present will be duplicated,
with one copy in the premiss for <math>A</math> and one in the premiss for <math>B</math>.

The <math>\forall</math> connective is also treated similarly.
Its peculiarity is that introducing <math>\forall\xi</math> requires that <math>\xi</math> does
not appear free in the context.
For all rules with one premiss except the quantifier rules, the set of fresh
variables in the same in the premiss and the conclusion, so everything works
well.
Other rules might change the set of free variables, but problems are avoided
by choosing for <math>\xi</math> a variable that is fresh for the whole proof we are
considering.
}}

Remark that this result is proved using only commutation rules, except when the
formula is introduced by an axiom rule.
Furthermore, if axioms are applied only on atoms, this particular case
disappears.

A consequence of this fact is that, when searching for a proof of some
sequent <math>\vdash\Gamma</math>, one can always start by decomposing negative
connectives in <math>\Gamma</math> without losing provability.
Applying this result to successive connectives, we can get generalized
formulations for more complex formulas. For instance:

* <math>\vdash\Gamma,(A\parr B)\parr(B\with C)</math> is provable
* iff <math>\vdash\Gamma,A\parr B,B\with C</math> is provable
* iff <math>\vdash\Gamma,A\parr B,B</math> and <math>\vdash\Gamma,A\parr B,C</math> are provable
* iff <math>\vdash\Gamma,A,B,B</math> and <math>\vdash\Gamma,A,B,C</math> are provable
So without loss of generality, we can assume that any proof of
<math>\vdash\Gamma,(A\parr B)\parr(B\with C)</math> ends like

<math>
  \AxRule{ \vdash \Gamma, A, B, B }
  \UnaRule{ \vdash \Gamma, A\parr B, B }
  \AxRule{ \vdash \Gamma, A, B, C }
  \UnaRule{ \vdash \Gamma, A\parr B, C }
  \BinRule{ \vdash \Gamma, A\parr B, B\with C }
  \UnaRule{ \vdash \Gamma, (A\parr B)\parr(B\with C) }
  \DisplayProof
</math>

In order to define a general statement for compound formulas, as well as an
analogous result for positive connectives, we need to give a proper status to
clusters of connectives of the same polarity.

== Generalized connectives and rules ==

{{Definition|
A ''positive generalized connective'' is a parametrized formula
<math>P[X_1,\ldots,X_n]</math> made from the variables <math>X_i</math> using the connectives
<math>\tens</math>, <math>\plus</math>, <math>\one</math>, <math>\zero</math>.

A ''negative generalized connective'' is a parametrized formula
<math>N[X_1,\ldots,X_n]</math> made from the variables <math>X_i</math> using the connectives
<math>\parr</math>, <math>\with</math>, <math>\bot</math>, <math>\top</math>.

If <math>C[X_1,\ldots,X_n]</math> is a generalized connectives (of any polarity), the
''dual'' of <math>C</math> is the connective <math>C^*</math> such that
<math>C^*[X_1\orth,\ldots,X_n\orth]=C[X_1,\ldots,X_n]\orth</math>.
}}

It is clear that dualization of generalized connectives is involutive and
exchanges polarities.
We do not include quantifiers in this definition, mainly for simplicity.
Extending the notion to quantifiers would only require taking proper care of
the scope of variables.

Sequent calculus provides introduction rules for each connective. Negative
connectives have one rule, positive ones may have any number of rules, namely
2 for <math>\plus</math> and 0 for <math>\zero</math>. We can derive introduction rules for the
generalized connectives by combining the different possible introduction rules
for each of their components.

Considering the previous example
<math>N[X_1,X_2,X_3]=(X_1\parr X_2)\parr(X_2\with X_3)</math>, we can derive an
introduction rule for <math>N</math> as

<math>
  \AxRule{ \vdash \Gamma, X_1, X_2, X_2 }
  \UnaRule{ \vdash \Gamma, X_1\parr X_2, X_2 }
  \AxRule{ \vdash \Gamma, X_1, X_2, X_3 }
  \UnaRule{ \vdash \Gamma, X_1\parr X_2, X_3 }
  \BinRule{ \vdash \Gamma, X_1\parr X_2, X_2\with X_3 }
  \UnaRule{ \vdash \Gamma, (X_1\parr X_2)\parr(X_2\with X_3) }
  \DisplayProof
\quad\text{or}\quad
  \AxRule{ \vdash \Gamma, X_1, X_2, X_2 }
  \AxRule{ \vdash \Gamma, X_1, X_2, X_3 }
  \BinRule{ \vdash \Gamma, X_1, X_2, X_2\with X_3 }
  \UnaRule{ \vdash \Gamma, X_1\parr X_2, X_2\with X_3 }
  \UnaRule{ \vdash \Gamma, (X_1\parr X_2)\parr(X_2\with X_3) }
  \DisplayProof
</math>

but these rules only differ by the commutation of independent rules.
In particular, their premisses are the same.
The dual of <math>N</math> is <math>P[X_1,X_2,X_3]=(X_1\tens X_2)\tens(X_2\plus X_3)</math>, for
which we have two possible derivations:

<math>
  \AxRule{ \vdash \Gamma_1, X_1 }
  \AxRule{ \vdash \Gamma_2, X_2 }
  \BinRule{ \vdash \Gamma_1, \Gamma_2, X_1\tens X_2 }
  \AxRule{ \vdash \Gamma_3, X_2 }
  \UnaRule{ \vdash \Gamma_3, X_2\plus X_3 }
  \BinRule{ \vdash \Gamma_1, \Gamma_2, \Gamma_3, (X_1\tens X_2)\tens(X_2\plus X_3) }
  \DisplayProof
\qquad
  \AxRule{ \vdash \Gamma_1, X_1 }
  \AxRule{ \vdash \Gamma_2, X_2 }
  \BinRule{ \vdash \Gamma_1, \Gamma_2, X_1\tens X_2 }
  \AxRule{ \vdash \Gamma_3, X_3 }
  \UnaRule{ \vdash \Gamma_3, X_2\plus X_3 }
  \BinRule{ \vdash \Gamma_1, \Gamma_2, \Gamma_3, (X_1\tens X_2)\tens(X_2\plus X_3) }
  \DisplayProof
</math>

These are actually different, in particular their premisses differ.
Each possible derivation corresponds to the choice of one side of the <math>\plus</math>
connective.

We can remark that the branches of the negative inference precisely correspond
to the possible positive inferences:

* the first branch of the negative inference has a premiss <math>X_1,X_2,X_2</math> and the first positive inference has three premisses, holding <math>X_1</math>, <math>X_2</math> and <math>X_2</math> respectively.
* the second branch of the negative inference has a premiss <math>X_1,X_2,X_3</math> and the second positive inference has three premisses, holding <math>X_1</math>, <math>X_2</math> and <math>X_3</math> respectively.
This phenomenon extends to all generalized connectives.

{{Definition|
The ''branching'' of a generalized connective <math>P[X_1,\ldots,X_n]</math> is the
multiset <math>\mathcal{I}_P</math> of multisets over <math>\{1,\ldots,n\}</math> defined
inductively as

<math> \mathcal{I}_{P\tens Q} := [ I+J \mid I\in\mathcal{I}_P, J\in\mathcal{I}_Q ] </math>,
<math> \mathcal{I}_{P\plus Q} := \mathcal{I}_P + \mathcal{I}_Q </math>,
<math> \mathcal{I}_\one := [[]] </math>,
<math> \mathcal{I}_\zero := [] </math>,
<math> \mathcal{I}_{X_i} := [[i]] </math>.

The branching of a negative generalized connective is the branching of its
dual. Elements of a branching are called branches.
}}

In the example above, the branching will be <math>[[1,2,2],[1,2,3]]</math>, which
corresponds to the granches of the negative inference and to the cases of
positive inference.

{{Definition|
Let <math>\mathcal{I}</math> be a branching.
Write <math>\mathcal{I}</math> as <math>[I_1,\ldots,I_k]</math> and write each <math>I_j</math> as
<math>[i_{j,1},\ldots,i_{j,\ell_j}]</math>.
The derived rule for a negative generalized connective <math>N</math> with
branching <math>\mathcal{I}</math> is

<math>
    \AxRule{ \vdash \Gamma, A_{i_{1,1}}, \ldots, A_{i_{1,\ell_1}} }
    \AxRule{ \cdots }
    \AxRule{ \vdash \Gamma, A_{i_{k,1}}, \ldots, A_{i_{k,\ell_k}} }
    \LabelRule{N}
    \TriRule{ \vdash \Gamma, N[A_1,\ldots,A_n] }
    \DisplayProof
  </math>

For each branch <math>I=[i_1,\ldots,i_\ell]</math> of a positive generalized connective
<math>P</math>, the derived rule for branch <math>I</math> of <math>P</math> is

<math>
    \AxRule{ \vdash \Gamma_1, A_{i_1} }
    \AxRule{ \cdots }
    \AxRule{ \vdash \Gamma_\ell, A_{i_\ell} }
    \LabelRule{P_I}
    \TriRule{ \vdash \Gamma_1, \ldots, \Gamma_\ell, P[A_1,\ldots,A_n] }
    \DisplayProof
  </math>
}}

The reversibility property of negative connectives can be rephrased in a
generalized way as

{{Theorem|
Let <math>N</math> be a negative generalized connective. A sequent
<math>\vdash\Gamma,N[A_1,\ldots,A_n]</math> is provable if and only if, for each
<math>[i_1,\ldots,i_k]\in\mathcal{I}_N</math>, the sequent
<math>\vdash\Gamma,A_{i_1},\ldots,A_{i_k}</math> is provable.
}}

The corresponding property for positive connectives is the focalization
property, defined in the next section.

== Focalization ==

{{Definition|
A formula is ''positive'' if it has a main connective among
<math>\tens</math>, <math>\plus</math>, <math>\one</math>, <math>\zero</math>.
It is called ''negative'' if it has a main connective among
<math>\parr</math>, <math>\with</math>, <math>\bot</math>, <math>\top</math>.
It is called ''neutral'' if it is neither positive nor negative.
}}

If we extended the theory to include quantifiers in generalized connectives,
then the definition of positive and negative formulas would be extended to
include them too.

{{Definition|
A proof <math>\pi\vdash\Gamma,A</math> is said to be ''positively focused on <math>A</math>'' if it has the shape

<math>
    \AxRule{ \pi_1 \vdash \Gamma_1, A_{i_1} }
    \AxRule{ \cdots }
    \AxRule{ \pi_\ell \vdash \Gamma_\ell, A_{i_\ell} }
    \LabelRule{P_{[i_1,\ldots,i_\ell]}}
    \TriRule{ \vdash  \Gamma_1, \ldots, \Gamma_\ell, P[A_1,\ldots,A_n] }
    \DisplayProof
  </math>

where <math>P</math> is a positive generalized connective, the <math>A_i</math> ar non-positive
and <math>A=P[A_1,\ldots,A_n]</math>. The formula <math>A</math> is called the ''focus'' of the
proof <math>\pi</math>.
}}

In other words, a proof is positively focused on a conclusion <math>A</math> if its last rules
build <math>A</math> from some of its non-positive subformulas in one cluster of
inferences. Note that this notion only makes sense for a sequent made only
of positive formulas, since by this definition a proof is obviously positively focused on
any of its non-positive conclusions, using the degenerate generalized
connective <math>P[X]=X</math>.

{{Theorem|
A sequent <math>\vdash\Gamma</math> is cut-free provable if and only if it is provable
by a cut-free proof that is positively focused.
}}

{{Proof|
We reason by induction on a proof <math>\pi</math> of <math>\Gamma</math>.
As noted above, the result  trivially holds if <math>\Gamma</math> has a non-positive
formula.
We can thus assume that <math>\Gamma</math> contains only positive formulas and reason
on the nature of the last rule, which is necessarily the introduction of a
positive connective (it cannot be an axiom rule because an axiom  always has
at least on non-positive conclusion).

Suppose that the last rule of <math>\pi</math> introduces a tensor, so that <math>\pi</math> is

<math>
    \AxRule{ \rho \vdash \Gamma, A }
    \AxRule{ \theta \vdash \Delta, B }
    \BinRule{ \vdash \Gamma, \Delta, A\tens B }
    \DisplayProof
  </math>

By induction hypothesis, there are positively focused proofs <math>\rho'\vdash\Gamma,A</math>
and <math>\theta'\vdash\Delta,B</math>.
If <math>A</math> is the focus of <math>\rho'</math> and <math>B</math> is the focus of <math>\theta'</math>, then the
proof

<math>
    \AxRule{ \rho' \vdash \Gamma, A }
    \AxRule{ \theta' \vdash \Delta, B }
    \BinRule{ \vdash \Gamma, \Delta, A\tens B }
    \DisplayProof
  </math>

is positively focused on <math>A\tens B</math>, so we can conclude.
Otherwise, one of the two proofs is positively focused on another conclusion.
Without loss of generality, suppose that <math>\rho'</math> is not positively focused on <math>A</math>.
Then it decomposes as

<math>
    \AxRule{ \rho_1 \vdash \Gamma_1, C_{i_1} }
    \AxRule{ \cdots }
    \AxRule{ \rho_\ell \vdash \Gamma_\ell, C_{i_\ell} }
    \TriRule{ \vdash  \Gamma_1, \ldots, \Gamma_\ell, P[C_1,\ldots,C_n] }
    \DisplayProof
  </math>

where the <math>C_i</math> are not positive and <math>A</math> belongs to some context <math>\Gamma_j</math>
that we will write <math>\Gamma'_j,A</math>.
Then we can conclude with the proof

<math>
    \AxRule{ \rho_1 \vdash \Gamma_1, C_{i_1} \quad\cdots }
    \AxRule{ \rho_j \vdash \Gamma_j, A, C_{i_j} }
    \AxRule{ \theta \vdash \Delta, B }
    \BinRule{ \vdash \Gamma_j, \Delta, A\tens B, C_{i_j} }
    \AxRule{ \cdots\quad \rho_\ell \vdash \Gamma_\ell, C_{i_\ell} }
    \TriRule{ \vdash \Gamma_1, \ldots, \Gamma_\ell, \Delta, A\tens B, P[C_1,\ldots,C_n] }
    \DisplayProof
  </math>

which is positively focused on <math>P[C_1,\ldots,C_n]</math>.

If the last rule of <math>\pi</math> introduces a <math>\plus</math>, we proceed the same way
except that there is only one premiss.
If the last rule of <math>\pi</math> introduces a <math>\one</math>, then it is the only rule of
<math>\pi</math>, which is thus positively focused on this <math>\one</math>.
}}

As in the reversibility theorem, this proof only makes use of commutation of
independent rules.

These results say nothing about exponential modalities, because they respect
neither reversibility nor focalization. However, if we consider the fragment
of LL which consists only of multiplicative and additive connectives, we can
restrict the proof rules to enforce focalization without loss of
expressiveness.


= Semantics =


Linear Logic has numerous semantics some of which are described in details in the next sections.

* [[Coherent semantics]]
* [[Phase semantics]]
* [[Categorical semantics]]
* [[Relational semantics]]
* [[Finiteness semantics]]
* [[Geometry of interaction]]
* [[Game semantics]]

[[Provable formulas|Common properties]] may be found in most of these models. We will denote by <math>A\longrightarrow B</math> the fact that there is a canonical morphism from <math>A</math> to <math>B</math> and by <math>A\cong B</math> the fact that there is a canonical [[isomorphism]] between <math>A</math> and <math>B</math>. By &quot;canonical&quot; we mean that these (iso)morphisms are natural transformations.


= Sequent calculus =


This article presents the language and sequent calculus of second-order
linear logic and the basic properties of this sequent calculus.
The core of the article uses the two-sided system with negation as a proper
connective; the [[#One-sided sequent calculus|one-sided system]], often used
as the definition of linear logic, is presented at the end of the page.

== Formulas ==

Atomic formulas, written <math>\alpha,\beta,\gamma</math>, are predicates of
the form <math>p(t_1,\ldots,t_n)</math>, where the <math>t_i</math> are terms
from some first-order language.
The predicate symbol <math>p</math> may be either a predicate constant or a
second-order variable.
By convention we will write first-order variables as <math>x,y,z</math>,
second-order variables as <math>X,Y,Z</math>, and <math>\xi</math> for a
variable of arbitrary order (see [[Notations]]).

Formulas, represented by capital letters <math>A</math>, <math>B</math>,
<math>C</math>, are built using the following connectives:

{| style="border-spacing: 2em 0"
|-
| <math>\alpha</math>
| atom
| <math>A\orth</math>
| negation
|-
| <math>A \tens B</math>
| tensor
| <math>A \parr B</math>
| par
| multiplicatives
|-
| <math>\one</math>
| one
| <math>\bot</math>
| bottom
| multiplicative units
|-
| <math>A \plus B</math>
| plus
| <math>A \with B</math>
| with
| additives
|-
| <math>\zero</math>
| zero
| <math>\top</math>
| top
| additive units
|-
| <math>\oc A</math>
| of course
| <math>\wn A</math>
| why not
| exponentials
|-
| <math>\exists \xi.A</math>
| there exists
| <math>\forall \xi.A</math>
| for all
| quantifiers
|}

Each line (except the first one) corresponds to a particular class of
connectives, and each class consists in a pair of connectives.
Those in the left column are called [[positive formula|positive]] and those in
the right column are called [[negative formula|negative]].
The ''tensor'' and ''with'' connectives are conjunctions while ''par'' and
''plus'' are disjunctions.
The exponential connectives are called ''modalities'', and traditionally read
''of course <math>A</math>'' for <math>\oc A</math> and ''why not
<math>A</math>'' for <math>\wn A</math>.
Quantifiers may apply to first- or second-order variables.

There is no connective for implication in the syntax of standard linear logic.
Instead, a ''linear implication'' is defined similarly to the decomposition
<math>A\imp B=\neg A\vee B</math> in classical logic, as
<math>A\limp B:=A\orth\parr B</math>.

Free and bound variables and first-order substitution are defined in the
standard way.
Formulas are always considered up to renaming of bound names.
If <math>A</math> is a formula, <math>X</math> is a second-order variable and
<math>B[x_1,\ldots,x_n]</math> is a formula with variables <math>x_i</math>,
then the formula <math>A[B/X]</math> is <math>A</math> where every atom
<math>X(t_1,\ldots,t_n)</math> is replaced by <math>B[t_1,\ldots,t_n]</math>.

== Sequents and proofs ==

A sequent is an expression <math>\Gamma\vdash\Delta</math> where
<math>\Gamma</math> and <math>\Delta</math> are finite multisets of formulas.
For a multiset <math>\Gamma=A_1,\ldots,A_n</math>, the notation
<math>\wn\Gamma</math> represents the multiset
<math>\wn A_1,\ldots,\wn A_n</math>.
Proofs are labelled trees of sequents, built using the following inference
rules:
* Identity group: <math>
\LabelRule{\rulename{axiom}}
\NulRule{ A \vdash A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash A, \Delta }
\AxRule{ \Gamma', A \vdash \Delta' }
\LabelRule{\rulename{cut}}
\BinRule{ \Gamma, \Gamma' \vdash \Delta, \Delta' }
\DisplayProof
</math>
* Negation: <math>
\AxRule{ \Gamma \vdash A, \Delta }
\UnaRule{ \Gamma, A\orth \vdash \Delta }
\LabelRule{n_L}
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma, A \vdash \Delta }
\UnaRule{ \Gamma \vdash A\orth, \Delta }
\LabelRule{n_R}
\DisplayProof
</math>
* Multiplicative group:
** tensor: <math>
\AxRule{ \Gamma, A, B \vdash \Delta }
\LabelRule{ \tens_L }
\UnaRule{ \Gamma, A \tens B \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash A, \Delta }
\AxRule{ \Gamma' \vdash B, \Delta' }
\LabelRule{ \tens_R }
\BinRule{ \Gamma, \Gamma' \vdash A \tens B, \Delta, \Delta' }
\DisplayProof
</math>
** par: <math>
\AxRule{ \Gamma, A \vdash \Delta }
\AxRule{ \Gamma', B \vdash \Delta' }
\LabelRule{ \parr_L }
\BinRule{ \Gamma, \Gamma', A \parr B \vdash \Delta, \Delta' }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash A, B, \Delta }
\LabelRule{ \parr_R }
\UnaRule{ \Gamma \vdash A \parr B, \Delta }
\DisplayProof
</math>
** one: <math>
\AxRule{ \Gamma \vdash \Delta }
\LabelRule{ \one_L }
\UnaRule{ \Gamma, \one \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\LabelRule{ \one_R }
\NulRule{ \vdash \one }
\DisplayProof
</math>
** bottom: <math>
\LabelRule{ \bot_L }
\NulRule{ \bot \vdash }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta }
\LabelRule{ \bot_R }
\UnaRule{ \Gamma \vdash \bot, \Delta }
\DisplayProof
</math>
* Additive group:
** plus: <math>
\AxRule{ \Gamma, A \vdash \Delta }
\AxRule{ \Gamma, B \vdash \Delta }
\LabelRule{ \plus_L }
\BinRule{ \Gamma, A \plus B \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash A, \Delta }
\LabelRule{ \plus_{R1} }
\UnaRule{ \Gamma \vdash A \plus B, \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash B, \Delta }
\LabelRule{ \plus_{R2} }
\UnaRule{ \Gamma \vdash A \plus B, \Delta }
\DisplayProof
</math>
** with: <math>
\AxRule{ \Gamma, A \vdash \Delta }
\LabelRule{ \with_{L1} }
\UnaRule{ \Gamma, A \with B \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma, B \vdash \Delta }
\LabelRule{ \with_{L2} }
\UnaRule{ \Gamma, A \with B \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash A, \Delta }
\AxRule{ \Gamma \vdash B, \Delta }
\LabelRule{ \with_R }
\BinRule{ \Gamma \vdash A \with B, \Delta }
\DisplayProof
</math>
** zero: <math>
\LabelRule{ \zero_L }
\NulRule{ \Gamma, \zero \vdash \Delta }
\DisplayProof
</math>
** top: <math>
\LabelRule{ \top_R }
\NulRule{ \Gamma \vdash \top, \Delta }
\DisplayProof
</math>
* Exponential group:
** of course: <math>
\AxRule{ \Gamma, A \vdash \Delta }
\LabelRule{ d_L }
\UnaRule{ \Gamma, \oc A \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta }
\LabelRule{ w_L }
\UnaRule{ \Gamma, \oc A \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma, \oc A, \oc A \vdash \Delta }
\LabelRule{ c_L }
\UnaRule{ \Gamma, \oc A \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \oc A_1, \ldots, \oc A_n \vdash B ,\wn B_1, \ldots, \wn B_m }
\LabelRule{ \oc_R }
\UnaRule{ \oc A_1, \ldots, \oc A_n \vdash \oc B ,\wn B_1, \ldots, \wn B_m }
\DisplayProof
</math>
** why not: <math>
\AxRule{ \Gamma \vdash A, \Delta }
\LabelRule{ d_R }
\UnaRule{ \Gamma \vdash \wn A, \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta }
\LabelRule{ w_R }
\UnaRule{ \Gamma \vdash \wn A, \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \wn A, \wn A, \Delta }
\LabelRule{ c_R }
\UnaRule{ \Gamma \vdash \wn A, \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \oc A_1, \ldots, \oc A_n, A \vdash \wn B_1, \ldots, \wn B_m }
\LabelRule{ \wn_L }
\UnaRule{ \oc A_1, \ldots, \oc A_n, \wn A \vdash \wn B_1, \ldots, \wn B_m }
\DisplayProof
</math>
* Quantifier group (in the <math>\exists_L</math> and <math>\forall_R</math> rules, <math>\xi</math> must not occur free in <math>\Gamma</math> or <math>\Delta</math>):
** there exists: <math>
\AxRule{ \Gamma , A \vdash \Delta }
\LabelRule{ \exists_L }
\UnaRule{ \Gamma, \exists\xi.A \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta, A[t/x] }
\LabelRule{ \exists^1_R }
\UnaRule{ \Gamma \vdash \Delta, \exists x.A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta, A[B/X] }
\LabelRule{ \exists^2_R }
\UnaRule{ \Gamma \vdash \Delta, \exists X.A }
\DisplayProof
</math>
** for all: <math>
\AxRule{ \Gamma, A[t/x] \vdash \Delta }
\LabelRule{ \forall^1_L }
\UnaRule{ \Gamma, \forall x.A \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma, A[B/X] \vdash \Delta }
\LabelRule{ \forall^2_L }
\UnaRule{ \Gamma, \forall X.A \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta, A }
\LabelRule{ \forall_R }
\UnaRule{ \Gamma \vdash \Delta, \forall\xi.A }
\DisplayProof
</math>

The left rules for ''of course'' and right rules for ''why not'' are called
''dereliction'', ''weakening'' and ''contraction'' rules.
The right rule for ''of course'' and the left rule for ''why not'' are called
''promotion'' rules.
Note the fundamental fact that there are no contraction and weakening rules
for arbitrary formulas, but only for the formulas starting with the
<math>\wn</math> modality.
This is what distinguishes linear logic from classical logic: if weakening and
contraction were allowed for arbitrary formulas, then <math>\tens</math> and <math>\with</math>
would be identified, as well as <math>\plus</math> and <math>\parr</math>, <math>\one</math> and <math>\top</math>,
<math>\zero</math> and <math>\bot</math>.
By ''identified'', we mean here that replacing a <math>\tens</math> with a <math>\with</math> or
vice versa would preserve provability.

Sequents are considered as multisets, in other words as sequences up to
permutation.
An alternative presentation would be to define a sequent as a finite sequence
of formulas and to add the exchange rules:
: <math>
\AxRule{ \Gamma_1, A, B, \Gamma_2 \vdash \Delta }
\LabelRule{\rulename{exchange}_L}
\UnaRule{ \Gamma_1, B, A, \Gamma_2 \vdash \Delta }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \Gamma \vdash \Delta_1, A, B, \Delta_2 }
\LabelRule{\rulename{exchange}_R}
\UnaRule{ \Gamma \vdash \Delta_1, B, A, \Delta_2 }
\DisplayProof
</math>

== Equivalences ==

Two formulas <math>A</math> and <math>B</math> are (linearly) equivalent,
written <math>A\linequiv B</math>, if both implications <math>A\limp B</math>
and <math>B\limp A</math> are provable.
Equivalently, <math>A\linequiv B</math> if both <math>A\vdash B</math> and
<math>B\vdash A</math> are provable.
Another formulation of <math>A\linequiv B</math> is that, for all
<math>\Gamma</math> and <math>\Delta</math>, <math>\Gamma\vdash\Delta,A</math>
is provable if and only if <math>\Gamma\vdash\Delta,B</math> is provable.

Two related notions are [[isomorphism]] (stronger than equivalence) and
[[equiprovability]] (weaker than equivalence).

=== De Morgan laws ===

Negation is involutive:
: <math>A\linequiv A\biorth</math>
Duality between connectives:
{|
|-
|align=&quot;right&quot;| <math> ( A \tens B )\orth </math>
| <math>\linequiv A\orth \parr B\orth </math>
|width=30|
|align=&quot;right&quot;| <math> ( A \parr B )\orth </math>
| <math>\linequiv A\orth \tens B\orth </math>
|-
|align=&quot;right&quot;| <math> \one\orth </math>
| <math>\linequiv \bot </math>
|
|align=&quot;right&quot;| <math> \bot\orth </math>
| <math>\linequiv \one </math>
|-
|align=&quot;right&quot;| <math> ( A \plus B )\orth </math>
| <math>\linequiv A\orth \with B\orth </math>
|
|align=&quot;right&quot;| <math> ( A \with B )\orth </math>
| <math>\linequiv A\orth \plus B\orth </math>
|-
|align=&quot;right&quot;| <math> \zero\orth </math>
| <math>\linequiv \top </math>
|
|align=&quot;right&quot;| <math> \top\orth </math>
| <math>\linequiv \zero </math>
|-
|align=&quot;right&quot;| <math> ( \oc A )\orth </math>
| <math>\linequiv \wn ( A\orth ) </math>
|
|align=&quot;right&quot;| <math> ( \wn A )\orth </math>
| <math>\linequiv \oc ( A\orth ) </math>
|-
|align=&quot;right&quot;| <math> ( \exists \xi.A )\orth </math>
| <math>\linequiv \forall \xi.( A\orth ) </math>
|
|align=&quot;right&quot;| <math> ( \forall \xi.A )\orth </math>
| <math>\linequiv \exists \xi.( A\orth ) </math>
|}

=== Fundamental equivalences ===

* Associativity, commutativity, neutrality:
*: <math>
A \tens (B \tens C) \linequiv (A \tens B) \tens C </math> &amp;emsp; <math>
A \tens B \linequiv B \tens A </math> &amp;emsp; <math>
A \tens \one \linequiv A </math>
*: <math>
A \parr (B \parr C) \linequiv (A \parr B) \parr C </math> &amp;emsp; <math>
A \parr B \linequiv B \parr A </math> &amp;emsp; <math>
A \parr \bot \linequiv A </math>
*: <math>
A \plus (B \plus C) \linequiv (A \plus B) \plus C </math> &amp;emsp; <math>
A \plus B \linequiv B \plus A </math> &amp;emsp; <math>
A \plus \zero \linequiv A </math>
*: <math>
A \with (B \with C) \linequiv (A \with B) \with C </math> &amp;emsp; <math>
A \with B \linequiv B \with A </math> &amp;emsp; <math>
A \with \top \linequiv A </math>
* Idempotence of additives:
*: <math>
A \plus A \linequiv A </math> &amp;emsp; <math>
A \with A \linequiv A </math>
* Distributivity of multiplicatives over additives:
*: <math>
A \tens (B \plus C) \linequiv (A \tens B) \plus (A \tens C) </math> &amp;emsp; <math>
A \tens \zero \linequiv \zero </math>
*: <math>
A \parr (B \with C) \linequiv (A \parr B) \with (A \parr C) </math> &amp;emsp; <math>
A \parr \top \linequiv \top </math>
* Defining property of exponentials:
*: <math>
\oc(A \with B) \linequiv \oc A \tens \oc B </math> &amp;emsp; <math>
\oc\top \linequiv \one </math>
*: <math>
\wn(A \plus B) \linequiv \wn A \parr \wn B </math> &amp;emsp; <math>
\wn\zero \linequiv \bot </math>
* Monoidal structure of exponentials:
*: <math>
\oc A \tens \oc A \linequiv \oc A </math> &amp;emsp; <math>
\oc \one \linequiv \one </math>
*: <math>
\wn A \parr \wn A \linequiv \wn A </math> &amp;emsp; <math>
\wn \bot \linequiv \bot </math>
* Digging:
*: <math>
\oc\oc A \linequiv \oc A </math> &amp;emsp; <math>
\wn\wn A \linequiv \wn A </math>
* Other properties of exponentials:
*: <math>
\oc\wn\oc\wn A \linequiv \oc\wn A </math> &amp;emsp; <math>
\oc\wn \one \linequiv \one </math>
*: <math>
\wn\oc\wn\oc A \linequiv \wn\oc A </math> &amp;emsp; <math>
\wn\oc \bot \linequiv \bot </math>
These properties of exponentials lead to the [[lattice of exponential modalities]].
* Commutation of quantifiers (<math>\zeta</math> does not occur in <math>A</math>):
*: <math>
\exists \xi. \exists \psi. A \linequiv \exists \psi. \exists \xi. A </math> &amp;emsp; <math>
\exists \xi.(A \plus B) \linequiv \exists \xi.A \plus \exists \xi.B </math> &amp;emsp; <math>
\exists \zeta.(A\tens B) \linequiv A\tens\exists \zeta.B </math> &amp;emsp; <math>
\exists \zeta.A \linequiv A </math>
*: <math>
\forall \xi. \forall \psi. A \linequiv \forall \psi. \forall \xi. A </math> &amp;emsp; <math>
\forall \xi.(A \with B) \linequiv \forall \xi.A \with \forall \xi.B </math> &amp;emsp; <math>
\forall \zeta.(A\parr B) \linequiv A\parr\forall \zeta.B </math> &amp;emsp; <math>
\forall \zeta.A \linequiv A </math>

=== Definability ===

The units and the additive connectives can be defined using second-order
quantification and exponentials, indeed the following equivalences hold:
* <math> \zero \linequiv \forall X.X </math>
* <math> \one \linequiv \forall X.(X \limp X) </math>
* <math> A \plus B \linequiv \forall X.(\oc(A \limp X) \limp \oc(B \limp X) \limp X) </math>
The constants <math>\top</math> and <math>\bot</math> and the connective
<math>\with</math> can be defined by duality.

Any pair of connectives that has the same rules as <math>\tens/\parr</math> is
equivalent to it, the same holds for additives, but not for exponentials.

Other [[List of equivalences|basic equivalences]] exist.

== Properties of proofs ==

=== Cut elimination and consequences ===

{{Theorem|title=cut elimination|
For every sequent <math>\Gamma\vdash\Delta</math>, there is a proof of
<math>\Gamma\vdash\Delta</math> if and only if there is a proof of
<math>\Gamma\vdash\Delta</math> that does not use the cut rule.}}

This property is proved using a set of rewriting rules on proofs, using
appropriate termination arguments (see the specific articles on
[[cut elimination]] for detailed proofs), it is the core of the proof/program
correspondence.

It has several important consequences:

{{Definition|title=subformula|
The subformulas of a formula <math>A</math> are <math>A</math> and, inductively, the subformulas of its immediate subformulas:
* the immediate subformulas of <math>A\tens B</math>, <math>A\parr B</math>, <math>A\plus B</math>, <math>A\with B</math> are <math>A</math> and <math>B</math>,
* the only immediate subformula of <math>\oc A</math> and <math>\wn A</math> is <math>A</math>,
* <math>\one</math>, <math>\bot</math>, <math>\zero</math>, <math>\top</math> and atomic formulas have no immediate subformula,
* the immediate subformulas of <math>\exists x.A</math> and <math>\forall x.A</math> are all the <math>A[t/x]</math> for all first-order terms <math>t</math>,
* the immediate subformulas of <math>\exists X.A</math> and <math>\forall X.A</math> are all the <math>A[B/X]</math> for all formulas <math>B</math> (with the appropriate number of parameters).}}

{{Theorem|title=subformula property|
A sequent <math>\Gamma\vdash\Delta</math> is provable if and only if it is the conclusion of a proof in which each intermediate conclusion is made of subformulas of the
formulas of <math>\Gamma</math> and <math>\Delta</math>.}}
{{Proof|By the cut elimination theorem, if a sequent is provable, then it is provable by a cut-free proof.
In each rule except the cut rule, all formulas of the premisses are either
formulas of the conclusion, or immediate subformulas of it, therefore
cut-free proofs have the subformula property.}}

The subformula property means essentially nothing in the second-order system,
since any formula is a subformula of a quantified formula where the quantified
variable occurs.
However, the property is very meaningful if the sequent <math>\Gamma</math> does not use
second-order quantification, as it puts a strong restriction on the set of
potential proofs of a given sequent.
In particular, it implies that the first-order fragment without quantifiers is
decidable.

{{Theorem|title=consistency|
The empty sequent <math>\vdash</math> is not provable.
Subsequently, it is impossible to prove both a formula <math>A</math> and its
negation <math>A\orth</math>; it is impossible to prove <math>\zero</math> or
<math>\bot</math>.}}
{{Proof|If a sequent is provable, then it is the conclusion of a cut-free proof.
In each rule except the cut rule, there is at least one formula in conclusion.
Therefore <math>\vdash</math> cannot be the conclusion of a proof.
The other properties are immediate consequences: if <math>\vdash A\orth</math>
and <math>\vdash A</math> are provable, then by the left negation rule
<math>A\orth\vdash</math> is provable, and by the cut rule one gets empty
conclusion, which is not possible.
As particular cases, since <math>\one</math> and <math>\top</math> are
provable, <math>\bot</math> and <math>\zero</math> are not, since they are
equivalent to <math>\one\orth</math> and <math>\top\orth</math>
respectively.}}

=== Expansion of identities ===

Let us write <math>\pi:\Gamma\vdash\Delta</math> to signify that
<math>\pi</math> is a proof with conclusion <math>\Gamma\vdash\Delta</math>.

{{Proposition|title=<math>\eta</math>-expansion|
For every proof <math>\pi</math>, there is a proof <math>\pi'</math> with the
same conclusion as <math>\pi</math> in which the axiom rule is only used with
atomic formulas.
If <math>\pi</math> is cut-free, then there is a cut-free <math>\pi'</math>.}}
{{Proof|It suffices to prove that for every formula <math>A</math>, the sequent
<math>A\vdash A</math> has a cut-free proof in which the axiom rule is used
only for atomic formulas.
We prove this by induction on <math>A</math>.
* If <math>A</math> is atomic, then <math>A\vdash A</math> is an instance of the atomic axiom rule.
* If <math>A=A_1\tens A_2</math> then we have<br><math>
\AxRule{ \pi_1 : A_1 \vdash A_1 }
\AxRule{ \pi_2 : A_2 \vdash A_2 }
\LabelRule{ \tens_R }
\BinRule{ A_1, A_2 \vdash A_1 \tens A_2 }
\LabelRule{ \tens_L }
\UnaRule{ A_1 \tens A_2 \vdash A_1 \tens A_2 }
\DisplayProof
</math><br>where <math>\pi_1</math> and <math>\pi_2</math> exist by induction hypothesis.
* If <math>A=A_1\parr A_2</math> then we have<br><math>
\AxRule{ \pi_1 : A_1 \vdash A_1 }
\AxRule{ \pi_2 : A_2 \vdash A_2 }
\LabelRule{ \parr_L }
\BinRule{ A_1 \parr A_2 \vdash A_1, A_2 }
\LabelRule{ \parr_R }
\UnaRule{ A_1 \parr A_2 \vdash A_1 \parr A_2 }
\DisplayProof
</math><br>where <math>\pi_1</math> and <math>\pi_2</math> exist by induction hypothesis.
* All other connectives follow the same pattern.}}

The interesting thing with <math>\eta</math>-expansion is that, we can always assume that
each connective is explicitly introduced by its associated rule (except in the
case where there is an occurrence of the <math>\top</math> rule).

=== Reversibility ===

{{Definition|title=reversibility|
A connective <math>c</math> is called ''reversible'' if
* for every proof <math>\pi:\Gamma\vdash\Delta,c(A_1,\ldots,A_n)</math>, there is a proof <math>\pi'</math> with the same conclusion in which <math>c(A_1,\ldots,A_n)</math> is introduced by the last rule,
* if <math>\pi</math> is cut-free then there is a cut-free <math>\pi'</math>.}}

{{Proposition|
The connectives <math>\parr</math>, <math>\bot</math>, <math>\with</math>, <math>\top</math> and <math>\forall</math> are reversible.}}
{{Proof|Using the <math>\eta</math>-expansion property, we assume that the axiom rule is only applied to atomic formulas.
Then each top-level connective is introduced either by its associated (left or
right) rule or in an instance of the <math>\zero_L</math> or
<math>\top_R</math> rule.

For <math>\parr</math>, consider a proof <math>\pi\Gamma\vdash\Delta,A\parr
B</math>.
If <math>A\parr B</math> is introduced by a <math>\parr_R</math> rule (not
necessarily the last rule in <math>\pi</math>), then if we remove this rule
we get a proof of <math>\vdash\Gamma,A,B</math> (this can be proved by a
straightforward induction on <math>\pi</math>).
If it is introduced in the context of a <math>\zero_L</math> or
<math>\top_R</math> rule, then this rule can be changed so that
<math>A\parr B</math> is replaced by <math>A,B</math>.
In either case, we can apply a final <math>\parr</math> rule to get the
expected proof.

For <math>\bot</math>, the same technique applies: if it is introduced by a
<math>\bot_R</math> rule, then remove this rule to get a proof of
<math>\vdash\Gamma</math>, if it is introduced by a <math>\zero_L</math> or
<math>\top_R</math> rule, remove the <math>\bot</math> from this rule, then
apply the <math>\bot</math> rule at the end of the new proof.

For <math>\with</math>, consider a proof
<math>\pi:\Gamma\vdash\Delta,A\with B</math>.
If the connective is introduced by a <math>\with</math> rule then this rule is
applied in a context like

<math>
\AxRule{ \pi_1 \Gamma' \vdash \Delta', A }
\AxRule{ \pi_2 \Gamma' \vdash \Delta', B }
\LabelRule{ \with }
\BinRule{ \Gamma' \vdash \Delta', A \with B }
\DisplayProof
</math>

Since the formula <math>A\with B</math> is not involved in other rules (except
as context), if we replace this step by <math>\pi_1</math> in <math>\pi</math>
we finally get a proof <math>\pi'_1:\Gamma\vdash\Delta,A</math>.
If we replace this step by <math>\pi_2</math> we get a proof
<math>\pi'_2:\Gamma\vdash\Delta,B</math>.
Combining <math>\pi_1</math> and <math>\pi_2</math> with a final
<math>\with</math> rule we finally get the expected proof.
The case when the <math>\with</math> was introduced in a <math>\top</math>
rule is solved as before.

For <math>\top</math> the result is trivial: just choose <math>\pi'</math> as
an instance of the <math>\top</math> rule with the appropriate conclusion.

For <math>\forall</math>, consider a proof
<math>\pi:\Gamma\vdash\Delta,\forall\xi.A</math>.
Up to renaming, we can assume that <math>\xi</math> occurs free only above the
rule that introduces the quantifier.
If the quantifier is introduced by a <math>\forall</math> rule, then if we
remove this rule, we can check that we get a proof of
<math>\Gamma\vdash\Delta,A</math> on which we can finally apply the
<math>\forall</math> rule.
The case when the <math>\forall</math> was introduced in a <math>\top</math>
rule is solved as before.

Note that, in each case, if the proof we start from is cut-free, our
transformations do not introduce a cut rule.
However, if the original proof has cuts, then the final proof may have more
cuts, since in the case of <math>\with</math> we duplicated a part of the
original proof.}}

A corresponding property for positive connectives is [[Reversibility and focalization|focalization]], which states that clusters of positive formulas can be treated in one step, under certain circumstances.

== One-sided sequent calculus ==

The sequent calculus presented above is very symmetric: for every left
introduction rule, there is a right introduction rule for the dual connective
that has the exact same structure.
Moreover, because of the involutivity of negation, a sequent
<math>\Gamma,A\vdash\Delta</math> is provable if and only if the sequent
<math>\Gamma\vdash A\orth,\Delta</math> is provable.
From these remarks, we can define an equivalent one-sided sequent calculus:
* Formulas are considered up to De Morgan duality. Equivalently, one can consider that negation is not a connective but a syntactically defined operation on formulas. In this case, negated atoms <math>\alpha\orth</math> must be considered as another kind of atomic formulas.
* Sequents have the form <math>\vdash\Gamma</math>.
The inference rules are essentially the same except that the left hand side of
sequents is kept empty:
* Identity group:
*: <math>
\LabelRule{\rulename{axiom}}
\NulRule{ \vdash A\orth, A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, A }
\AxRule{ \vdash \Delta, A\orth }
\LabelRule{\rulename{cut}}
\BinRule{ \vdash \Gamma, \Delta }
\DisplayProof
</math>
* Multiplicative group:
*: <math>
\AxRule{ \vdash \Gamma, A }
\AxRule{ \vdash \Delta, B }
\LabelRule{ \tens }
\BinRule{ \vdash \Gamma, \Delta, A \tens B }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, A, B }
\LabelRule{ \parr }
\UnaRule{ \vdash \Gamma, A \parr B }
\DisplayProof
</math> &amp;emsp; <math>
\LabelRule{ \one }
\NulRule{ \vdash \one }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma }
\LabelRule{ \bot }
\UnaRule{ \vdash \Gamma, \bot }
\DisplayProof
</math>
* Additive group:
*: <math>
\AxRule{ \vdash \Gamma, A }
\LabelRule{ \plus_1 }
\UnaRule{ \vdash \Gamma, A \plus B }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, B }
\LabelRule{ \plus_2 }
\UnaRule{ \vdash \Gamma, A \plus B }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, A }
\AxRule{ \vdash \Gamma, B }
\LabelRule{ \with }
\BinRule{ \vdash, \Gamma, A \with B }
\DisplayProof
</math> &amp;emsp; <math>
\LabelRule{ \top }
\NulRule{ \vdash \Gamma, \top }
\DisplayProof
</math>
* Exponential group:
*: <math>
\AxRule{ \vdash \Gamma, A }
\LabelRule{ d }
\UnaRule{ \vdash \Gamma, \wn A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma }
\LabelRule{ w }
\UnaRule{ \vdash \Gamma, \wn A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, \wn A, \wn A }
\LabelRule{ c }
\UnaRule{ \vdash \Gamma, \wn A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \wn\Gamma, B }
\LabelRule{ \oc }
\UnaRule{ \vdash \wn\Gamma, \oc B }
\DisplayProof
</math>
* Quantifier group (in the <math>\forall</math> rule, <math>\xi</math> must not occur free in <math>\Gamma</math>):
*: <math>
\AxRule{ \vdash \Gamma, A[t/x] }
\LabelRule{ \exists^1 }
\UnaRule{ \vdash \Gamma, \exists x.A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, A[B/X] }
\LabelRule{ \exists^2 }
\UnaRule{ \vdash \Gamma, \exists X.A }
\DisplayProof
</math> &amp;emsp; <math>
\AxRule{ \vdash \Gamma, A }
\LabelRule{ \forall }
\UnaRule{ \vdash \Gamma, \forall \xi.A }
\DisplayProof
</math>

{{Theorem|A two-sided sequent <math>\Gamma\vdash\Delta</math> is provable if
and only if the sequent <math>\vdash\Gamma\orth,\Delta</math> is provable in
the one-sided system.}}

The one-sided system enjoys the same properties as the two-sided one,
including cut elimination, the subformula property, etc.
This formulation is often used when studying proofs because it is much lighter
than the two-sided form while keeping the same expressiveness.
In particular, [[proof-nets]] can be seen as a quotient of one-sided sequent
calculus proofs under commutation of rules.

== Variations ==

=== Exponential rules ===

* The promotion rule, on the right-hand side for example,
<math>
\AxRule{ \oc A_1, \ldots, \oc A_n \vdash B, \wn B_1, \ldots, \wn B_m }
\LabelRule{ \oc_R }
\UnaRule{ \oc A_1, \ldots, \oc A_n \vdash \oc B, \wn B_1, \ldots, \wn B_m }
\DisplayProof
</math>
can be replaced by a ''multi-functorial'' promotion rule
<math>
\AxRule{ A_1, \ldots, A_n \vdash B, B_1, \ldots, B_m }
\LabelRule{ \oc_R \rulename{mf}}
\UnaRule{ \oc A_1, \ldots, \oc A_n \vdash \oc B, \wn B_1, \ldots, \wn B_m }
\DisplayProof
</math>
and a ''digging'' rule
<math>
\AxRule{ \Gamma \vdash \wn\wn A, \Delta }
\LabelRule{ \wn\wn}
\UnaRule{ \Gamma \vdash \wn A, \Delta }
\DisplayProof
</math>,
without modifying the provability.

Note that digging violates the subformula property.

* In presence of the digging rule <math>
\AxRule{ \Gamma \vdash \wn\wn A, \Delta }
\LabelRule{ \wn\wn}
\UnaRule{ \Gamma \vdash \wn A, \Delta }
\DisplayProof
</math>, the multiplexing rule <math>
\AxRule{\Gamma\vdash A^{(n)},\Delta}
\LabelRule{\rulename{mplex}}
\UnaRule{\Gamma\vdash \wn A,\Delta}
\DisplayProof
</math> (where <math>A^{(n)}</math> stands for n occurrences of formula <math>A</math>) is equivalent (for provability) to the triple of rules: contraction, weakening, dereliction.

=== Non-symmetric sequents ===

The same remarks that lead to the definition of the one-sided calculus can
lead the definition of other simplified systems:
* A one-sided variant with sequents of the form <math>\Gamma\vdash</math> could be defined.
* When considering formulas up to De Morgan duality, an equivalent system is obtained by considering only the left and right rules for positive connectives (or the ones for negative connectives only, obviously).
* [[Intuitionistic linear logic]] is the two-sided system where the right-hand side is constrained to always contain exactly one formula (with a few associated restrictions).
* Similar restrictions are used in various [[semantics]] and [[proof search]] formalisms.

=== Mix rules ===

It is quite common to consider [[Mix|mix rules]]:
<math>
\LabelRule{\rulename{Mix}_0}
\NulRule{\vdash}
\DisplayProof
\qquad
\AxRule{\Gamma \vdash \Delta}
\AxRule{\Gamma' \vdash \Delta'}
\LabelRule{\rulename{Mix}_2}
\BinRule{\Gamma,\Gamma' \vdash \Delta,\Delta'}
\DisplayProof
</math>


= System L =


'''System L''' is a family of syntax for a variety of variants of linear logic, inspired from classical calculi such as <math>\bar\lambda\mu\tilde\mu</math>-calculus. These, in turn, stem from the study of abstract machines for <math>\lambda</math>-calculus. In this realm, [[Polarized linear logic|polarization]] and [[focalization]] are features that appear naturally. Positives are typically values, and negatives pattern-matches. Contraction and weakening are implicit.

We present here a system for explicitely polarized and focalized linear logic. Polarization classifies terms and types between positive and negative; focalization separates values from non-values.

== Definitions ==

Positive types: <math>P ::= 1 \mid P_1 \otimes P_2 \mid 0 \mid P_1 \oplus P_2 \mid \shpos N \mid \oc N</math>

Negative types: <math>N ::= \bot \mid N_1 \parr N_2 \mid \top \mid N_1 \with N_2 \mid \shneg P \mid \wn P</math>

Positive values: <math>v^+ ::= x^+ \mid () \mid (v_1^+, v_2^+) \mid inl(v^+) \mid inr(v^+) \mid \shpos t^- \mid \mu(\wn x^+).c</math>

Positive terms: <math>t^+ ::= v^+ \mid \mu x^-.c</math>

Negative terms: <math>t^- ::= x^- \mid \mu x^+.c \mid \mu().c \mid \mu(x^+, y^+).c \mid \mu [\cdot] \mid \mu[inl(x^+).c_1 \mid inr(y^+).c_2] \mid \mu(\shpos x^-).c \mid \wn v^+</math>

Commands: <math>c ::= \langle t^+ \mid t^- \rangle</math>

== Typing ==

There are as many typing sequents classes as there are terms classes. Typing of positive values corresponds to focalized sequents, and commands are cuts.

Positive values: sequents are of the form <math>\vdash \Gamma :: v^+ : P</math>.


<math>
\LabelRule{\rulename{ax}^+}
\NulRule{\vdash x^+:P\orth :: x^+: P}
\DisplayProof
</math>


<math>
\LabelRule{1}
\NulRule{\vdash \ :: () : 1}
\DisplayProof
</math>


<math>
\AxRule{\vdash \Gamma_1 :: v_1^+ : P_1}
\AxRule{\vdash \Gamma_2 :: v_2^+ : P_2}
\LabelRule{\rulename{\otimes}}
\BinRule{\vdash\Gamma_1, \Gamma_2 :: (v_1^+, v_2^+) : P_1\otimes P_2}
\DisplayProof
</math>


<math>
\AxRule{\vdash \Gamma :: v^+ : P_1}
\LabelRule{\rulename{\oplus_1}}
\UnaRule{\vdash\Gamma :: inl(v^+) : P_1\oplus P_2}
\DisplayProof
\qquad
\AxRule{\vdash \Gamma :: v^+ : P_2}
\LabelRule{\rulename{\oplus_2}}
\UnaRule{\vdash\Gamma :: inr(v^+) : P_1\oplus P_2}
\DisplayProof
</math>


<math>
\AxRule{\vdash \Gamma \mid t^- : N}
\LabelRule{\shpos}
\UnaRule{\vdash\Gamma :: \shpos t^- : \shpos N}
\DisplayProof
</math>


<math>
\AxRule{c \vdash \wn\Gamma, x^+ : N}
\LabelRule{\oc}
\UnaRule{\vdash\wn\Gamma :: \mu(\wn x^+).c : \oc N}
\DisplayProof
</math>


Positive terms: sequents are of the form <math>\vdash\Gamma\mid t^+:P</math>.


<math>
\AxRule{\vdash \Gamma :: v^+ : P}
\LabelRule{\rulename{foc}}
\UnaRule{\vdash\Gamma \mid v^+ : P}
\DisplayProof
</math>


<math>
\AxRule{c \vdash \Gamma, x^- : P}
\LabelRule{\rulename{\mu^-}}
\UnaRule{\vdash\Gamma \mid\mu x^-.c : P}
\DisplayProof
</math>


Negative terms: sequents are of the form <math>\vdash\Gamma\mid t^-:N</math>.


<math>
\LabelRule{\rulename{ax}^-}
\NulRule{\vdash x^-:N\orth \mid x^-: N}
\DisplayProof
</math>


<math>
\AxRule{c\vdash \Gamma, x^+: N}
\LabelRule{\mu^+}
\UnaRule{\vdash\Gamma \mid \mu x^+.c : N}
\DisplayProof
</math>


<math>
\AxRule{c \vdash \Gamma}
\LabelRule{\bot}
\UnaRule{\vdash \Gamma \mid \mu().c : \bot}
\DisplayProof
</math>


<math>
\AxRule{c\vdash \Gamma, x^+: N_1, y^+: N_2}
\LabelRule{\rulename{\parr}}
\UnaRule{\vdash\Gamma \mid \mu(x^+, y^+).c : N_1 \parr N_2}
\DisplayProof
</math>


<math>
\LabelRule{\rulename{\top}}
\NulRule{\vdash \Gamma \mid \mu[\cdot] : \top}
\DisplayProof
</math>


<math>
\AxRule{c_1\vdash \Gamma, x^+:N_1}
\AxRule{c_2\vdash \Gamma, y^+:N_2}
\LabelRule{\rulename{\with}}
\BinRule{\vdash\Gamma \mid \mu[inl(x^+).c_1 \mid inr(y^+).c_2] : N_1 \with N_2}
\DisplayProof
</math>


<math>
\AxRule{c\vdash \Gamma, x^-: P}
\LabelRule{\shneg}
\UnaRule{\vdash\Gamma \mid \mu(\shpos x^-).c : \shneg P}
\DisplayProof
</math>


<math>
\AxRule{\vdash \Gamma :: v^+ : P}
\LabelRule{\wn}
\UnaRule{\vdash\Gamma \mid \wn v^+ : \wn P}
\DisplayProof
</math>


Commands:


<math>
\AxRule{\vdash \Gamma \mid t^+ : P}
\AxRule{\vdash \Delta \mid t^- : P\orth}
\LabelRule{\rulename{cut}}
\BinRule{\langle t^+ \mid t^-\rangle\vdash\Gamma, \Delta}
\DisplayProof
</math>


<math>
\AxRule{c \vdash \Gamma}
\LabelRule{\rulename{wkn}}
\UnaRule{c \vdash\Gamma, x^+: \wn P}
\DisplayProof
</math>


<math>
\AxRule{c \vdash \Gamma, x_1^+:\wn P, x_2^+:\wn P}
\LabelRule{\rulename{ctr}}
\UnaRule{c[x_1^+ := x^+, x_2^+ := x^+] \vdash\Gamma, x^+: \wn P}
\DisplayProof
</math>


== Reduction rules ==

<math>\langle v^+ \mid \mu x^+.c \rangle \rightarrow c[ x^+ := v^+] </math>

<math>\langle \mu x^-.c \mid t^- \rangle \rightarrow c[x^- := t^-] </math>

<math>\langle () \mid \mu().c \rangle \rightarrow c </math>

<math>\langle (v_1^+, v_2^+) \mid \mu(x^+, y^+).c \rangle \rightarrow c[x^+ := v_1^+, y^+ := v_2^+] </math>

<math>\langle inl(v^+) \mid \mu[inl(x^+).c_1 \mid inr(y^+).c_2] \rangle \rightarrow c_1[x^+ := v^+] </math>

<math>\langle inr(v^+) \mid \mu[inl(x^+).c_1 \mid inr(y^+).c_2] \rangle \rightarrow c_2[y^+ := v^+] </math>

<math>\langle \shpos t^- \mid \mu(\shpos x^-).c \rangle \rightarrow c[x^- := t^-] </math>

<math>\langle \mu(\wn x^+).c \mid \wn v^+ \rangle \rightarrow c[x^+ := v^+] </math>

== References ==

* {{BibEntry|bibtype=proceedings|author=Pierre-Louis Curien and Guillaume Munch-Maccagnoni|title=The duality of computation under focus|booktitle=IFIP TCS|year=2010}}


= Translations of classical logic =


== T-translation <math>A\imp B \mapsto \oc{\wn{A}}\limp\wn{B}</math> ==

Formulas are translated as:

<math>
\begin{array}{rcl}
X^T &amp; = &amp; X \\
(A\imp B)^T &amp; = &amp; \oc{\wn{A^T}}\limp\wn{B^T} \\
(A\wedge B)^T &amp; = &amp; \wn{A^T} \with \wn{B^T} \\
T^T &amp; = &amp; \top \\
(A\vee B)^T &amp; = &amp; \wn{A^T}\parr\wn{B^T} \\
F^T &amp; = &amp; \bot \\
(\neg A)^T &amp; = &amp; \wn{\oc{(A^T)\orth}} \\
(\forall\xi A)^T &amp; = &amp; \forall\xi \wn{A^T} \\
(\exists\xi A)^T &amp; = &amp; \exists\xi \oc{\wn{A^T}}
\end{array}
</math>

This is extended to sequents by <math>(\Gamma\vdash\Delta)^T = \oc{\wn{\Gamma^T}}\vdash\wn{\Delta^T}</math>.

This allows one to translate the rules of classical logic into linear logic:

<math>
\LabelRule{\rulename{ax}}
\NulRule{A\vdash A}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{\wn{A^T}\vdash\wn{A^T}}
\LabelRule{\oc L}
\UnaRule{\oc{\wn{A^T}}\vdash\wn{A^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma',A\vdash\Delta'}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,\Gamma'\vdash\Delta,\Delta'}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash\wn{A^T},\wn{\Delta^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash\oc{\wn{A^T}},\wn{\Delta^T}}
\AxRule{\oc{\wn{\Gamma'^T}},\oc{\wn{A^T}}\vdash\wn{\Delta'^T}}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\wn{\Gamma^T}},\oc{\wn{\Gamma'^T}}\vdash\wn{\Delta^T},\wn{\Delta'^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,A\vdash\Delta}
\LabelRule{c L}
\UnaRule{\Gamma,A\vdash\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}},\oc{\wn{A^T}}\vdash\wn{\Delta^T}}
\LabelRule{\oc c L}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,A,\Delta}
\LabelRule{c R}
\UnaRule{\Gamma\vdash A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash\wn{A^T},\wn{A^T},\wn{\Delta^T}}
\LabelRule{\wn c R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash\wn{A^T},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash\Delta}
\LabelRule{w L}
\UnaRule{\Gamma,A\vdash\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash\wn{\Delta^T}}
\LabelRule{\oc w L}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash\Delta}
\LabelRule{w R}
\UnaRule{\Gamma\vdash A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash\wn{\Delta^T}}
\LabelRule{\wn w R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash\wn{A^T},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash B,\Delta}
\LabelRule{\imp R}
\UnaRule{\Gamma\vdash A\imp B,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash\wn{B^T},\wn{\Delta^T}}
\LabelRule{\limp R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \oc{\wn{A^T}}\limp\wn{B^T},\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{(\oc{\wn{A^T}}\limp\wn{B^T})},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma',B\vdash\Delta'}
\LabelRule{\imp L}
\BinRule{\Gamma,\Gamma',A\imp B\vdash\Delta,\Delta'}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash\wn{A^T},\wn{\Delta^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash\oc{\wn{A^T}},\wn{\Delta^T}}
\LabelRule{\rulename{ax}}
\NulRule{\wn{B^T}\vdash\wn{B^T}}
\LabelRule{\limp L}
\BinRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\limp\wn{B^T}\vdash\wn{B^T},\wn{\Delta^T}}
\LabelRule{\wn L}
\UnaRule{\oc{\wn{\Gamma^T}},\wn{(\oc{\wn{A^T}}\limp\wn{B^T})}\vdash\wn{B^T},\wn{\Delta^T}}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{\wn{(\oc{\wn{A^T}}\limp\wn{B^T})}}\vdash\wn{B^T},\wn{\Delta^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{\wn{(\oc{\wn{A^T}}\limp\wn{B^T})}}\vdash\oc{\wn{B^T}},\wn{\Delta^T}}
\AxRule{\oc{\wn{\Gamma'^T}},\oc{\wn{B^T}}\vdash\wn{\Delta'^T}}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\wn{\Gamma^T}},\oc{\wn{\Gamma'^T}},\oc{\wn{(\oc{\wn{A^T}}\limp\wn{B^T})}}\vdash\wn{\Delta^T},\wn{\Delta'^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma\vdash B,\Delta}
\LabelRule{\wedge R}
\BinRule{\Gamma\vdash A\wedge B,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T},\wn{\Delta^T}}
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{B^T},\wn{\Delta^T}}
\LabelRule{\with R}
\BinRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T}\with \wn{B^T},\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{(\wn{A^T}\with \wn{B^T})},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\LabelRule{\wedge_1 L}
\UnaRule{\Gamma,A\wedge B\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{\wn{A^T}\vdash \wn{A^T}}
\LabelRule{\with_1 L}
\UnaRule{\wn{A^T}\with \wn{B^T}\vdash \wn{A^T}}
\LabelRule{\wn L}
\UnaRule{\wn{(\wn{A^T}\with \wn{B^T})}\vdash \wn{A^T}}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{(\wn{A^T}\with \wn{B^T})}}\vdash \wn{A^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{(\wn{A^T}\with \wn{B^T})}}\vdash \oc{\wn{A^T}}}
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash \wn{\Delta^T}}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\wn{\Gamma^T}},\oc{\wn{(\wn{A^T}\with \wn{B^T})}}\vdash \wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{T R}
\NulRule{\Gamma\vdash T,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\top R}
\NulRule{\oc{\wn{\Gamma^T}}\vdash \top,\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{\top},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,B,\Delta}
\LabelRule{\vee R}
\UnaRule{\Gamma\vdash A\vee B,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T},\wn{B^T},\wn{\Delta^T}}
\LabelRule{\parr R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T}\parr\wn{B^T},\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{(\wn{A^T}\parr\wn{B^T})},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\AxRule{\Gamma',B\vdash \Delta'}
\LabelRule{\vee L}
\BinRule{\Gamma,\Gamma',A\vee B\vdash \Delta,\Delta'}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{\wn{A^T}\vdash \wn{A^T}}
\LabelRule{\rulename{ax}}
\NulRule{\wn{B^T}\vdash \wn{B^T}}
\LabelRule{\parr L}
\BinRule{\wn{A^T}\parr \wn{B^T}\vdash \wn{A^T},\wn{B^T}}
\LabelRule{\wn L}
\UnaRule{\wn{(\wn{A^T}\parr \wn{B^T})}\vdash \wn{A^T},\wn{B^T}}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{(\wn{A^T}\parr \wn{B^T})}}\vdash \wn{A^T},\wn{B^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{(\wn{A^T}\parr \wn{B^T})}}\vdash \wn{A^T},\oc{\wn{B^T}}}
\AxRule{\oc{\wn{\Gamma'^T}},\oc{\wn{B^T}}\vdash \wn{\Delta'^T}}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\wn{\Gamma'^T}},\oc{\wn{(\wn{A^T}\parr \wn{B^T})}}\vdash \wn{A^T},\wn{\Delta'^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{\Gamma'^T}},\oc{\wn{(\wn{A^T}\parr \wn{B^T})}}\vdash \oc{\wn{A^T}},\wn{\Delta'^T}}
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash \wn{\Delta^T}}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\wn{\Gamma^T}},\oc{\wn{\Gamma'^T}},\oc{\wn{(\wn{A^T}\parr \wn{B^T})}}\vdash \wn{\Delta^T},\wn{\Delta'^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash \Delta}
\LabelRule{F R}
\UnaRule{\Gamma\vdash F,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{\Delta^T}}
\LabelRule{\bot R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \bot,\wn{\Delta^T}}
\LabelRule{\wn R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{\bot},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{F L}
\NulRule{F\vdash {}}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\bot L}
\NulRule{\bot\vdash {}}
\LabelRule{\wn L}
\UnaRule{\wn{\bot}\vdash {}}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{\bot}}\vdash {}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\LabelRule{\neg R}
\UnaRule{\Gamma\vdash \neg A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash \wn{\Delta^T}}
\LabelRule{(.)\orth R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{\oc{(A^T)\orth}},\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{\wn{\oc{(A^T)\orth}}},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\LabelRule{\neg L}
\UnaRule{\Gamma,\neg A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T},\wn{\Delta^T}}
\LabelRule{(.)\orth L}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{(A^T)\orth}\vdash \wn{\Delta^T}}
\LabelRule{\wn L}
\UnaRule{\oc{\wn{\Gamma^T}},\wn{\oc{(A^T)\orth}}\vdash \wn{\Delta^T}}
\LabelRule{\wn L}
\UnaRule{\oc{\wn{\Gamma^T}},\wn{\wn{\oc{(A^T)\orth}}}\vdash \wn{\Delta^T}}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{\wn{\wn{\oc{(A^T)\orth}}}}\vdash \wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\LabelRule{\forall R}
\UnaRule{\Gamma\vdash \forall\xi A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T},\wn{\Delta^T}}
\LabelRule{\forall R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \forall\xi \wn{A^T},\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{\forall\xi \wn{A^T}},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A[\tau/\xi]\vdash \Delta}
\LabelRule{\forall L}
\UnaRule{\Gamma,\forall\xi A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{\wn{A^T}[\tau^T/\xi]\vdash \wn{A^T}[\tau^T/\xi]}
\LabelRule{\forall L}
\UnaRule{\forall\xi \wn{A^T}\vdash \wn{A^T}[\tau^T/\xi]}
\LabelRule{\wn L}
\UnaRule{\wn{\forall\xi \wn{A^T}}\vdash \wn{A^T}[\tau^T/\xi]}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{\forall\xi \wn{A^T}}}\vdash \wn{A^T}[\tau^T/\xi]}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{\forall\xi \wn{A^T}}}\vdash \oc{\wn{A^T}}[\tau^T/\xi]}
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{(A^T[\tau^T/\xi])}}\vdash \wn{\Delta^T}}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\wn{\Gamma^T}},\oc{\wn{\forall\xi \wn{A^T}}}\vdash \wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A[\tau/\xi],\Delta}
\LabelRule{\exists R}
\UnaRule{\Gamma\vdash \exists\xi A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}}\vdash \wn{A^T[\tau^T/\xi]},\wn{\Delta^T}}
\LabelRule{\oc R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \oc{\wn{A^T[\tau^T/\xi]}},\wn{\Delta^T}}
\LabelRule{\exists R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \exists\xi \oc{\wn{A^T}},\wn{\Delta^T}}
\LabelRule{\wn d R}
\UnaRule{\oc{\wn{\Gamma^T}}\vdash \wn{\exists\xi \oc{\wn{A^T}}},\wn{\Delta^T}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\LabelRule{\exists L}
\UnaRule{\Gamma,\exists\xi A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\wn{\Gamma^T}},\oc{\wn{A^T}}\vdash \wn{\Delta^T}}
\LabelRule{\exists L}
\UnaRule{\oc{\wn{\Gamma^T}},\exists\xi\oc{\wn{A^T}}\vdash \wn{\Delta^T}}
\LabelRule{\wn L}
\UnaRule{\oc{\wn{\Gamma^T}},\wn{\exists\xi\oc{\wn{A^T}}}\vdash \wn{\Delta^T}}
\LabelRule{\oc d L}
\UnaRule{\oc{\wn{\Gamma^T}},\oc{\wn{\exists\xi\oc{\wn{A^T}}}}\vdash \wn{\Delta^T}}
\DisplayProof
</math>

=== Alternative presentation ===

It is also possible to define <math>A^{\overline{T}}</math> by:

<math>
\begin{array}{rcl}
X^{\overline{T}} &amp; = &amp; \wn{X} \\
(A\imp B)^{\overline{T}} &amp; = &amp; \wn{(\oc{A^{\overline{T}}}\limp B^{\overline{T}})} \\
(A\wedge B)^{\overline{T}} &amp; = &amp; \wn{(A^{\overline{T}} \with B^{\overline{T}})} \\
T^{\overline{T}} &amp; = &amp; \wn{\top} \\
(A\vee B)^{\overline{T}} &amp; = &amp; \wn{(A^{\overline{T}}\parr B^{\overline{T}})} \\
F^{\overline{T}} &amp; = &amp; \wn{\bot} \\
(\neg A)^{\overline{T}} &amp; = &amp; \wn{\wn{(A^{\overline{T}})\orth}} \\
(\forall\xi A)^{\overline{T}} &amp; = &amp; \wn{\forall\xi A^{\overline{T}}} \\
(\exists\xi A)^{\overline{T}} &amp; = &amp; \wn{\exists\xi \oc{A^{\overline{T}}}}
\end{array}
</math>

If we define <math>(\Gamma\vdash\Delta)^{\overline{T}} = \oc{\Gamma^{\overline{T}}}\vdash \Delta^{\overline{T}}</math>, we have <math>(\Gamma\vdash\Delta)^{\overline{T}} = (\Gamma\vdash\Delta)^T</math> and thus we obtain the same translation of proofs.


== Q-translation <math>A\imp B \mapsto \oc{(A\limp\wn{B})}</math> ==

Formulas are translated as:

<math>
\begin{array}{rcl}
X^Q &amp; = &amp; \oc{X} \\
(A\imp B)^Q &amp; = &amp; \oc{(A^Q\limp\wn{B^Q})} \\
(A\wedge B)^Q &amp; = &amp; \oc{(A^Q \tens B^Q)} \\
T^Q &amp; = &amp; \oc{\one} \\
(A\vee B)^Q &amp; = &amp; \oc{(A^Q\plus B^Q)} \\
F^Q &amp; = &amp; \oc{\zero} \\
(\neg A)^Q &amp; = &amp; \oc{(A^Q)\orth} \\
(\forall\xi A)^Q &amp; = &amp; \oc{\forall\xi \wn{A^Q}} \\
(\exists\xi A)^Q &amp; = &amp; \oc{\exists\xi A^Q}
\end{array}
</math>

The translation of any formula starts with <math>\oc</math>, we define <math>A^{\underline{Q}}</math> such that <math>A^Q=\oc{A^{\underline{Q}}}</math>.

The translation of sequents is <math>(\Gamma\vdash\Delta)^Q = \Gamma^Q\vdash\wn{\Delta^Q}</math>.

This allows one to translate the rules of classical logic into linear logic:

<math>
\LabelRule{\rulename{ax}}
\NulRule{A\vdash A}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{A^Q\vdash A^Q}
\LabelRule{\wn d R}
\UnaRule{A^Q\vdash \wn{A^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma',A\vdash \Delta'}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,\Gamma'\vdash \Delta,\Delta'}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\AxRule{\Gamma'^Q,A^Q\vdash \wn{\Delta'^Q}}
\LabelRule{\wn L}
\UnaRule{\Gamma'^Q,\wn{A^Q}\vdash \wn{\Delta'^Q}}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^Q,\Gamma'^Q\vdash \wn{\Delta^Q},\wn{\Delta'^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,A\vdash \Delta}
\LabelRule{c L}
\UnaRule{\Gamma,A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q,A^Q\vdash \wn{\Delta^Q}}
\LabelRule{\oc c L}
\UnaRule{\Gamma^Q,A^Q\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,A,\Delta}
\LabelRule{c R}
\UnaRule{\Gamma\vdash A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{A^Q},\wn{\Delta^Q}}
\LabelRule{\wn c R}
\UnaRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash \Delta}
\LabelRule{w L}
\UnaRule{\Gamma,A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{\Delta^Q}}
\LabelRule{\oc w L}
\UnaRule{\Gamma^Q,A^Q\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash \Delta}
\LabelRule{w R}
\UnaRule{\Gamma\vdash A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{\Delta^Q}}
\LabelRule{\wn w R}
\UnaRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash B,\Delta}
\LabelRule{\imp R}
\UnaRule{\Gamma\vdash A\imp B,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q\vdash \wn{B^Q},\wn{\Delta^Q}}
\LabelRule{\limp R}
\UnaRule{\Gamma^Q\vdash A^Q\limp \wn{B^Q},\wn{\Delta^Q}}
\LabelRule{\oc R}
\UnaRule{\Gamma^Q\vdash \oc{(A^Q\limp \wn{B^Q})},\wn{\Delta^Q}}
\LabelRule{\wn d R}
\UnaRule{\Gamma^Q\vdash \wn{\oc{(A^Q\limp \wn{B^Q})}},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma',B\vdash \Delta'}
\LabelRule{\imp L}
\BinRule{\Gamma,\Gamma',A\imp B\vdash \Delta,\Delta'}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\LabelRule{\rulename{ax}}
\NulRule{A^Q\vdash A^Q}
\AxRule{\Gamma'^Q,B^Q\vdash \wn{\Delta'^Q}}
\LabelRule{\wn L}
\UnaRule{\Gamma'^Q,\wn{B^Q}\vdash \wn{\Delta'^Q}}
\LabelRule{\limp L}
\BinRule{\Gamma'^Q,A^Q\limp \wn{B^Q},A^Q\vdash \wn{\Delta'^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma'^Q,\oc{(A^Q\limp \wn{B^Q})},A^Q\vdash \wn{\Delta'^Q}}
\LabelRule{\wn L}
\UnaRule{\Gamma'^Q,\oc{(A^Q\limp \wn{B^Q})},\wn{A^Q}\vdash \wn{\Delta'^Q}}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^Q,\Gamma'^Q,\oc{(A^Q\limp \wn{B^Q})}\vdash \wn{\Delta^Q},\wn{\Delta'^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\AxRule{\Gamma'\vdash B,\Delta'}
\LabelRule{\wedge R}
\BinRule{\Gamma,\Gamma'\vdash A\wedge B,\Delta,\Delta'}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\AxRule{\Gamma'^Q\vdash \wn{B^Q},\wn{\Delta'^Q}}
\LabelRule{\rulename{ax}}
\NulRule{A^Q\vdash A^Q}
\LabelRule{\rulename{ax}}
\NulRule{B^Q\vdash B^Q}
\LabelRule{\tens R}
\BinRule{A^Q,B^Q\vdash A^Q\tens B^Q}
\LabelRule{\oc R}
\UnaRule{A^Q,B^Q\vdash \oc{(A^Q\tens B^Q)}}
\LabelRule{\wn d R}
\UnaRule{A^Q,B^Q\vdash \wn{\oc{(A^Q\tens B^Q)}}}
\LabelRule{\wn L}
\UnaRule{A^Q,\wn{B^Q}\vdash \wn{\oc{(A^Q\tens B^Q)}}}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma'^Q,A^Q\vdash \wn{\oc{(A^Q\tens B^Q)}},\wn{\Delta'^Q}}
\LabelRule{\wn L}
\UnaRule{\Gamma'^Q,\wn{A^Q}\vdash \wn{\oc{(A^Q\tens B^Q)}},\wn{\Delta'^Q}}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^Q,\Gamma'^Q\vdash \wn{\oc{(A^Q\tens B^Q)}},\wn{\Delta^Q},\wn{\Delta'^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,B\vdash \Delta}
\LabelRule{\wedge L}
\UnaRule{\Gamma,A\wedge B\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q,B^Q\vdash \wn{\Delta^Q}}
\LabelRule{\tens L}
\UnaRule{\Gamma^Q,A^Q\tens B^Q\vdash \wn{\Delta^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma^Q,\oc{(A^Q\tens B^Q)}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{T R}
\NulRule{{}\vdash T}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\one R}
\NulRule{{}\vdash \one}
\LabelRule{\oc R}
\UnaRule{{}\vdash \oc{\one}}
\LabelRule{\wn d R}
\UnaRule{{}\vdash \wn{\oc{\one}}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash \Delta}
\LabelRule{T L}
\UnaRule{\Gamma,T\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{\Delta^Q}}
\LabelRule{\one L}
\UnaRule{\Gamma^Q,\one\vdash \wn{\Delta^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma^Q,\oc{\one}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\LabelRule{\vee_1 R}
\UnaRule{\Gamma\vdash A\vee B,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\LabelRule{\rulename{ax}}
\NulRule{A^Q\vdash A^Q}
\LabelRule{\plus_1 R}
\UnaRule{A^Q\vdash A^Q\plus B^Q}
\LabelRule{\oc R}
\UnaRule{A^Q\vdash \oc{(A^Q\plus B^Q)}}
\LabelRule{\wn d R}
\UnaRule{A^Q\vdash \wn{\oc{(A^Q\plus B^Q)}}}
\LabelRule{\wn L}
\UnaRule{\wn{A^Q}\vdash \wn{\oc{(A^Q\plus B^Q)}}}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^Q\vdash \wn{\oc{(A^Q\plus B^Q)}},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\AxRule{\Gamma,B\vdash \Delta}
\LabelRule{\vee L}
\BinRule{\Gamma,A\vee B\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q\vdash \wn{\Delta^Q}}
\AxRule{\Gamma^Q,B^Q\vdash \wn{\Delta^Q}}
\LabelRule{\plus L}
\BinRule{\Gamma^Q,A^Q\plus B^Q\vdash \wn{\Delta^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma^Q,\oc{(A^Q\plus B^Q)}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\LabelRule{F L}
\NulRule{\Gamma,F\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\zero L}
\NulRule{\Gamma^Q,\zero\vdash \wn{\Delta^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma^Q,\oc{\zero}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\LabelRule{\neg R}
\UnaRule{\Gamma\vdash \neg A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q\vdash \wn{\Delta^Q}}
\LabelRule{(.)\orth R}
\UnaRule{\Gamma^Q\vdash (A^Q)\orth,\wn{\Delta^Q}}
\LabelRule{\oc R}
\UnaRule{\Gamma^Q\vdash \oc{(A^Q)\orth},\wn{\Delta^Q}}
\LabelRule{\wn d R}
\UnaRule{\Gamma^Q\vdash \wn{\oc{(A^Q)\orth}},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\LabelRule{\neg L}
\UnaRule{\Gamma,\neg A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\LabelRule{(.)\orth L}
\UnaRule{\Gamma^Q,\oc{(A^Q)\orth}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A,\Delta}
\LabelRule{\forall R}
\UnaRule{\Gamma\vdash \forall\xi A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q},\wn{\Delta^Q}}
\LabelRule{\forall R}
\UnaRule{\Gamma^Q\vdash \forall\xi \wn{A^Q},\wn{\Delta^Q}}
\LabelRule{\oc R}
\UnaRule{\Gamma^Q\vdash \oc{\forall\xi \wn{A^Q}},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

We use <math>(A[\tau/\xi])^Q=A^Q[\tau^{\underline{Q}}/\xi]</math>.

<math>
\AxRule{\Gamma,A[\tau/\xi]\vdash \Delta}
\LabelRule{\forall L}
\UnaRule{\Gamma,\forall\xi A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q[\tau^{\underline{Q}}/\xi]\vdash \wn{\Delta^Q}}
\LabelRule{\wn L}
\UnaRule{\Gamma^Q,\wn{A^Q[\tau^{\underline{Q}}/\xi]}\vdash \wn{\Delta^Q}}
\LabelRule{\forall L}
\UnaRule{\Gamma^Q,\forall\xi \wn{A^Q}\vdash \wn{\Delta^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma^Q,\oc{\forall\xi \wn{A^Q}}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A[\tau/\xi],\Delta}
\LabelRule{\exists R}
\UnaRule{\Gamma\vdash \exists\xi A,\Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q\vdash \wn{A^Q[\tau^{\underline{Q}}/\xi]},\wn{\Delta^Q}}
\LabelRule{\rulename{ax}}
\NulRule{A^Q[\tau^{\underline{Q}}/\xi]\vdash A^Q[\tau^{\underline{Q}}/\xi]}
\LabelRule{\exists R}
\UnaRule{A^Q[\tau^{\underline{Q}}/\xi]\vdash \exists\xi A^Q}
\LabelRule{\oc R}
\UnaRule{A^Q[\tau^{\underline{Q}}/\xi]\vdash \oc{\exists\xi A^Q}}
\LabelRule{\wn d R}
\UnaRule{A^Q[\tau^{\underline{Q}}/\xi]\vdash \wn{\oc{\exists\xi A^Q}}}
\LabelRule{\wn L}
\UnaRule{\wn{A^Q[\tau^{\underline{Q}}/\xi]}\vdash \wn{\oc{\exists\xi A^Q}}}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^Q\vdash \wn{\oc{\exists\xi A^Q}},\wn{\Delta^Q}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash \Delta}
\LabelRule{\exists L}
\UnaRule{\Gamma,\exists\xi A\vdash \Delta}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^Q,A^Q\vdash \wn{\Delta^Q}}
\LabelRule{\exists L}
\UnaRule{\Gamma^Q,\exists\xi A^Q\vdash \wn{\Delta^Q}}
\LabelRule{\oc d L}
\UnaRule{\Gamma^Q,\oc{\exists\xi A^Q}\vdash \wn{\Delta^Q}}
\DisplayProof
</math>


=== Alternative presentation ===

It is also possible to define <math>A^{\underline{Q}}</math> as the primitive construction.

<math>
\begin{array}{rcl}
X^{\underline{Q}} &amp; = &amp; X \\
(A\imp B)^{\underline{Q}} &amp; = &amp; \oc{A^{\underline{Q}}}\limp\wn{\oc{B^{\underline{Q}}}} \\
(A\wedge B)^{\underline{Q}} &amp; = &amp; \oc{A^{\underline{Q}}}\tens\oc{B^{\underline{Q}}} \\
T^{\underline{Q}} &amp; = &amp; \one \\
(A\vee B)^{\underline{Q}} &amp; = &amp; \oc{A^{\underline{Q}}}\plus\oc{B^{\underline{Q}}} \\
F^{\underline{Q}} &amp; = &amp; \zero \\
(\neg A)^{\underline{Q}} &amp; = &amp; \wn{(A^{\underline{Q}})\orth} \\
(\forall\xi A)^{\underline{Q}} &amp; = &amp; \forall\xi \wn{\oc{A^{\underline{Q}}}} \\
(\exists\xi A)^{\underline{Q}} &amp; = &amp; \exists\xi \oc{A^{\underline{Q}}}
\end{array}
</math>

If we define <math>(\Gamma\vdash\Delta)^{\underline{Q}} = \oc{\Gamma^{\underline{Q}}}\vdash\wn{\oc{\Delta^{\underline{Q}}}}</math>, we have <math>(\Gamma\vdash\Delta)^{\underline{Q}} = (\Gamma\vdash\Delta)^Q</math> and thus we obtain the same translation of proofs.


= Translations of intuitionistic logic =


The genesis of linear logic comes with a decomposition of the intuitionistic implication. Once linear logic properly defined, it corresponds to a translation of intuitionistic logic into linear logic, often called ''Girard's translation''. In fact Jean-Yves Girard has defined two translations in his linear logic paper<ref name=&quot;ll&quot;>{{BibEntry|bibtype=journal|author=Girard, Jean-Yves|title=[http://iml.univ-mrs.fr/~girard/linear.pdf Linear logic]|journal=Theoretical Computer Science|volume=50|issue=1|pages=1-101|doi=10.1016/0304-3975(87)90045-4|year=1987}}</ref>. We call them the [[call-by-name]] translation and the [[call-by-value]] translation.

These translations can be extended to [[translations of classical logic]] into linear logic.

== Call-by-name Girard's translation <math>A\imp B \mapsto \oc{A}\limp B</math> ==

Formulas are translated as:

<math>
\begin{array}{rcl}
X^n &amp; = &amp; X \\
(A\imp B)^n &amp; = &amp; \oc{A^n}\limp B^n \\
(A\wedge B)^n &amp; = &amp; A^n \with B^n \\
T^n &amp; = &amp; \top \\
(A\vee B)^n &amp; = &amp; \oc{A^n}\plus\oc{B^n} \\
F^n &amp; = &amp; \zero \\
(\forall\xi A)^n &amp; = &amp; \forall\xi A^n \\
(\exists\xi A)^n &amp; = &amp; \exists\xi \oc{A^n}
\end{array}
</math>

This is extended to sequents by <math>(\Gamma\vdash A)^n = \oc{\Gamma^n}\vdash A^n</math>.

This allows one to translate the rules of intuitionistic logic into linear logic:

<math>
\LabelRule{\rulename{ax}}
\NulRule{A\vdash A}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{A^n\vdash A^n}
\LabelRule{\oc d L}
\UnaRule{\oc{A^n}\vdash A^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,A\vdash B}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,\Delta\vdash B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash A^n}
\LabelRule{\oc R}
\UnaRule{\oc{\Gamma^n}\vdash \oc{A^n}}
\AxRule{\oc{\Delta^n},\oc{A^n}\vdash B^n}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\Gamma^n},\oc{\Delta^n}\vdash B^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,A\vdash C}
\LabelRule{c L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n},\oc{A^n},\oc{A^n}\vdash C^n}
\LabelRule{\oc c L}
\UnaRule{\oc{\Gamma^n},\oc{A^n}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash C}
\LabelRule{w L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash C^n}
\LabelRule{\oc w L}
\UnaRule{\oc{\Gamma^n},\oc{A^n}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash B}
\LabelRule{\imp R}
\UnaRule{\Gamma\vdash A\imp B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n},\oc{A^n}\vdash B^n}
\LabelRule{\limp R}
\UnaRule{\oc{\Gamma^n}\vdash \oc{A^n}\limp B^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,B\vdash C}
\LabelRule{\imp L}
\BinRule{\Gamma,\Delta,A\imp B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash A^n}
\LabelRule{\oc R}
\UnaRule{\oc{\Gamma^n}\vdash \oc{A^n}}
\LabelRule{\rulename{ax}}
\NulRule{B^n\vdash B^n}
\LabelRule{\limp L}
\BinRule{\oc{\Gamma^n},\oc{A^n}\limp B^n\vdash B^n}
\LabelRule{\oc d L}
\UnaRule{\oc{\Gamma^n},\oc{(\oc{A^n}\limp B^n)}\vdash B^n}
\LabelRule{\oc R}
\UnaRule{\oc{\Gamma^n},\oc{(\oc{A^n}\limp B^n)}\vdash \oc{B^n}}
\AxRule{\oc{\Delta^n},\oc{B^n}\vdash C^n}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\Gamma^n},\oc{\Delta^n},\oc{(\oc{A^n}\limp B^n)}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Gamma\vdash B}
\LabelRule{\wedge R}
\BinRule{\Gamma\vdash A\wedge B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash A^n}
\AxRule{\oc{\Gamma^n}\vdash B^n}
\LabelRule{\with R}
\BinRule{\oc{\Gamma^n}\vdash A^n\with B^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\LabelRule{\wedge_1 L}
\UnaRule{\Gamma,A\wedge B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{A^n\vdash A^n}
\LabelRule{\with_1 L}
\UnaRule{A^n\with B^n\vdash A^n}
\LabelRule{\oc d L}
\UnaRule{\oc{(A^n\with B^n)}\vdash A^n}
\LabelRule{\oc R}
\UnaRule{\oc{(A^n\with B^n)}\vdash \oc{A^n}}
\AxRule{\oc{\Gamma^n},\oc{A^n}\vdash C^n}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\Gamma^n},\oc{(A^n\with B^n)}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\LabelRule{T R}
\NulRule{\Gamma\vdash T}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\top R}
\NulRule{\oc{\Gamma^n}\vdash \top}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\vee_1 R}
\UnaRule{\Gamma\vdash A\vee B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash A^n}
\LabelRule{\oc R}
\UnaRule{\oc{\Gamma^n}\vdash \oc{A^n}}
\LabelRule{\plus_1 R}
\UnaRule{\oc{\Gamma^n}\vdash \oc{A^n}\plus\oc{B^n}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\AxRule{\Gamma,B\vdash C}
\LabelRule{\vee L}
\BinRule{\Gamma,A\vee B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n},\oc{A^n}\vdash C^n}
\AxRule{\oc{\Gamma^n},\oc{B^n}\vdash C^n}
\LabelRule{\plus L}
\BinRule{\oc{\Gamma^n},\oc{A^n}\plus\oc{B^n}\vdash C^n}
\LabelRule{\oc d L}
\UnaRule{\oc{\Gamma^n},\oc{(\oc{A^n}\plus\oc{B^n})}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\LabelRule{F L}
\NulRule{\Gamma,F\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\zero L}
\NulRule{\oc{\Gamma^n},\zero\vdash C^n}
\LabelRule{\oc d L}
\UnaRule{\oc{\Gamma^n},\oc{\zero}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\forall R}
\UnaRule{\Gamma\vdash \forall\xi A}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash A^n}
\LabelRule{\forall R}
\UnaRule{\oc{\Gamma^n}\vdash \forall\xi A^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A[\tau/\xi]\vdash C}
\LabelRule{\forall L}
\UnaRule{\Gamma,\forall\xi A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{A^n[\tau^n/\xi]\vdash A^n[\tau^n/\xi]}
\LabelRule{\forall L}
\UnaRule{\forall\xi A^n\vdash A^n[\tau^n/\xi]}
\LabelRule{\oc d L}
\UnaRule{\oc{\forall\xi A^n}\vdash A^n[\tau^n/\xi]}
\LabelRule{\oc R}
\UnaRule{\oc{\forall\xi A^n}\vdash \oc{(A^n[\tau^n/\xi])}}
\AxRule{\oc{\Gamma^n},\oc{(A^n[\tau^n/\xi])}\vdash C^n}
\LabelRule{\rulename{cut}}
\BinRule{\oc{\Gamma^n},\oc{\forall\xi A^n}\vdash C^n}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A[\tau/\xi]}
\LabelRule{\exists R}
\UnaRule{\Gamma\vdash \exists\xi A}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n}\vdash A^n[\tau^n/\xi]}
\LabelRule{\oc R}
\UnaRule{\oc{\Gamma^n}\vdash \oc{(A^n[\tau^n/\xi])}}
\LabelRule{\exists R}
\UnaRule{\oc{\Gamma^n}\vdash \exists\xi\oc{A^n}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\LabelRule{\exists L}
\UnaRule{\Gamma,\exists\xi A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\oc{\Gamma^n},\oc{A^n}\vdash C^n}
\LabelRule{\exists L}
\UnaRule{\oc{\Gamma^n},\exists\xi\oc{A^n}\vdash C^n}
\LabelRule{\oc d L}
\UnaRule{\oc{\Gamma^n},\oc{\exists\xi\oc{A^n}}\vdash C^n}
\DisplayProof
</math>

== Call-by-value translation <math>A\imp B \mapsto \oc{(A\limp B)}</math> ==

Formulas are translated as:

<math>
\begin{array}{rcl}
X^v &amp; = &amp; \oc{X} \\
(A\imp B)^v &amp; = &amp; \oc{(A^v\limp B^v)} \\
(A\wedge B)^v &amp; = &amp; \oc{(A^v \tens B^v)} \\
T^v &amp; = &amp; \oc{\one} \\
(A\vee B)^v &amp; = &amp; \oc{(A^v\plus B^v)} \\
F^v &amp; = &amp; \oc{\zero} \\
(\forall\xi A)^v &amp; = &amp; \oc{\forall\xi A^v} \\
(\exists\xi A)^v &amp; = &amp; \oc{\exists\xi A^v}
\end{array}
</math>

The translation of any formula starts with <math>\oc</math>, we define <math>A^{\underline{v}}</math> such that <math>A^v=\oc{A^{\underline{v}}}</math>.

The translation of sequents is <math>(\Gamma\vdash A)^v = \Gamma^v\vdash A^v</math>.

This allows one to translate the rules of intuitionistic logic into linear logic:

<math>
\LabelRule{\rulename{ax}}
\NulRule{A\vdash A}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{A^v\vdash A^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,A\vdash B}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,\Delta\vdash B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash A^v}
\AxRule{\Delta^v,A^v\vdash B^v}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^v,\Delta^v\vdash B^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,A\vdash C}
\LabelRule{c L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v,A^v,A^v\vdash C^v}
\LabelRule{\oc c L}
\UnaRule{\Gamma^v,A^v\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash C}
\LabelRule{w L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash C^v}
\LabelRule{\oc w L}
\UnaRule{\Gamma^v,A^v\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash B}
\LabelRule{\imp R}
\UnaRule{\Gamma\vdash A\imp B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v,A^v\vdash B^v}
\LabelRule{\limp R}
\UnaRule{\Gamma^v\vdash A^v\limp B^v}
\LabelRule{\oc R}
\UnaRule{\Gamma^v\vdash \oc{(A^v\limp B^v)}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,B\vdash C}
\LabelRule{\imp L}
\BinRule{\Gamma,\Delta,A\imp B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash A^v}
\AxRule{\Delta^v,B^v\vdash C^v}
\LabelRule{\limp L}
\BinRule{\Gamma^v,\Delta^v,A^v\limp B^v\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\Delta^v,\oc{(A^v\limp B^v)}\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta\vdash B}
\LabelRule{\wedge R}
\BinRule{\Gamma,\Delta\vdash A\wedge B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash A^v}
\AxRule{\Delta^v\vdash B^v}
\LabelRule{\tens R}
\BinRule{\Gamma^v,\Delta^v\vdash A^v\tens B^v}
\LabelRule{\oc R}
\UnaRule{\Gamma^v,\Delta^v\vdash \oc{(A^v\tens B^v)}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,B\vdash C}
\LabelRule{\wedge L}
\UnaRule{\Gamma,A\wedge B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v,A^v,B^v\vdash C^v}
\LabelRule{\tens L}
\UnaRule{\Gamma^v,A^v\tens B^v\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\oc{(A^v\tens B^v)}\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\LabelRule{T R}
\NulRule{{}\vdash T}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\one R}
\NulRule{{}\vdash \one}
\LabelRule{\oc R}
\UnaRule{{}\vdash \oc{\one}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash C}
\LabelRule{T L}
\UnaRule{\Gamma,T\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash C^v}
\LabelRule{\one L}
\UnaRule{\Gamma^v,\one\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\oc{\one}\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\vee_1 R}
\UnaRule{\Gamma\vdash A\vee B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash A^v}
\LabelRule{\plus_1 R}
\UnaRule{\Gamma^v\vdash A^v\plus B^v}
\LabelRule{\oc R}
\UnaRule{\Gamma^v\vdash \oc{(A^v\plus B^v)}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\AxRule{\Gamma,B\vdash C}
\LabelRule{\vee L}
\BinRule{\Gamma,A\vee B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v,A^v\vdash C^v}
\AxRule{\Gamma^v,B^v\vdash C^v}
\LabelRule{\plus L}
\BinRule{\Gamma^v,A^v\plus B^v\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\oc{(A^v\plus B^v)}\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\LabelRule{F L}
\NulRule{\Gamma,F\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\zero L}
\NulRule{\Gamma^v,\zero\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\oc{\zero}\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\forall R}
\UnaRule{\Gamma\vdash \forall\xi A}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash A^v}
\LabelRule{\forall R}
\UnaRule{\Gamma^v\vdash \forall\xi A^v}
\LabelRule{\oc R}
\UnaRule{\Gamma^v\vdash \oc{\forall\xi A^v}}
\DisplayProof
</math>

<br />

We use <math>(A[\tau/\xi])^v=A^v[\tau^{\underline{v}}/\xi]</math>.

<math>
\AxRule{\Gamma,A[\tau/\xi]\vdash C}
\LabelRule{\forall L}
\UnaRule{\Gamma,\forall\xi A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v,A^v[\tau^{\underline{v}}/\xi]\vdash C^v}
\LabelRule{\forall L}
\UnaRule{\Gamma^v,\forall\xi A^v\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\oc{\forall\xi A^v}\vdash C^v}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A[\tau/\xi]}
\LabelRule{\exists R}
\UnaRule{\Gamma\vdash \exists\xi A}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v\vdash A^v[\tau^{\underline{v}}/\xi]}
\LabelRule{\exists R}
\UnaRule{\Gamma^v\vdash \exists\xi A^v}
\LabelRule{\oc R}
\UnaRule{\Gamma^v\vdash \oc{\exists\xi A^v}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\LabelRule{\exists L}
\UnaRule{\Gamma,\exists\xi A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^v,A^v\vdash C^v}
\LabelRule{\exists L}
\UnaRule{\Gamma^v,\exists\xi A^v\vdash C^v}
\LabelRule{\oc d L}
\UnaRule{\Gamma^v,\oc{\exists\xi A^v}\vdash C^v}
\DisplayProof
</math>


=== Alternative presentation ===

It is also possible to define <math>A^{\underline{v}}</math> as the primitive construction.

<math>
\begin{array}{rcl}
X^{\underline{v}} &amp; = &amp; X \\
(A\imp B)^{\underline{v}} &amp; = &amp; \oc{A^{\underline{v}}}\limp\oc{B^{\underline{v}}} \\
(A\wedge B)^{\underline{v}} &amp; = &amp; \oc{A^{\underline{v}}}\tens\oc{B^{\underline{v}}} \\
T^{\underline{v}} &amp; = &amp; \one \\
(A\vee B)^{\underline{v}} &amp; = &amp; \oc{A^{\underline{v}}}\plus\oc{B^{\underline{v}}} \\
F^{\underline{v}} &amp; = &amp; \zero \\
(\forall\xi A)^{\underline{v}} &amp; = &amp; \forall\xi \oc{A^{\underline{v}}} \\
(\exists\xi A)^{\underline{v}} &amp; = &amp; \exists\xi \oc{A^{\underline{v}}}
\end{array}
</math>

If we define <math>(\Gamma\vdash A)^{\underline{v}} = \oc{\Gamma^{\underline{v}}}\vdash\oc{A^{\underline{v}}}</math>, we have <math>(\Gamma\vdash A)^{\underline{v}} = (\Gamma\vdash A)^v</math> and thus we obtain the same translation of proofs.

== Call-by-value Girard's translation <math>A\imp B \mapsto \oc{(A\limp B)}</math> ==

The original version of the call-by-value translation given by Jean-Yves Girard<ref name=&quot;ll&quot; />&amp;nbsp;is an optimisation of the previous one using properties of [[positive formula]]s.

Formulas are translated as:

<math>
\begin{array}{rcl}
X^w &amp; = &amp; \oc{X} \\
(A\imp B)^w &amp; = &amp; \oc{(A^w\limp B^w)} \\
(A\wedge B)^w &amp; = &amp; A^w \tens B^w \\
T^w &amp; = &amp; \one \\
(A\vee B)^w &amp; = &amp; A^w\plus B^w \\
F^w &amp; = &amp; \zero \\
(\forall\xi A)^w &amp; = &amp; \oc{\forall\xi A^w} \\
(\exists\xi A)^w &amp; = &amp; \exists\xi A^w
\end{array}
</math>

The translation of any formula is a [[positive formula]].

The translation of sequents is <math>(\Gamma\vdash A)^w = \Gamma^w\vdash A^w</math>.

This allows one to translate the rules of intuitionistic logic into linear logic:

<math>
\LabelRule{\rulename{ax}}
\NulRule{A\vdash A}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\rulename{ax}}
\NulRule{A^w\vdash A^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,A\vdash B}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma,\Delta\vdash B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash A^w}
\AxRule{\Delta^w,A^w\vdash B^w}
\LabelRule{\rulename{cut}}
\BinRule{\Gamma^w,\Delta^w\vdash B^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,A\vdash C}
\LabelRule{c L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w,A^w,A^w\vdash C^w}
\LabelRule{+ c L}
\UnaRule{\Gamma^w,A^w\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash C}
\LabelRule{w L}
\UnaRule{\Gamma,A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash C^w}
\LabelRule{+ w L}
\UnaRule{\Gamma^w,A^w\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash B}
\LabelRule{\imp R}
\UnaRule{\Gamma\vdash A\imp B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w,A^w\vdash B^w}
\LabelRule{\limp R}
\UnaRule{\Gamma^w\vdash A^w\limp B^w}
\LabelRule{+ \oc R}
\UnaRule{\Gamma^w\vdash \oc{(A^w\limp B^w)}}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta,B\vdash C}
\LabelRule{\imp L}
\BinRule{\Gamma,\Delta,A\imp B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash A^w}
\AxRule{\Delta^w,B^w\vdash C^w}
\LabelRule{\limp L}
\BinRule{\Gamma^w,\Delta^w,A^w\limp B^w\vdash C^w}
\LabelRule{\oc d L}
\UnaRule{\Gamma^w,\Delta^w,\oc{(A^w\limp B^w)}\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\AxRule{\Delta\vdash B}
\LabelRule{\wedge R}
\BinRule{\Gamma,\Delta\vdash A\wedge B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash A^w}
\AxRule{\Delta^w\vdash B^w}
\LabelRule{\tens R}
\BinRule{\Gamma^w,\Delta^w\vdash A^w\tens B^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A,B\vdash C}
\LabelRule{\wedge L}
\UnaRule{\Gamma,A\wedge B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w,A^w,B^w\vdash C^w}
\LabelRule{\tens L}
\UnaRule{\Gamma^w,A^w\tens B^w\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\LabelRule{T R}
\NulRule{{}\vdash T}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\one R}
\NulRule{{}\vdash \one}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\vee_1 R}
\UnaRule{\Gamma\vdash A\vee B}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash A^w}
\LabelRule{\plus_1 R}
\UnaRule{\Gamma^w\vdash A^w\plus B^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\AxRule{\Gamma,B\vdash C}
\LabelRule{\vee L}
\BinRule{\Gamma,A\vee B\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w,A^w\vdash C^w}
\AxRule{\Gamma^w,B^w\vdash C^w}
\LabelRule{\plus L}
\BinRule{\Gamma^w,A^w\plus B^w\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\LabelRule{F L}
\NulRule{\Gamma,F\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\LabelRule{\zero L}
\NulRule{\Gamma^w,\zero\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A}
\LabelRule{\forall R}
\UnaRule{\Gamma\vdash \forall\xi A}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash A^w}
\LabelRule{\forall R}
\UnaRule{\Gamma^w\vdash \forall\xi A^w}
\LabelRule{+ \oc R}
\UnaRule{\Gamma^w\vdash \oc{\forall\xi A^w}}
\DisplayProof
</math>

<br />

We use <math>(A[\tau/\xi])^w\linequiv A^w[\tau^w/\xi]</math>.

<math>
\AxRule{\Gamma,A[\tau/\xi]\vdash C}
\LabelRule{\forall L}
\UnaRule{\Gamma,\forall\xi A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w,(A[\tau/\xi])^w\vdash C^w}
\UnaRule{\Gamma^w,A^w[\tau^w/\xi]\vdash C^w}
\LabelRule{\forall L}
\UnaRule{\Gamma^w,\forall\xi A^w\vdash C^w}
\LabelRule{\oc d L}
\UnaRule{\Gamma^w,\oc{\forall\xi A^w}\vdash C^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma\vdash A[\tau/\xi]}
\LabelRule{\exists R}
\UnaRule{\Gamma\vdash \exists\xi A}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w\vdash (A[\tau/\xi])^w}
\UnaRule{\Gamma^w\vdash A^w[\tau^w/\xi]}
\LabelRule{\exists R}
\UnaRule{\Gamma^w\vdash \exists\xi A^w}
\DisplayProof
</math>

<br />

<math>
\AxRule{\Gamma,A\vdash C}
\LabelRule{\exists L}
\UnaRule{\Gamma,\exists\xi A\vdash C}
\DisplayProof
\qquad\mapsto\qquad
\AxRule{\Gamma^w,A^w\vdash C^w}
\LabelRule{\exists L}
\UnaRule{\Gamma^w,\exists\xi A^w\vdash C^w}
\DisplayProof
</math>
